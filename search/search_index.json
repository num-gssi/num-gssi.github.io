{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Photo by Alee Kadhim Obeid Welcome to the GSSI Numerical Analysis and Data Science Group website! We work on mathematical and computational methods to solve complex problems in physics, data science, machine learning and other real-world applications, combining numerical analysis, machine learning, optimization, quantum computing and applied mathematics. Our Research Areas Numerical Analysis & Dynamical Systems We design and analyze numerical methods for dynamical systems, including ordinary, delay, and stochastic differential equations, with a focus on stability in high-dimensional systems. Our work extends to matrix theory, with contributions to eigenvalue optimization, matrix perturbation theory, matrix nearness problems, control, and graph theory. Machine Learning & Data Science We are interested in understanding different ways to train deep learning models: using implicit layers by studying their stability and possible effective application and also using low-parametric training for neural networks. These modelistic choices are often justified by empirically observed implicit biases of optimization, which is our interest to characterize. Our interest also extends to developing surrogate deep-learning models for complex physical phenomena that are usually impossible to simulate using standard numerical methods. Computational Fluid & Plasma Dynamics We study high-performance numerical methods for hyperbolic partial differential equations and kinetic models, with applications in gas dynamics, fluid dynamics, and plasma physics. Our purposed technique enhance the accuracy, efficiency and physical consistency of simulations used in scientific and industrial applications. Quantum Computing We develop quantum algorithms for solving computationally hard combinatorial optimization problems. Our focus is on leveraging Quantum Annealers and Neutral Atom quantum hardware to tackle real-world optimization challenges. By evaluating different quantum processing units (QPUs), we seek alternatives to classical methods for problems where quantum computing can offer significant speedups. Optimization & Optimal Transport We address the challenges posed by nonlinear least squares problems by developing novel algorithms that accelerate convergence and reduce computational time. Our work also extends to optimal transport problems with concave cost functions. For news, data and code, please also refer to our GitHub Repository . About GSSI The Gran Sasso Science Institute (GSSI) is an international PhD school and a center for research and higher education in the areas of Physics, Mathematics, Computer Science and Social Sciences. Professors, researchers and students from all scientific backgrounds are selected internationally, following the standards set by the best research centers and PhD schools worldwide.","title":"Home"},{"location":"#welcome","text":"to the GSSI Numerical Analysis and Data Science Group website! We work on mathematical and computational methods to solve complex problems in physics, data science, machine learning and other real-world applications, combining numerical analysis, machine learning, optimization, quantum computing and applied mathematics.","title":"Welcome"},{"location":"#our-research-areas","text":"Numerical Analysis & Dynamical Systems We design and analyze numerical methods for dynamical systems, including ordinary, delay, and stochastic differential equations, with a focus on stability in high-dimensional systems. Our work extends to matrix theory, with contributions to eigenvalue optimization, matrix perturbation theory, matrix nearness problems, control, and graph theory. Machine Learning & Data Science We are interested in understanding different ways to train deep learning models: using implicit layers by studying their stability and possible effective application and also using low-parametric training for neural networks. These modelistic choices are often justified by empirically observed implicit biases of optimization, which is our interest to characterize. Our interest also extends to developing surrogate deep-learning models for complex physical phenomena that are usually impossible to simulate using standard numerical methods. Computational Fluid & Plasma Dynamics We study high-performance numerical methods for hyperbolic partial differential equations and kinetic models, with applications in gas dynamics, fluid dynamics, and plasma physics. Our purposed technique enhance the accuracy, efficiency and physical consistency of simulations used in scientific and industrial applications. Quantum Computing We develop quantum algorithms for solving computationally hard combinatorial optimization problems. Our focus is on leveraging Quantum Annealers and Neutral Atom quantum hardware to tackle real-world optimization challenges. By evaluating different quantum processing units (QPUs), we seek alternatives to classical methods for problems where quantum computing can offer significant speedups. Optimization & Optimal Transport We address the challenges posed by nonlinear least squares problems by developing novel algorithms that accelerate convergence and reduce computational time. Our work also extends to optimal transport problems with concave cost functions. For news, data and code, please also refer to our GitHub Repository .","title":"Our Research Areas"},{"location":"#about-gssi","text":"The Gran Sasso Science Institute (GSSI) is an international PhD school and a center for research and higher education in the areas of Physics, Mathematics, Computer Science and Social Sciences. Professors, researchers and students from all scientific backgrounds are selected internationally, following the standards set by the best research centers and PhD schools worldwide.","title":"About GSSI"},{"location":"calendar/","text":"Calendar","title":"Calendar"},{"location":"calendar/#calendar","text":"","title":"Calendar"},{"location":"people/","text":"Faculty Nicola Guglielmi Nicola Gugliemi is full professor in Numerical Analysis and Chair of the Doctoral School in Mathematics at GSSI. He is interested in the general area of scientific computing, particularly numerical analysis of ordinary and delay differential equations, stability analysis of (discretized) dynamical systems - including variable coefficient and switched systems - and ode methods in matrix perturbation theory and control, for high-dimensional problems. More specifically: Stiff and singularly perturbed problems (in particular in the framework of delay differential equations); software development for general classes of implicit delay differential equations; stability analysis of numerical integrators for ordinary and delay differential equations; computation of the joint spectral characteristics of a set of matrices, with application to contractivity analysis of time dependent systems of differential and difference equations; ode-based pseudospectral computations for both unstructured and structured problems; non-smooth analysis of discontinuous differential equations with focus on the computation of weak solutions. Personal Web Page Francesco Tudisco Francesco Tudisco is an Associate Professor (Reader) in Machine Learning affiliated with the University of Edinburgh, the Maxwell Institute for Mathematical Sciences, and the Gran Sasso Science Institute. His research interests lie at the intersection between machine learning and applied mathematics, particularly numerical analysis and scientific computing. His recent work includes the design and analysis of physics-inspired deep learning models for scientific simulations and climate forecasting, machine learning on graphs, compression and speed-up of deep learning models, trustworthiness and robustness of deep learning models, and theory of deep learning, including the analysis of neural networks in the infinite width and infinite depth limits. Personal Web Page. Postdocs Angelo Alberto Casulli Asma Farooq Francesco Paolo Maiale Piero Deidda PhD Students Arturo De Marinis Arturo De Marinis is a PhD Student in Mathematics in Natural, Social and Life Sciences at GSSI. He holds a degree in Mathematics from the University of Bari Aldo Moro. In his thesis, he proposed numerical methods for trajectory optimization of unmanned aerial vehicles based on optimal control theory. His current research interests lie between numerical analysis and machine learning, in particular he is now studying the stability of new machine learning models for supervised learning, the so called neural ordinary differential equations. Bernardo Collufio Bernardo Collufio is a PhD student with PNRR fellowship in \u201cMathematics in Natural, Social and Life Sciences\u201d curriculum at GSSI. He obtained his Bachelor\u2019s degree in Mathematics at University of Messina and Master\u2019s degree in Applied Mathematics at University of Pavia, with a thesis entitled \"Mathematical and numerical study of first ionization phenomena for atomic hydrogen in an extended phase space\". His main interests concern the study of numerical schemes for solving kinetic PDEs arising in many physical processes in the context of Gas Dynamics, Fluid Dynamics and Plasma Physics, with a particular focus on high accuracy, asymptotic preserving property and preservation of conserved quantities. Dayana Savostianova Dayana Savostianova is a PhD Student in Mathematics in Natural, Social and Life Sciences, GSSI. Previously, she was a Research Intern at the Complex Systems Modelling and Control Laboratory, Department of Computer Sciences, Higher School of Economics (Moscow, Russia). She holds a BSc in Applied Mathematics and Information Science from Higher School of Economics. Her current research interests are in the scope of complex systems, mathematical modelling and direct numerical simulations of natural processes. Her previous work was focused on predictability in Self-Organised Criticality systems. Emanuele Zangrando Francesco Fabbri Joint supervision with Prof. Francesco Viola . Grigorii (Grisha) Buklei Grigorii (Grisha) Buklei is a PhD Student in Mathematics in Natural, Social and Life Sciences, GSSI. He was awarded with PNRR-funded scholarship. Previously, Grigorii was a Research Intern at the Complex Systems Modelling and Control Laboratory, Department of Computer Sciences, Higher School of Economics (Moscow, Russia). He holds a BSc in Applied Mathematics and Information Science from the Higher School of Economics. His current research interests are in the scope of mathematical modeling, Bayesian experimental design, and algorithmic game theory. Helena Biscevic Joint supervision with Prof. Raffaele D'Ambrosio . Pietro Sittoni Pietro Sittoni is a PhD student in Mathematics in Natural, Social, and Life Sciences. He holds a MSc in Data Science from the University of Padova. During his thesis, he studied the properties of subhomogeneous operators, exploiting them to present a new analysis of the existence and uniqueness of fixed points, using it to design stable deep equilibrium models and applying it to mathematical optimization. Rinat Kamalov Joint supervision with Prof. Vladimir Protasov . Sara Tarquini Former Members Anton Savostianov (PhD student) Alessia And\u00f2 (postdoc) Konstantin Prokopchik (PhD student) Mattia Manucci (PhD student) Miryam Gnazzo (PhD student) Stefano Sicilia (PhD student)","title":"People"},{"location":"people/#faculty","text":"","title":"Faculty"},{"location":"people/#nicola-guglielmi","text":"Nicola Gugliemi is full professor in Numerical Analysis and Chair of the Doctoral School in Mathematics at GSSI. He is interested in the general area of scientific computing, particularly numerical analysis of ordinary and delay differential equations, stability analysis of (discretized) dynamical systems - including variable coefficient and switched systems - and ode methods in matrix perturbation theory and control, for high-dimensional problems. More specifically: Stiff and singularly perturbed problems (in particular in the framework of delay differential equations); software development for general classes of implicit delay differential equations; stability analysis of numerical integrators for ordinary and delay differential equations; computation of the joint spectral characteristics of a set of matrices, with application to contractivity analysis of time dependent systems of differential and difference equations; ode-based pseudospectral computations for both unstructured and structured problems; non-smooth analysis of discontinuous differential equations with focus on the computation of weak solutions. Personal Web Page","title":"Nicola Guglielmi"},{"location":"people/#francesco-tudisco","text":"Francesco Tudisco is an Associate Professor (Reader) in Machine Learning affiliated with the University of Edinburgh, the Maxwell Institute for Mathematical Sciences, and the Gran Sasso Science Institute. His research interests lie at the intersection between machine learning and applied mathematics, particularly numerical analysis and scientific computing. His recent work includes the design and analysis of physics-inspired deep learning models for scientific simulations and climate forecasting, machine learning on graphs, compression and speed-up of deep learning models, trustworthiness and robustness of deep learning models, and theory of deep learning, including the analysis of neural networks in the infinite width and infinite depth limits. Personal Web Page.","title":"Francesco Tudisco"},{"location":"people/#postdocs","text":"","title":"Postdocs"},{"location":"people/#angelo-alberto-casulli","text":"","title":"Angelo Alberto Casulli"},{"location":"people/#asma-farooq","text":"","title":"Asma Farooq"},{"location":"people/#francesco-paolo-maiale","text":"","title":"Francesco Paolo Maiale"},{"location":"people/#piero-deidda","text":"","title":"Piero Deidda"},{"location":"people/#phd-students","text":"","title":"PhD Students"},{"location":"people/#arturo-de-marinis","text":"Arturo De Marinis is a PhD Student in Mathematics in Natural, Social and Life Sciences at GSSI. He holds a degree in Mathematics from the University of Bari Aldo Moro. In his thesis, he proposed numerical methods for trajectory optimization of unmanned aerial vehicles based on optimal control theory. His current research interests lie between numerical analysis and machine learning, in particular he is now studying the stability of new machine learning models for supervised learning, the so called neural ordinary differential equations.","title":"Arturo De Marinis"},{"location":"people/#bernardo-collufio","text":"Bernardo Collufio is a PhD student with PNRR fellowship in \u201cMathematics in Natural, Social and Life Sciences\u201d curriculum at GSSI. He obtained his Bachelor\u2019s degree in Mathematics at University of Messina and Master\u2019s degree in Applied Mathematics at University of Pavia, with a thesis entitled \"Mathematical and numerical study of first ionization phenomena for atomic hydrogen in an extended phase space\". His main interests concern the study of numerical schemes for solving kinetic PDEs arising in many physical processes in the context of Gas Dynamics, Fluid Dynamics and Plasma Physics, with a particular focus on high accuracy, asymptotic preserving property and preservation of conserved quantities.","title":"Bernardo Collufio"},{"location":"people/#dayana-savostianova","text":"Dayana Savostianova is a PhD Student in Mathematics in Natural, Social and Life Sciences, GSSI. Previously, she was a Research Intern at the Complex Systems Modelling and Control Laboratory, Department of Computer Sciences, Higher School of Economics (Moscow, Russia). She holds a BSc in Applied Mathematics and Information Science from Higher School of Economics. Her current research interests are in the scope of complex systems, mathematical modelling and direct numerical simulations of natural processes. Her previous work was focused on predictability in Self-Organised Criticality systems.","title":"Dayana Savostianova"},{"location":"people/#emanuele-zangrando","text":"","title":"Emanuele Zangrando"},{"location":"people/#francesco-fabbri","text":"Joint supervision with Prof. Francesco Viola .","title":"Francesco Fabbri"},{"location":"people/#grigorii-grisha-buklei","text":"Grigorii (Grisha) Buklei is a PhD Student in Mathematics in Natural, Social and Life Sciences, GSSI. He was awarded with PNRR-funded scholarship. Previously, Grigorii was a Research Intern at the Complex Systems Modelling and Control Laboratory, Department of Computer Sciences, Higher School of Economics (Moscow, Russia). He holds a BSc in Applied Mathematics and Information Science from the Higher School of Economics. His current research interests are in the scope of mathematical modeling, Bayesian experimental design, and algorithmic game theory.","title":"Grigorii (Grisha) Buklei"},{"location":"people/#helena-biscevic","text":"Joint supervision with Prof. Raffaele D'Ambrosio .","title":"Helena Biscevic"},{"location":"people/#pietro-sittoni","text":"Pietro Sittoni is a PhD student in Mathematics in Natural, Social, and Life Sciences. He holds a MSc in Data Science from the University of Padova. During his thesis, he studied the properties of subhomogeneous operators, exploiting them to present a new analysis of the existence and uniqueness of fixed points, using it to design stable deep equilibrium models and applying it to mathematical optimization.","title":"Pietro Sittoni"},{"location":"people/#rinat-kamalov","text":"Joint supervision with Prof. Vladimir Protasov .","title":"Rinat Kamalov"},{"location":"people/#sara-tarquini","text":"","title":"Sara Tarquini"},{"location":"people/#former-members","text":"Anton Savostianov (PhD student) Alessia And\u00f2 (postdoc) Konstantin Prokopchik (PhD student) Mattia Manucci (PhD student) Miryam Gnazzo (PhD student) Stefano Sicilia (PhD student)","title":"Former Members"},{"location":"pictures/","text":"Group Pictures February 19, 2025 With Prof. Yuji Nakatsukasa (University of Oxford). February 17, 2025 With Prof. Luigi Brugnano (University of Florence) and Prof. Felice Iavernaro (University of Bari). February 13, 2025 With Prof. Matthias Voigt (UniDistance). February 5, 2025 With Prof. Giovanni Russo (University of Catania) and Prof. Clarissa Astuto (University of Catania). November 27, 2024 November 21, 2024 With Prof. Bertram D\u00fcring (University of Warwick), Prof. Carola-Bibiane Sch\u00f6nlieb (University of Cambridge), Prof. Emre Mengi (Ko\u00e7 University) and Prof. Nikos Pitsianis (Aristotle University of Thessaloniki). May 8, 2024 With Prof. Desmond Higham (University of Edinburgh), Prof. Catherine Higham , (University of Glasgow) and Prof. Paola Boito (University of Pisa). April 19, 2024 With Prof. Ernst Hairer (University of Geneve). January 17, 2024 With Prof. Matthias Voigt (UniDistance). October 11, 2023 With Prof. Simone Brugiapaglia (Concordia University). September 15, 2023 With Prof. Marino Zennaro (University of Trieste), Prof. Raffaele D'Ambrosio (University of L'Aquila) and Prof. Vladimir Protasov (University of L'Aquila).","title":"Pictures"},{"location":"pictures/#group-pictures","text":"February 19, 2025 With Prof. Yuji Nakatsukasa (University of Oxford). February 17, 2025 With Prof. Luigi Brugnano (University of Florence) and Prof. Felice Iavernaro (University of Bari). February 13, 2025 With Prof. Matthias Voigt (UniDistance). February 5, 2025 With Prof. Giovanni Russo (University of Catania) and Prof. Clarissa Astuto (University of Catania). November 27, 2024 November 21, 2024 With Prof. Bertram D\u00fcring (University of Warwick), Prof. Carola-Bibiane Sch\u00f6nlieb (University of Cambridge), Prof. Emre Mengi (Ko\u00e7 University) and Prof. Nikos Pitsianis (Aristotle University of Thessaloniki). May 8, 2024 With Prof. Desmond Higham (University of Edinburgh), Prof. Catherine Higham , (University of Glasgow) and Prof. Paola Boito (University of Pisa). April 19, 2024 With Prof. Ernst Hairer (University of Geneve). January 17, 2024 With Prof. Matthias Voigt (UniDistance). October 11, 2023 With Prof. Simone Brugiapaglia (Concordia University). September 15, 2023 With Prof. Marino Zennaro (University of Trieste), Prof. Raffaele D'Ambrosio (University of L'Aquila) and Prof. Vladimir Protasov (University of L'Aquila).","title":"Group Pictures"},{"location":"reading_group/","text":"COMPiLE Reading Group We organize a reading group on COMPutational LEarning , specifically on topics related to Numerics/Matrix Theory/Graph Theory with particular emphasis on their application/connection with Machine Learning, Data Mining and Network Science. We meet regularly to read a paper of common interest, together, with the goal of creating some motivation for staying up to date with the new papers or to catch up with some papers that you wanted to read but postponed for some time. Recently, as an alternative, we have decided to let people present their own research work, so that everyone in the group knows what the others are doing, with the aim of fostering exchange of ideas and collaborations. If you are interested in participating please complete the form below to be to be notified about the meetings: Google Form: Subscription to Reading Group mailing list If you have suggestions for papers to ready you can propose your ideas here: Google Form: Suggestions for papers to read If you want inspiration for a possible paper to read, here is a list of the papers that have been suggested so far: Suggested papers for COMPiLE Reading Group The reading group is organized by our early career group members and supervised/coordinated by Nicola Guglielmi and Francesco Tudisco . Organizer Period Bernardo Collufio , GSSI, L'Aquila December 2024 -- Present Piero Deidda , GSSI, L'Aquila January 2024 -- Present Arturo De Marinis , GSSI, L'Aquila February 2023 -- February 2025 Dayana Savostianova , GSSI, L'Aquila November 2021 -- Summer 2022 Academic Year 2024/25 Presenter Title, Abstract & Other Info Date & Venue Rinat Kamalov GSSI, L'Aquila Low-Rank Approximation of Matrices and Tensors in the Chebyshev Norm Abstract April 9, 2025 5:15 PM Room C, ex-ISEF building Pietro Sittoni GSSI, L'Aquila Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach and Its Implications for Neural PDE Solvers by Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Tom Goldstein This presentation explores a novel language model architecture that scales test-time computation by implicitly reasoning in latent space. The model achieves this by iterating a recurrent block, allowing it to unroll to an arbitrary depth at test time. This approach contrasts with mainstream reasoning models, which scale computation by generating more tokens. Furthermore, it does not require specialized training data, unlike chain-of-thought-based methods. Finally, we will discuss its potential implications for neural PDE solvers. Read the paper March 3, 2025 3:00 PM Main Lecture Hall, ex-ISEF building Th\u00e9ophile Dolmaire Universit\u00e0 degli Studi dell'Aquila, L'Aquila Inelastic collapse in dimension d = 1: motivation, results, and open questions The inelastic Boltzmann equation describes granular media, composed of a large number of particles that collide inelastically with each other, dissipating kinetic energy at each collision (such as sand, snow or interstellar dust). Depending on the degree of inelasticity of the collisions, various fascinating phenomena emerge, such as onset of inhomogeneities. Nevertheless, the derivation of this equation is still an open mathematical problem. One major difficulty, already at the level of the particle system, comes from the phenomenon of inelastic collapse, when infinitely many collisions take place in finite time. We will consider inelastic hard spheres, with fixed restitution coefficient r. In the d-dimensional case (d \u2265 2), it has been observed that particles collapse along linear structure, which motivated the study of the one-dimensional particle system. We will present the mathematical approach of the one-dimensional collapse developed in the literature, before turning to the many open questions that remain in this field. We will investigate in much detail the cases of three and four particles. Besides, relying on a new conserved quantity we will prove that the order of the collisions for a system of four particles can be determined by studying a simpler two-dimensional dynamical system. Full abstract Slides of the talk February 18, 2025 5:00 PM Conference Room, ex-INPS building Grigorii (Grisha) Buklei GSSI, L'Aquila Gradient Flows on Graphons by Sewoong Oh, Soumik Pal, Raghav Somani, Raghavendra Tripathi This presentation explores the recently developed framework of gradient flows on graphons, as introduced in the paper \u201cGradient Flows on Graphons: Existence, Convergence, Continuity Equations.\u201d Gradient flows on graphons provide a framework for understanding how large-scale networks evolve over time. Graphons, which represent the limit of dense graphs, provide a powerful way to study graph dynamics using continuous mathematics. We introduce the concept of gradient flows in metric spaces and explain how they can be extended to graphons using the invariant L^2 -metric \\delta_2 . The key results focus on proving the existence and convergence of these flows, showing how they emerge as natural limits of gradient descent on large graphs. Finally, we highlight an intriguing connection to optimal transport, where the graphon metric \\delta_2 behaves similarly to the Wasserstein-2 distance W_2 , suggesting new ways to analyze dynamic networks and large-scale optimization problems. Read the paper Slides of the talk Recording of the talk (available to GSSI only) February 18, 2025 4:00 PM Conference Room, ex-INPS building Bernardo Collufio GSSI, L'Aquila About positive-preserving property of the semi-lagrangian scheme for BGK equation Kinetic equations are well known for their applications in several fields such as gas dynamics, plasma physics, traffic flow, swarming dynamics and socioeconomic modeling. Various numerical methods for solving these equations have been extensively studied over the past few decades; among these, semi-Lagrangian methods have recently gained attention for their good properties in terms of accuracy and stability. Unlike Eulerian methods, which consist of a direct discretization of the equation on an Eulerian grid, semi-Lagrangian schemes make use of the Lagrangian perspective, applying a time scheme over the characteristics and reconstructing the PDF using interpolation techniques; this avoids using the convective operator and results in unconditionally stable schemes allowing for large time steps. However, physically relevant properties of the solution, like positivity, may not always be guaranteed. The positive-preserving property may be crucial for applications in Kinetic Theory since the unknown distribution function represents the density of particles in phase space. Still, no positive-preserving semi-Lagrangian schemes have been proposed in the literature. In this talk I will focus on the BGK equation, which has several applications in collisional gas and plasma dynamics, and after investigating the reasons behind the lack of positivity preservation in current schemes I will demonstrate how this could be enforced by adapting strategies previously applied in the context of Runge-Kutta IMEX methods. February 5, 2025 5:00 PM Main Lecture Hall, ex-ISEF building Arturo De Marinis GSSI, L'Aquila Approximation properties of neural ODEs We study the universal approximation property (UAP) of shallow neural networks whose activation function is defined as the flow of a neural ODE. We prove the UAP for the space of such shallow neural networks in the space of continuous functions. In particular, we also prove the UAP with the weight matrices constrained to have unit norm. Furthermore, we are able to bound from above the Lipschitz constant of the flow of the neural ODE, that tells us how much a perturbation in input is amplified or shrunk in output. If the upper bound is large, then so it may be the Lipschitz constant, leading to the undesirable situation where certain small perturbations in input cause large changes in output. Therefore, we compute a perturbation to the weight matrix of the neural ODE such that the flow of the perturbed neural ODE has Lipschitz constant bounded from above as we desire. This leads to a stable flow and so to a stable shallow neural network. However, the stabilized shallow neural network with unit norm weight matrices does not satisfy the universal approximation property anymore. Nevertheless, we are able to prove approximation bounds that tell us how poorly and how accurately a continuous target function can be approximated by the stabilized shallow neural network. November 27, 2024 5:00 PM Main Lecture Hall, ex-ISEF building Academic Year 2023/24 Presenter Title, Abstract & Other Info Date Angelo Alberto Casulli GSSI, L'Aquila Learning Poisson Equation with Rank-Structured Matrices We explore how rank-structured matrices can be leveraged to efficiently represent and learn elliptic operators, with a focus on the 1D Poisson equation as a model problem. We begin by introducing key concepts in numerical linear algebra, including the use of rank-structured matrices to approximate the discretized Poisson operator. The talk then delves into techniques from randomized linear algebra, showing how these methods can be applied to recover rank-structured matrices from matrix-vector products. This approach enables the reconstruction of the Poisson operator\u2019s discretization by solving only a small number of Poisson equations. Finally, we extend the discussion to the 2D Poisson problem, highlighting the additional challenges that arise in this more complex setting. October 23, 202 4:00 PM Main Lecture Hall, ex-ISEF building Francesco Fabbri GSSI, L'Aquila \u03b2-Variational Autoencoder Graph Neural Network for Synthetic Generation of Abdominal Aorta Aneurisms Synthetic data, particularly in the medical field, is artificially created to replicate real medical data's statistical properties and patterns. This process is essential for addressing privacy concerns, as it allows researchers to work with data that mirrors real patient information without compromising individual privacy. Generative algorithms play a crucial role in this process by leveraging the probability distribution of the original data to create synthetic data through random sampling from features-rich latent spaces. Creating synthetic medical data from a small patient population by extracting key anatomical features offers a viable solution to the challenge of obtaining large databases of real patients. This approach allows for the development of a virtual patient population that encompasses a wide range of anatomical and physiological conditions, facilitating extensive clinical tests and statistical analyses. The medical field increasingly demands patient-specific testing, necessitating extensive validation before adopting new medical processes. While linear approaches are robust, they cannot often capture non-linear features leading to large latent spaces. In contrast, neural network-based reduced-order models can effectively capture these non-linear features with a reduced latent space. Concurrently, the use of generative algorithms and these advanced modeling tools aids in developing effective data generators. In this work, starting from a reduced real-word dataset, we propose a \u03b2-Variational Autoencoder Graph Neural Network-based framework that will allow for the creation of synthetic Abdominal Aorta Aneurisms (AAA) that maintain the statistical correlations and anatomical relationships present in real-world datasets using a small dimension latent space. October 16, 2024 5:30 PM Main Lecture Hall, ex-ISEF building Rinat Kamalov GSSI, L'Aquila Non-Chebyshev Uniform Approximation Consider the problem of the best uniform approximation of a continuous function by linear combinations of a finite system of functions (not necessarily Chebyshev) under arbitrary linear constraints on a convex domain. By modifying the concept of alternance and of the Remez iterative procedure we present a method, which demonstrates its efficiency in numerical problems. Under certain favorable assumptions, we establish the proof of linear convergence rate. Special focus is dedicated to systems of complex exponents, Gaussian functions, Cauchy functions, lacunar algebraic and trigonometric polynomials. We explore applications to signal processing, switching dynamical systems, and to Markov-Bernstein type inequalities. May 3, 2024 3:00 PM Main Lecture Hall, ex-ISEF building Sara Tarquini GSSI, L'Aquila Testing Quantum and Simulated Annealers on the Drone Delivery Packing Problem Using drones to perform human-related tasks can play a key role in various fields, such as disaster response, agriculture, defense, healthcare, and many others. The drone delivery packing problem (DDPP) arises in the context of logistics in response to increasing demand in the delivery process along with the necessity of lowering human intervention. The DDPP is usually formulated as a combinatorial optimization (CO) problem, aiming to minimize drone usage with specific battery constraints while ensuring timely consistent deliveries with fixed locations and energy budget. In the work I am presenting, we propose two alternative formulations of the DDPP as a quadratic unconstrained binary optimization (QUBO) problem and test the performance of quantum annealing (QA) approaches, as compared to simulated annealing (SA) and classical branch-and-bound global optimization tools. Motivated by the rapidly progressing technological advances realizing upgraded quantum devices, the goal is to identify, with the DDPP as a prototype of a complex CO problem, the advantages and limitations of modern quantum annealing hardware, and the extent to which quantum annealers have the potential to represent a realistic alternative to simulated annealing and deterministic approaches. The talk will start with an introduction aimed at helping the audience understand the basics of quantum annealing technology and optimization methods. April 19, 2024 3:30 PM Main Lecture Hall, ex-ISEF building Miryam Gnazzo GSSI, L'Aquila An oracle for the solution of nearness problems in matrix theory Given a matrix A with a certain property, a matrix nearness problem consists in finding the closest matrix to A that loses that property. Solving nearness problems may be a challenging task and several methods have been developed for the numerical solution of this class of problems. Depending on the property associated with the nearness problem, we may need a different resolution technique. In the first part of the talk, I will provide an introduction to this class of problems, including a way to extend the notion of matrix nearness problems to the setting of matrix polynomials. In the second part, I will describe a new method for the solution of a wide class of nearness problems in matrix theory, including its extension to matrix polynomials. The key idea consists in using an oracle, which can give us information on the solution of the problem, and in combining this output with a Riemannian optimization-based solver. It is also possible to adapt the same methodology to the setting of matrix nearness problems with a prescribed linear structure for the perturbations. A few examples of this possibility will be provided during the talk. March 14, 2024 3:00 PM Auditorium, Rectorate building Shafqat Ali SISSA, Trieste Stabilized reduced basis methods for parametrized viscous flows In this talk I will discuss the stability of the Reduced Basis (RB) Method. In the RB approximation of saddle point problems the Galerkin projection on the reduced space does not guarantee the inf-sup approximation stability even if a stable high fidelity method was used to generate snapshots. For problems in computational fluid dynamics, the lack of inf-sup stability is reflected by the inability to accurately approximate the pressure field. In this context, inf-sup stability is usually recovered through the enrichment of the velocity space with suitable supremizer functions. The main goal of this work is to propose an alternative approach, which relies on the residual based stabilization techniques customarily employed in the Finite Element literature, such as Brezzi-Pitkaranta, Franca-Hughes, streamline upwind Petrov-Galerkin, Galerkin Least Square. In the spirit of offline-online reduced basis computational splitting, two such options are proposed, namely offline-only stabilization and offline-online stabilization. These approaches are then compared to (and combined with) the state of the art supremizer enrichment approach. Numerical results highlight that the proposed methodology allows to obtain smaller reduced basis spaces (i.e., neglecting supremizer enrichment) for which a modified inf-sup stability is still preserved at the reduced order level. January 23, 2024 3:15 PM Library, ex-ISEF building Emanuele Zangrando GSSI, L'Aquila Representations for partially exchangeable arrays of random variables by David J. Aldous The success of deep learning is due, to a large extent, to the remarkable effectiveness of gradient-based optimization methods applied to large neural networks. The purpose of this work is to propose a modern view and a general mathematical framework for loss landscapes and efficient optimization in over-parameterized machine learning models and systems of non-linear equations, a setting that includes over-parameterized deep neural networks. Our starting observation is that optimization landscapes corresponding to such systems are generally not convex, even locally around a global minimum, a condition we call essential non-convexity. We argue that instead they satisfy PL\u204e, a variant of the Polyak-\u0141ojasiewicz condition on most (but not all) of the parameter space, which guarantees both the existence of solutions and efficient optimization by (stochastic) gradient descent (SGD/GD). The PL\u204e condition of these systems is closely related to the condition number of the tangent kernel associated to a non-linear system showing how a PL\u204e-based non-linear theory parallels classical analyses of over-parameterized linear equations. We show that wide neural networks satisfy the PL\u204e condition, which explains the (S)GD convergence to a global minimum. Finally we propose a relaxation of the PL\u204e condition applicable to \u201calmost\u201d over-parameterized systems. Read the paper December 6, 2023 3:15 PM Conference Room, ex-INPS building Anton Savostianov GSSI, L'Aquila Representations for partially exchangeable arrays of random variables by David J. Aldous Consider an array of random variables (Xi,j), 1 \u2264 i,j < \u221e, such that permutations of rows or of columns do not alter the distribution of the array. We show that such an array may be represented as functions f(\u03b1, \u03bei, \u03b7j, \u03bbi,j) of underlying i.i.d. random variables. This result may be useful in characterizing arrays with additional structure. For example, we characterize random matrices whose distribution is invariant under orthogonal rotation, confirming a conjecture of Dawid. Read the paper November 15, 2023 4:00 PM Auditorium, Rectorate building Academic Year 2022/23 Presenter Title, Abstract & Other Info Date Stefano Sicilia GSSI, L'Aquila Why are big data matrices approximately low rank? by Madeleine Udell, Alex Townsend Matrices of (approximate) low rank are pervasive in data science, appearing in movie preferences, text documents, survey data, medical records, and genomics. While there is a vast literature on how to exploit low rank structure in these datasets, there is less attention paid to explaining why the low rank structure appears in the first place. Here, we explain the effectiveness of low rank models in data science by considering a simple generative model for these matrices: we suppose that each row or column is associated to a (possibly high dimensional) bounded latent variable, and entries of the matrix are generated by applying a piecewise analytic function to these latent variables. These matrices are in general full rank. However, we show that we can approximate every entry of an m \\times n matrix drawn from this model to within a fixed absolute error by a low rank matrix whose rank grows as \\scrO (log(m + n)). Hence any sufficiently large matrix from such a latent variable model can be approximated, up to a small entrywise error, by a low rank matrix. Read the paper October 17, 2023 4:00 PM Auditorium, Rectorate building Miryam Gnazzo GSSI, L'Aquila Robust Rational Approximations of Nonlinear Eigenvalue Problems by Stefan Gu\u0308ttel, Gian Maria Negri Porzio, Franc\u0327oise Tisseur We develop algorithms that construct robust (i.e., reliable for a given tolerance and scaling independent) rational approximants of matrix-valued functions on a given subset of the complex plane. We consider matrix-valued functions provided in both split form (i.e., as a sum of scalar functions times constant coefficient matrices) and as a black box form. We develop a new error analysis and use it for the construction of stopping criteria, one for each form. Our criterion for split forms adds weights chosen relative to the importance of each scalar function, leading to the weighted adaptive Antoulas--Anderson (AAA) algorithm, a variant of the set-valued AAA algorithm that can guarantee to return a rational approximant with a user-chosen accuracy. We propose two-phase approaches for black box matrix-valued functions that construct a surrogate AAA approximation in phase one and refine it in phase two, leading to the surrogate AAA algorithm with exact search and the surrogate AAA algorithm with cyclic Leja--Bagby refinement. The stopping criterion for black box matrix-valued functions is updated at each step of phase two to include information from the previous step. When convergence occurs, our two-phase approaches return rational approximants with a user-chosen accuracy. We select problems from the NLEVP collection that represent a variety of matrix-valued functions of different sizes and properties and use them to benchmark our algorithms. Read the paper July 6, 2023 5:00 PM Piero Deidda GSSI, L'Aquila Spectral Decompositions using One-Homogeneous Functionals by Martin Burger, Guy Gilboa, Michael Moeller, Lina Eckardt, Daniel Cremers This paper discusses the use of absolutely one-homogeneous regularization functionals in a variational, scale space, and inverse scale space setting to define a nonlinear spectral decomposition of input data. We present several theoretical results that explain the relation between the different definitions. Additionally, results on the orthogonality of the decomposition, a Parseval-type identity and the notion of generalized (nonlinear) eigenvectors closely link our nonlinear multiscale decompositions to the well-known linear filtering theory. Numerical results are used to illustrate our findings. Read the paper May 31, 2023 5:00 PM Auditorium, Rectorate building Emanuele Natale Universit\u00e9 C\u00f4te d\u2019Azur, France On the Random Subset Sum Problem and Neural Networks The Random Subset Sum Problem (RSSP) is a fundamental problem in mathematical optimization, especially in the understanding of the statistical behavior of integer linear programs. Recently, the theory related to the problem has found applications also in theoretical machine learning, providing key tools for the proof of the Strong Lottery Ticket Hypothesis (SLTH) for dense neural network architectures. In this talk, I will present two recent joint works that push the application of the RSSP further. First, we provide a proof of the SLTH for convolutional neural networks, generalizing the original result. Second, we leverage those ideas to provide a novel way to build neuromorphic hardware. March 23, 2023 5:00 PM Library, ex-ISEF building Arturo De Marinis GSSI, L'Aquila The Forward-Forward Algorithm: Some Preliminary Investigations by Geoffrey Hinton The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth further investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes could be separated in time, the negative passes could be done offline, which would make the learning much simpler in the positive pass and allow video to be pipelined through the network without ever storing activities or stopping to propagate derivatives. Read the paper February 28, 2023 11:00 AM Main Lecture Hall, ex-ISEF building Academic Year 2021/22 Presenter Title, Abstract & Other Info Date Pierluigi Crescenzi GSSI, L'Aquila Proving the Lottery Ticket Hypothesis for Convolutional Neural Networks by Arthur da Cunha, Emanuele Natale, Laurent Viennot The lottery ticket hypothesis states that a randomly-initialized neural network contains a small subnetwork which, when trained in isolation, can compete with the performance of the original network. Recent theoretical works proved an even stronger version: every sufficiently overparameterized (dense) neural network contains a subnetwork that, even without training, achieves accuracy comparable to that of the trained large network. These works left as an open problem to extend the result to convolutional neural networks (CNNs). In this work we provide such generalization by showing that, with high probability, it is possible to approximate any CNN by pruning a random CNN whose size is larger by a logarithmic factor. Read the paper April 27, 2022 4:00 PM Main Lecture Hall, ex-ISEF building Arturo De Marinis GSSI, L'Aquila Structure-preserving deep learning by Elena Celledoni, Matthias J. Ehrhardt, Christian Etmann, Robert I. McLachlan, Brynjulf Owren, Carola-Bibiane Sch\u00f6nlieb, Ferdia Sherry Over the past few years, deep learning has risen to the foreground as a topic of massive interest, mainly as a result of successes obtained in solving large-scale image processing tasks. There are multiple challenging mathematical problems involved in applying deep learning: most deep learning methods require the solution of hard optimisation problems, and a good understanding of the tradeoff between computational effort, amount of data and model complexity is required to successfully design a deep learning approach for a given problem. A large amount of progress made in deep learning has been based on heuristic explorations, but there is a growing effort to mathematically understand the structure in existing deep learning methods and to systematically design new deep learning methods to preserve certain types of structure in deep learning. In this article, we review a number of these directions: some deep neural networks can be understood as discretisations of dynamical systems, neural networks can be designed to have desirable properties such as invertibility or group equivariance, and new algorithmic frameworks based on conformal Hamiltonian systems and Riemannian manifolds to solve the optimisation problems have been proposed. We conclude our review of each of these topics by discussing some open problems that we consider to be interesting directions for future research. Read the paper April 6, 2022 Giuseppe Lipardi GSSI, L'Aquila Computing semiclassical quantum dynamics with Hagedorn wavepackets by Erwan Faou, Vasile Gradinaru, Christian Lubich We consider the approximation of multiparticle quantum dynamics in the semiclassical regime by Hagedorn wavepackets, which are products of complex Gaussians with polynomials that form an orthonormal $L^2$ basis and preserve their type under propagation in Schr\u00f6dinger equations with quadratic potentials. We build a fully explicit, time-reversible time-stepping algorithm to approximate the solution of the Hagedorn wavepacket dynamics. The algorithm is based on a splitting between the kinetic and potential part of the Hamiltonian operator, as well as on a splitting of the potential into its local quadratic approximation and the remainder. The algorithm is robust in the semiclassical limit. It reduces to the Strang splitting of the Schr\u00f6dinger equation in the limit of the full basis set, and it advances positions and momenta by the St\u00f6rmer\u2013Verlet method for the classical equations of motion. The algorithm allows for the treatment of multiparticle problems by thinning out the basis according to a hyperbolic cross approximation and of high-dimensional problems by Hartree-type approximations in a moving coordinate frame. Read the paper March 30, 2022 3:00 PM Martino Caliaro GSSI, L'Aquila Stability analysis of a chain of non-identical vehicles under bilateral cruise control by Berthold Horn Bilateral cruise control (BCC) suppresses traffic flow instabilities. Previously, for simplicity of analysis, vehicles in BCC traffic flow were assumed to be identical, i.e., using the same gains for control. In this study, we analyze the stability of an inhomogeneous vehicular chain in which the gains used by different vehicles are not the same. Not unexpectedly, mathematical analysis becomes more difficult, and leads to a quadratic eigenvalue problem. We study several different cases, and shows that a chain of vehicles under bilateral cruise control is stable even when the vehicles do not all have the same control system properties. Numerical simulations validate the analysis. Read the paper March 23, 2022 4:00 PM Vishnu Sanjay GSSI, L'Aquila Hodge Laplacians on Graphs by Lek-Heng Lim This is an elementary introduction to the Hodge Laplacian on a graph, a higher-order generalization of the graph Laplacian. We will discuss basic properties including cohomology and Hodge theory. The main feature of our approach is simplicity, requiring only knowledge of linear algebra and graph theory. We have also isolated the algebra from the topology to show that a large part of cohomology and Hodge theory is nothing more than the linear algebra of matrices satisfying AB=0. For the remaining topological aspect, we cast our discussions entirely in terms of graphs as opposed to less-familiar topological objects like simplicial complexes. Read the paper March 16, 2022 4:00 PM Helena Biscevic GSSI, L'Aquila Time-symmetric integration in astrophysics by David M. Hernandez, Edmund Bertschinger Calculating the long term solution of ordinary differential equations, such as those of the N-body problem, is central to understanding a wide range of dynamics in astrophysics, from galaxy formation to planetary chaos. Because generally no analytic solution exists to these equations, researchers rely on numerical methods which are prone to various errors. In an effort to mitigate these errors, powerful symplectic integrators have been employed. But symplectic integrators can be severely limited because they are not compatible with adaptive stepping and thus they have difficulty accommodating changing time and length scales. A promising alternative is time-reversible integration, which can handle adaptive time stepping, but the errors due to time-reversible integration in astrophysics are less understood. The goal of this work is to study analytically and numerically the errors caused by time-reversible integration, with and without adaptive stepping. We derive the modified differential equations of these integrators to perform the error analysis. As an example, we consider the trapezoidal rule, a reversible non-symplectic integrator, and show it gives secular energy error increase for a pendulum problem and for a H\u00e9non---Heiles orbit. We conclude that using reversible integration does not guarantee good energy conservation and that, when possible, use of symplectic integrators is favored. We also show that time-symmetry and time-reversibility are properties that are distinct for an integrator. Read the paper March 8, 2022 2:00 PM Pierpaolo Bilotto GSSI, L'Aquila On the spectra of general random graphs by Fan Chung, Mary Radcliffe We consider random graphs such that each edge is determined by an independent random variable, where the probability of each edge is not assumed to be equal. We use a Chernoff inequality for matrices to show that the eigenvalues of the adjacency matrix and the normalized Laplacian of such a random graph can be approximated by those of the weighted expectation graph, with error bounds dependent upon the minimum and maximum expected degrees. In particular, we use these results to bound the spectra of random graphs with given expected degree sequences, including random power law graphs. Moreover, we prove a similar result giving concentration of the spectrum of a matrix martingale on its expectation. Read the paper March 2, 2022 4:00 PM Mattia Manucci GSSI, L'Aquila Physics-informed machine learning for reduced-order modeling of nonlinear problems by Wenqian Chen, Qian Wang, Jan S. Hesthaven, Chuhua Zhang A reduced basis method based on a physics-informed machine learning framework is developed for efficient reduced-order modeling of parametrized partial differential equations (PDEs). A feedforward neural network is used to approximate the mapping from the time-parameter to the reduced coefficients. During the offline stage, the network is trained by minimizing the weighted sum of the residual loss of the reduced-order equations, and the data loss of the labeled reduced coefficients that are obtained via the projection of high-fidelity snapshots onto the reduced space. Such a network is referred to as physics-reinforced neural network (PRNN). As the number of residual points in time-parameter space can be very large, an accurate network \u2013 referred to as physics-informed neural network (PINN) \u2013 can be trained by minimizing only the residual loss. However, for complex nonlinear problems, the solution of the reduced-order equation is less accurate than the projection of high-fidelity solution onto the reduced space. Therefore, the PRNN trained with the snapshot data is expected to have higher accuracy than the PINN. Numerical results demonstrate that the PRNN is more accurate than the PINN and a purely data-driven neural network for complex problems. During the reduced basis refinement, the PRNN may obtain higher accuracy than the direct reduced-order model based on a Galerkin projection. The online evaluation of PINN/PRNN is orders of magnitude faster than that of the Galerkin reduced-order model. Read the paper February 23, 2022 5:00 PM Tommaso Tonolo GSSI, L'Aquila Mean Field Analysis of Hypergraph Contagion Model by Desmond J. Higham, Henry-Louis de Kergorlay We typically interact in groups, not just in pairs. For this reason, it has recently been proposed that the spread of information, opinion or disease should be modelled over a hypergraph rather than a standard graph. The use of hyperedges naturally allows for a nonlinear rate of transmission, in terms of both the group size and the number of infected group members, as is the case, for example, when social distancing is encouraged. We consider a general class of individual-level, stochastic, susceptible-infected-susceptible models on a hypergraph, and focus on a mean field approximation proposed in [Arruda et al., Phys. Rev. Res., 2020]. We derive spectral conditions under which the mean field model predicts local or global stability of the infection-free state. We also compare these results with (a) a new condition that we derive for decay to zero in mean for the exact process, (b) conditions for a different mean field approximation in [Higham and de Kergorlay, Proc. Roy. Soc. A, 2021], and (c) numerical simulations of the microscale model. Read the paper February 23, 2022 4:00 PM Arturo De Marinis GSSI, L'Aquila Regularized nonlinear acceleration by Damien Scieur, Alexandre d'Aspremont, Francis Bach We describe a convergence acceleration technique for unconstrained optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method. The weights in this average are computed via a simple linear system, whose solution can be updated online. This acceleration scheme runs in parallel to the base algorithm, providing improved estimates of the solution on the fly, while the original optimization method is running. Numerical experiments are detailed on classical classification problems. Read the paper February 9, 2022 5:00 PM Konstantin Prokopchik GSSI, L'Aquila Consensus Dynamics and Opinion Formation on Hypergraphs by Leonie Neuh\u00e4user, Renaud Lambiotte, Michael T. Schaub In this chapter, we derive and analyse models for consensus dynamics on hypergraphs. As we discuss, unless there are nonlinear node interaction functions, it is always possible to rewrite the system in terms of a new network of effective pairwise node interactions, regardless of the initially underlying multi-way interaction structure. We thus focus on dynamics based on a certain class of non-linear interaction functions, which can model different sociological phenomena such as peer pressure and stubbornness. Unlike for linear consensus dynamics on networks, we show how our nonlinear model dynamics can cause shifts away from the average system state. We examine how these shifts are influenced by the distribution of the initial states, the underlying hypergraph structure and different forms of non-linear scaling of the node interaction function. Read the paper January 26, 2022 5:00 PM Main Lecture Hall, ex-ISEF building Simone Fioravanti GSSI, L'Aquila EigenGame: PCA as Nash Equilibrium by Ian Gemp, Brian McWilliams, Claire Vernade, Thore Graepel We present a novel view on principal component analysis (PCA) as a competitive game in which each approximate eigenvector is controlled by a player whose goal is to maximize their own utility function. We analyze the properties of this PCA game and the behavior of its gradient based updates. The resulting algorithm -- which combines elements from Oja's rule with a generalized Gram-Schmidt orthogonalization -- is naturally decentralized and hence parallelizable through message passing. We demonstrate the scalability of the algorithm with experiments on large image datasets and neural network activations. We discuss how this new view of PCA as a differentiable game can lead to further algorithmic developments and insights. Read the paper December 9, 2021 5:00 PM Main Lecture Hall, ex-ISEF building Dayana Savostianova GSSI, L'Aquila Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching by Jonas Geiping, Liam Fowl, W. Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller, Tom Goldstein Data Poisoning attacks modify training data to maliciously control a model trained on such data. In this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a particularly malicious poisoning attack that is both \"from scratch\" and \"clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. Previous poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets. The central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset. Finally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems. Read the paper November 24, 2021 5:00 PM Auditorium, Rectorate building","title":"Reading Group"},{"location":"reading_group/#compile-reading-group","text":"We organize a reading group on COMPutational LEarning , specifically on topics related to Numerics/Matrix Theory/Graph Theory with particular emphasis on their application/connection with Machine Learning, Data Mining and Network Science. We meet regularly to read a paper of common interest, together, with the goal of creating some motivation for staying up to date with the new papers or to catch up with some papers that you wanted to read but postponed for some time. Recently, as an alternative, we have decided to let people present their own research work, so that everyone in the group knows what the others are doing, with the aim of fostering exchange of ideas and collaborations. If you are interested in participating please complete the form below to be to be notified about the meetings: Google Form: Subscription to Reading Group mailing list If you have suggestions for papers to ready you can propose your ideas here: Google Form: Suggestions for papers to read If you want inspiration for a possible paper to read, here is a list of the papers that have been suggested so far: Suggested papers for COMPiLE Reading Group The reading group is organized by our early career group members and supervised/coordinated by Nicola Guglielmi and Francesco Tudisco . Organizer Period Bernardo Collufio , GSSI, L'Aquila December 2024 -- Present Piero Deidda , GSSI, L'Aquila January 2024 -- Present Arturo De Marinis , GSSI, L'Aquila February 2023 -- February 2025 Dayana Savostianova , GSSI, L'Aquila November 2021 -- Summer 2022","title":"COMPiLE Reading Group"},{"location":"reading_group/#academic-year-202425","text":"Presenter Title, Abstract & Other Info Date & Venue Rinat Kamalov GSSI, L'Aquila Low-Rank Approximation of Matrices and Tensors in the Chebyshev Norm Abstract April 9, 2025 5:15 PM Room C, ex-ISEF building Pietro Sittoni GSSI, L'Aquila Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach and Its Implications for Neural PDE Solvers by Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Tom Goldstein This presentation explores a novel language model architecture that scales test-time computation by implicitly reasoning in latent space. The model achieves this by iterating a recurrent block, allowing it to unroll to an arbitrary depth at test time. This approach contrasts with mainstream reasoning models, which scale computation by generating more tokens. Furthermore, it does not require specialized training data, unlike chain-of-thought-based methods. Finally, we will discuss its potential implications for neural PDE solvers. Read the paper March 3, 2025 3:00 PM Main Lecture Hall, ex-ISEF building Th\u00e9ophile Dolmaire Universit\u00e0 degli Studi dell'Aquila, L'Aquila Inelastic collapse in dimension d = 1: motivation, results, and open questions The inelastic Boltzmann equation describes granular media, composed of a large number of particles that collide inelastically with each other, dissipating kinetic energy at each collision (such as sand, snow or interstellar dust). Depending on the degree of inelasticity of the collisions, various fascinating phenomena emerge, such as onset of inhomogeneities. Nevertheless, the derivation of this equation is still an open mathematical problem. One major difficulty, already at the level of the particle system, comes from the phenomenon of inelastic collapse, when infinitely many collisions take place in finite time. We will consider inelastic hard spheres, with fixed restitution coefficient r. In the d-dimensional case (d \u2265 2), it has been observed that particles collapse along linear structure, which motivated the study of the one-dimensional particle system. We will present the mathematical approach of the one-dimensional collapse developed in the literature, before turning to the many open questions that remain in this field. We will investigate in much detail the cases of three and four particles. Besides, relying on a new conserved quantity we will prove that the order of the collisions for a system of four particles can be determined by studying a simpler two-dimensional dynamical system. Full abstract Slides of the talk February 18, 2025 5:00 PM Conference Room, ex-INPS building Grigorii (Grisha) Buklei GSSI, L'Aquila Gradient Flows on Graphons by Sewoong Oh, Soumik Pal, Raghav Somani, Raghavendra Tripathi This presentation explores the recently developed framework of gradient flows on graphons, as introduced in the paper \u201cGradient Flows on Graphons: Existence, Convergence, Continuity Equations.\u201d Gradient flows on graphons provide a framework for understanding how large-scale networks evolve over time. Graphons, which represent the limit of dense graphs, provide a powerful way to study graph dynamics using continuous mathematics. We introduce the concept of gradient flows in metric spaces and explain how they can be extended to graphons using the invariant L^2 -metric \\delta_2 . The key results focus on proving the existence and convergence of these flows, showing how they emerge as natural limits of gradient descent on large graphs. Finally, we highlight an intriguing connection to optimal transport, where the graphon metric \\delta_2 behaves similarly to the Wasserstein-2 distance W_2 , suggesting new ways to analyze dynamic networks and large-scale optimization problems. Read the paper Slides of the talk Recording of the talk (available to GSSI only) February 18, 2025 4:00 PM Conference Room, ex-INPS building Bernardo Collufio GSSI, L'Aquila About positive-preserving property of the semi-lagrangian scheme for BGK equation Kinetic equations are well known for their applications in several fields such as gas dynamics, plasma physics, traffic flow, swarming dynamics and socioeconomic modeling. Various numerical methods for solving these equations have been extensively studied over the past few decades; among these, semi-Lagrangian methods have recently gained attention for their good properties in terms of accuracy and stability. Unlike Eulerian methods, which consist of a direct discretization of the equation on an Eulerian grid, semi-Lagrangian schemes make use of the Lagrangian perspective, applying a time scheme over the characteristics and reconstructing the PDF using interpolation techniques; this avoids using the convective operator and results in unconditionally stable schemes allowing for large time steps. However, physically relevant properties of the solution, like positivity, may not always be guaranteed. The positive-preserving property may be crucial for applications in Kinetic Theory since the unknown distribution function represents the density of particles in phase space. Still, no positive-preserving semi-Lagrangian schemes have been proposed in the literature. In this talk I will focus on the BGK equation, which has several applications in collisional gas and plasma dynamics, and after investigating the reasons behind the lack of positivity preservation in current schemes I will demonstrate how this could be enforced by adapting strategies previously applied in the context of Runge-Kutta IMEX methods. February 5, 2025 5:00 PM Main Lecture Hall, ex-ISEF building Arturo De Marinis GSSI, L'Aquila Approximation properties of neural ODEs We study the universal approximation property (UAP) of shallow neural networks whose activation function is defined as the flow of a neural ODE. We prove the UAP for the space of such shallow neural networks in the space of continuous functions. In particular, we also prove the UAP with the weight matrices constrained to have unit norm. Furthermore, we are able to bound from above the Lipschitz constant of the flow of the neural ODE, that tells us how much a perturbation in input is amplified or shrunk in output. If the upper bound is large, then so it may be the Lipschitz constant, leading to the undesirable situation where certain small perturbations in input cause large changes in output. Therefore, we compute a perturbation to the weight matrix of the neural ODE such that the flow of the perturbed neural ODE has Lipschitz constant bounded from above as we desire. This leads to a stable flow and so to a stable shallow neural network. However, the stabilized shallow neural network with unit norm weight matrices does not satisfy the universal approximation property anymore. Nevertheless, we are able to prove approximation bounds that tell us how poorly and how accurately a continuous target function can be approximated by the stabilized shallow neural network. November 27, 2024 5:00 PM Main Lecture Hall, ex-ISEF building","title":"Academic Year 2024/25"},{"location":"reading_group/#academic-year-202324","text":"Presenter Title, Abstract & Other Info Date Angelo Alberto Casulli GSSI, L'Aquila Learning Poisson Equation with Rank-Structured Matrices We explore how rank-structured matrices can be leveraged to efficiently represent and learn elliptic operators, with a focus on the 1D Poisson equation as a model problem. We begin by introducing key concepts in numerical linear algebra, including the use of rank-structured matrices to approximate the discretized Poisson operator. The talk then delves into techniques from randomized linear algebra, showing how these methods can be applied to recover rank-structured matrices from matrix-vector products. This approach enables the reconstruction of the Poisson operator\u2019s discretization by solving only a small number of Poisson equations. Finally, we extend the discussion to the 2D Poisson problem, highlighting the additional challenges that arise in this more complex setting. October 23, 202 4:00 PM Main Lecture Hall, ex-ISEF building Francesco Fabbri GSSI, L'Aquila \u03b2-Variational Autoencoder Graph Neural Network for Synthetic Generation of Abdominal Aorta Aneurisms Synthetic data, particularly in the medical field, is artificially created to replicate real medical data's statistical properties and patterns. This process is essential for addressing privacy concerns, as it allows researchers to work with data that mirrors real patient information without compromising individual privacy. Generative algorithms play a crucial role in this process by leveraging the probability distribution of the original data to create synthetic data through random sampling from features-rich latent spaces. Creating synthetic medical data from a small patient population by extracting key anatomical features offers a viable solution to the challenge of obtaining large databases of real patients. This approach allows for the development of a virtual patient population that encompasses a wide range of anatomical and physiological conditions, facilitating extensive clinical tests and statistical analyses. The medical field increasingly demands patient-specific testing, necessitating extensive validation before adopting new medical processes. While linear approaches are robust, they cannot often capture non-linear features leading to large latent spaces. In contrast, neural network-based reduced-order models can effectively capture these non-linear features with a reduced latent space. Concurrently, the use of generative algorithms and these advanced modeling tools aids in developing effective data generators. In this work, starting from a reduced real-word dataset, we propose a \u03b2-Variational Autoencoder Graph Neural Network-based framework that will allow for the creation of synthetic Abdominal Aorta Aneurisms (AAA) that maintain the statistical correlations and anatomical relationships present in real-world datasets using a small dimension latent space. October 16, 2024 5:30 PM Main Lecture Hall, ex-ISEF building Rinat Kamalov GSSI, L'Aquila Non-Chebyshev Uniform Approximation Consider the problem of the best uniform approximation of a continuous function by linear combinations of a finite system of functions (not necessarily Chebyshev) under arbitrary linear constraints on a convex domain. By modifying the concept of alternance and of the Remez iterative procedure we present a method, which demonstrates its efficiency in numerical problems. Under certain favorable assumptions, we establish the proof of linear convergence rate. Special focus is dedicated to systems of complex exponents, Gaussian functions, Cauchy functions, lacunar algebraic and trigonometric polynomials. We explore applications to signal processing, switching dynamical systems, and to Markov-Bernstein type inequalities. May 3, 2024 3:00 PM Main Lecture Hall, ex-ISEF building Sara Tarquini GSSI, L'Aquila Testing Quantum and Simulated Annealers on the Drone Delivery Packing Problem Using drones to perform human-related tasks can play a key role in various fields, such as disaster response, agriculture, defense, healthcare, and many others. The drone delivery packing problem (DDPP) arises in the context of logistics in response to increasing demand in the delivery process along with the necessity of lowering human intervention. The DDPP is usually formulated as a combinatorial optimization (CO) problem, aiming to minimize drone usage with specific battery constraints while ensuring timely consistent deliveries with fixed locations and energy budget. In the work I am presenting, we propose two alternative formulations of the DDPP as a quadratic unconstrained binary optimization (QUBO) problem and test the performance of quantum annealing (QA) approaches, as compared to simulated annealing (SA) and classical branch-and-bound global optimization tools. Motivated by the rapidly progressing technological advances realizing upgraded quantum devices, the goal is to identify, with the DDPP as a prototype of a complex CO problem, the advantages and limitations of modern quantum annealing hardware, and the extent to which quantum annealers have the potential to represent a realistic alternative to simulated annealing and deterministic approaches. The talk will start with an introduction aimed at helping the audience understand the basics of quantum annealing technology and optimization methods. April 19, 2024 3:30 PM Main Lecture Hall, ex-ISEF building Miryam Gnazzo GSSI, L'Aquila An oracle for the solution of nearness problems in matrix theory Given a matrix A with a certain property, a matrix nearness problem consists in finding the closest matrix to A that loses that property. Solving nearness problems may be a challenging task and several methods have been developed for the numerical solution of this class of problems. Depending on the property associated with the nearness problem, we may need a different resolution technique. In the first part of the talk, I will provide an introduction to this class of problems, including a way to extend the notion of matrix nearness problems to the setting of matrix polynomials. In the second part, I will describe a new method for the solution of a wide class of nearness problems in matrix theory, including its extension to matrix polynomials. The key idea consists in using an oracle, which can give us information on the solution of the problem, and in combining this output with a Riemannian optimization-based solver. It is also possible to adapt the same methodology to the setting of matrix nearness problems with a prescribed linear structure for the perturbations. A few examples of this possibility will be provided during the talk. March 14, 2024 3:00 PM Auditorium, Rectorate building Shafqat Ali SISSA, Trieste Stabilized reduced basis methods for parametrized viscous flows In this talk I will discuss the stability of the Reduced Basis (RB) Method. In the RB approximation of saddle point problems the Galerkin projection on the reduced space does not guarantee the inf-sup approximation stability even if a stable high fidelity method was used to generate snapshots. For problems in computational fluid dynamics, the lack of inf-sup stability is reflected by the inability to accurately approximate the pressure field. In this context, inf-sup stability is usually recovered through the enrichment of the velocity space with suitable supremizer functions. The main goal of this work is to propose an alternative approach, which relies on the residual based stabilization techniques customarily employed in the Finite Element literature, such as Brezzi-Pitkaranta, Franca-Hughes, streamline upwind Petrov-Galerkin, Galerkin Least Square. In the spirit of offline-online reduced basis computational splitting, two such options are proposed, namely offline-only stabilization and offline-online stabilization. These approaches are then compared to (and combined with) the state of the art supremizer enrichment approach. Numerical results highlight that the proposed methodology allows to obtain smaller reduced basis spaces (i.e., neglecting supremizer enrichment) for which a modified inf-sup stability is still preserved at the reduced order level. January 23, 2024 3:15 PM Library, ex-ISEF building Emanuele Zangrando GSSI, L'Aquila Representations for partially exchangeable arrays of random variables by David J. Aldous The success of deep learning is due, to a large extent, to the remarkable effectiveness of gradient-based optimization methods applied to large neural networks. The purpose of this work is to propose a modern view and a general mathematical framework for loss landscapes and efficient optimization in over-parameterized machine learning models and systems of non-linear equations, a setting that includes over-parameterized deep neural networks. Our starting observation is that optimization landscapes corresponding to such systems are generally not convex, even locally around a global minimum, a condition we call essential non-convexity. We argue that instead they satisfy PL\u204e, a variant of the Polyak-\u0141ojasiewicz condition on most (but not all) of the parameter space, which guarantees both the existence of solutions and efficient optimization by (stochastic) gradient descent (SGD/GD). The PL\u204e condition of these systems is closely related to the condition number of the tangent kernel associated to a non-linear system showing how a PL\u204e-based non-linear theory parallels classical analyses of over-parameterized linear equations. We show that wide neural networks satisfy the PL\u204e condition, which explains the (S)GD convergence to a global minimum. Finally we propose a relaxation of the PL\u204e condition applicable to \u201calmost\u201d over-parameterized systems. Read the paper December 6, 2023 3:15 PM Conference Room, ex-INPS building Anton Savostianov GSSI, L'Aquila Representations for partially exchangeable arrays of random variables by David J. Aldous Consider an array of random variables (Xi,j), 1 \u2264 i,j < \u221e, such that permutations of rows or of columns do not alter the distribution of the array. We show that such an array may be represented as functions f(\u03b1, \u03bei, \u03b7j, \u03bbi,j) of underlying i.i.d. random variables. This result may be useful in characterizing arrays with additional structure. For example, we characterize random matrices whose distribution is invariant under orthogonal rotation, confirming a conjecture of Dawid. Read the paper November 15, 2023 4:00 PM Auditorium, Rectorate building","title":"Academic Year 2023/24"},{"location":"reading_group/#academic-year-202223","text":"Presenter Title, Abstract & Other Info Date Stefano Sicilia GSSI, L'Aquila Why are big data matrices approximately low rank? by Madeleine Udell, Alex Townsend Matrices of (approximate) low rank are pervasive in data science, appearing in movie preferences, text documents, survey data, medical records, and genomics. While there is a vast literature on how to exploit low rank structure in these datasets, there is less attention paid to explaining why the low rank structure appears in the first place. Here, we explain the effectiveness of low rank models in data science by considering a simple generative model for these matrices: we suppose that each row or column is associated to a (possibly high dimensional) bounded latent variable, and entries of the matrix are generated by applying a piecewise analytic function to these latent variables. These matrices are in general full rank. However, we show that we can approximate every entry of an m \\times n matrix drawn from this model to within a fixed absolute error by a low rank matrix whose rank grows as \\scrO (log(m + n)). Hence any sufficiently large matrix from such a latent variable model can be approximated, up to a small entrywise error, by a low rank matrix. Read the paper October 17, 2023 4:00 PM Auditorium, Rectorate building Miryam Gnazzo GSSI, L'Aquila Robust Rational Approximations of Nonlinear Eigenvalue Problems by Stefan Gu\u0308ttel, Gian Maria Negri Porzio, Franc\u0327oise Tisseur We develop algorithms that construct robust (i.e., reliable for a given tolerance and scaling independent) rational approximants of matrix-valued functions on a given subset of the complex plane. We consider matrix-valued functions provided in both split form (i.e., as a sum of scalar functions times constant coefficient matrices) and as a black box form. We develop a new error analysis and use it for the construction of stopping criteria, one for each form. Our criterion for split forms adds weights chosen relative to the importance of each scalar function, leading to the weighted adaptive Antoulas--Anderson (AAA) algorithm, a variant of the set-valued AAA algorithm that can guarantee to return a rational approximant with a user-chosen accuracy. We propose two-phase approaches for black box matrix-valued functions that construct a surrogate AAA approximation in phase one and refine it in phase two, leading to the surrogate AAA algorithm with exact search and the surrogate AAA algorithm with cyclic Leja--Bagby refinement. The stopping criterion for black box matrix-valued functions is updated at each step of phase two to include information from the previous step. When convergence occurs, our two-phase approaches return rational approximants with a user-chosen accuracy. We select problems from the NLEVP collection that represent a variety of matrix-valued functions of different sizes and properties and use them to benchmark our algorithms. Read the paper July 6, 2023 5:00 PM Piero Deidda GSSI, L'Aquila Spectral Decompositions using One-Homogeneous Functionals by Martin Burger, Guy Gilboa, Michael Moeller, Lina Eckardt, Daniel Cremers This paper discusses the use of absolutely one-homogeneous regularization functionals in a variational, scale space, and inverse scale space setting to define a nonlinear spectral decomposition of input data. We present several theoretical results that explain the relation between the different definitions. Additionally, results on the orthogonality of the decomposition, a Parseval-type identity and the notion of generalized (nonlinear) eigenvectors closely link our nonlinear multiscale decompositions to the well-known linear filtering theory. Numerical results are used to illustrate our findings. Read the paper May 31, 2023 5:00 PM Auditorium, Rectorate building Emanuele Natale Universit\u00e9 C\u00f4te d\u2019Azur, France On the Random Subset Sum Problem and Neural Networks The Random Subset Sum Problem (RSSP) is a fundamental problem in mathematical optimization, especially in the understanding of the statistical behavior of integer linear programs. Recently, the theory related to the problem has found applications also in theoretical machine learning, providing key tools for the proof of the Strong Lottery Ticket Hypothesis (SLTH) for dense neural network architectures. In this talk, I will present two recent joint works that push the application of the RSSP further. First, we provide a proof of the SLTH for convolutional neural networks, generalizing the original result. Second, we leverage those ideas to provide a novel way to build neuromorphic hardware. March 23, 2023 5:00 PM Library, ex-ISEF building Arturo De Marinis GSSI, L'Aquila The Forward-Forward Algorithm: Some Preliminary Investigations by Geoffrey Hinton The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth further investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes could be separated in time, the negative passes could be done offline, which would make the learning much simpler in the positive pass and allow video to be pipelined through the network without ever storing activities or stopping to propagate derivatives. Read the paper February 28, 2023 11:00 AM Main Lecture Hall, ex-ISEF building","title":"Academic Year 2022/23"},{"location":"reading_group/#academic-year-202122","text":"Presenter Title, Abstract & Other Info Date Pierluigi Crescenzi GSSI, L'Aquila Proving the Lottery Ticket Hypothesis for Convolutional Neural Networks by Arthur da Cunha, Emanuele Natale, Laurent Viennot The lottery ticket hypothesis states that a randomly-initialized neural network contains a small subnetwork which, when trained in isolation, can compete with the performance of the original network. Recent theoretical works proved an even stronger version: every sufficiently overparameterized (dense) neural network contains a subnetwork that, even without training, achieves accuracy comparable to that of the trained large network. These works left as an open problem to extend the result to convolutional neural networks (CNNs). In this work we provide such generalization by showing that, with high probability, it is possible to approximate any CNN by pruning a random CNN whose size is larger by a logarithmic factor. Read the paper April 27, 2022 4:00 PM Main Lecture Hall, ex-ISEF building Arturo De Marinis GSSI, L'Aquila Structure-preserving deep learning by Elena Celledoni, Matthias J. Ehrhardt, Christian Etmann, Robert I. McLachlan, Brynjulf Owren, Carola-Bibiane Sch\u00f6nlieb, Ferdia Sherry Over the past few years, deep learning has risen to the foreground as a topic of massive interest, mainly as a result of successes obtained in solving large-scale image processing tasks. There are multiple challenging mathematical problems involved in applying deep learning: most deep learning methods require the solution of hard optimisation problems, and a good understanding of the tradeoff between computational effort, amount of data and model complexity is required to successfully design a deep learning approach for a given problem. A large amount of progress made in deep learning has been based on heuristic explorations, but there is a growing effort to mathematically understand the structure in existing deep learning methods and to systematically design new deep learning methods to preserve certain types of structure in deep learning. In this article, we review a number of these directions: some deep neural networks can be understood as discretisations of dynamical systems, neural networks can be designed to have desirable properties such as invertibility or group equivariance, and new algorithmic frameworks based on conformal Hamiltonian systems and Riemannian manifolds to solve the optimisation problems have been proposed. We conclude our review of each of these topics by discussing some open problems that we consider to be interesting directions for future research. Read the paper April 6, 2022 Giuseppe Lipardi GSSI, L'Aquila Computing semiclassical quantum dynamics with Hagedorn wavepackets by Erwan Faou, Vasile Gradinaru, Christian Lubich We consider the approximation of multiparticle quantum dynamics in the semiclassical regime by Hagedorn wavepackets, which are products of complex Gaussians with polynomials that form an orthonormal $L^2$ basis and preserve their type under propagation in Schr\u00f6dinger equations with quadratic potentials. We build a fully explicit, time-reversible time-stepping algorithm to approximate the solution of the Hagedorn wavepacket dynamics. The algorithm is based on a splitting between the kinetic and potential part of the Hamiltonian operator, as well as on a splitting of the potential into its local quadratic approximation and the remainder. The algorithm is robust in the semiclassical limit. It reduces to the Strang splitting of the Schr\u00f6dinger equation in the limit of the full basis set, and it advances positions and momenta by the St\u00f6rmer\u2013Verlet method for the classical equations of motion. The algorithm allows for the treatment of multiparticle problems by thinning out the basis according to a hyperbolic cross approximation and of high-dimensional problems by Hartree-type approximations in a moving coordinate frame. Read the paper March 30, 2022 3:00 PM Martino Caliaro GSSI, L'Aquila Stability analysis of a chain of non-identical vehicles under bilateral cruise control by Berthold Horn Bilateral cruise control (BCC) suppresses traffic flow instabilities. Previously, for simplicity of analysis, vehicles in BCC traffic flow were assumed to be identical, i.e., using the same gains for control. In this study, we analyze the stability of an inhomogeneous vehicular chain in which the gains used by different vehicles are not the same. Not unexpectedly, mathematical analysis becomes more difficult, and leads to a quadratic eigenvalue problem. We study several different cases, and shows that a chain of vehicles under bilateral cruise control is stable even when the vehicles do not all have the same control system properties. Numerical simulations validate the analysis. Read the paper March 23, 2022 4:00 PM Vishnu Sanjay GSSI, L'Aquila Hodge Laplacians on Graphs by Lek-Heng Lim This is an elementary introduction to the Hodge Laplacian on a graph, a higher-order generalization of the graph Laplacian. We will discuss basic properties including cohomology and Hodge theory. The main feature of our approach is simplicity, requiring only knowledge of linear algebra and graph theory. We have also isolated the algebra from the topology to show that a large part of cohomology and Hodge theory is nothing more than the linear algebra of matrices satisfying AB=0. For the remaining topological aspect, we cast our discussions entirely in terms of graphs as opposed to less-familiar topological objects like simplicial complexes. Read the paper March 16, 2022 4:00 PM Helena Biscevic GSSI, L'Aquila Time-symmetric integration in astrophysics by David M. Hernandez, Edmund Bertschinger Calculating the long term solution of ordinary differential equations, such as those of the N-body problem, is central to understanding a wide range of dynamics in astrophysics, from galaxy formation to planetary chaos. Because generally no analytic solution exists to these equations, researchers rely on numerical methods which are prone to various errors. In an effort to mitigate these errors, powerful symplectic integrators have been employed. But symplectic integrators can be severely limited because they are not compatible with adaptive stepping and thus they have difficulty accommodating changing time and length scales. A promising alternative is time-reversible integration, which can handle adaptive time stepping, but the errors due to time-reversible integration in astrophysics are less understood. The goal of this work is to study analytically and numerically the errors caused by time-reversible integration, with and without adaptive stepping. We derive the modified differential equations of these integrators to perform the error analysis. As an example, we consider the trapezoidal rule, a reversible non-symplectic integrator, and show it gives secular energy error increase for a pendulum problem and for a H\u00e9non---Heiles orbit. We conclude that using reversible integration does not guarantee good energy conservation and that, when possible, use of symplectic integrators is favored. We also show that time-symmetry and time-reversibility are properties that are distinct for an integrator. Read the paper March 8, 2022 2:00 PM Pierpaolo Bilotto GSSI, L'Aquila On the spectra of general random graphs by Fan Chung, Mary Radcliffe We consider random graphs such that each edge is determined by an independent random variable, where the probability of each edge is not assumed to be equal. We use a Chernoff inequality for matrices to show that the eigenvalues of the adjacency matrix and the normalized Laplacian of such a random graph can be approximated by those of the weighted expectation graph, with error bounds dependent upon the minimum and maximum expected degrees. In particular, we use these results to bound the spectra of random graphs with given expected degree sequences, including random power law graphs. Moreover, we prove a similar result giving concentration of the spectrum of a matrix martingale on its expectation. Read the paper March 2, 2022 4:00 PM Mattia Manucci GSSI, L'Aquila Physics-informed machine learning for reduced-order modeling of nonlinear problems by Wenqian Chen, Qian Wang, Jan S. Hesthaven, Chuhua Zhang A reduced basis method based on a physics-informed machine learning framework is developed for efficient reduced-order modeling of parametrized partial differential equations (PDEs). A feedforward neural network is used to approximate the mapping from the time-parameter to the reduced coefficients. During the offline stage, the network is trained by minimizing the weighted sum of the residual loss of the reduced-order equations, and the data loss of the labeled reduced coefficients that are obtained via the projection of high-fidelity snapshots onto the reduced space. Such a network is referred to as physics-reinforced neural network (PRNN). As the number of residual points in time-parameter space can be very large, an accurate network \u2013 referred to as physics-informed neural network (PINN) \u2013 can be trained by minimizing only the residual loss. However, for complex nonlinear problems, the solution of the reduced-order equation is less accurate than the projection of high-fidelity solution onto the reduced space. Therefore, the PRNN trained with the snapshot data is expected to have higher accuracy than the PINN. Numerical results demonstrate that the PRNN is more accurate than the PINN and a purely data-driven neural network for complex problems. During the reduced basis refinement, the PRNN may obtain higher accuracy than the direct reduced-order model based on a Galerkin projection. The online evaluation of PINN/PRNN is orders of magnitude faster than that of the Galerkin reduced-order model. Read the paper February 23, 2022 5:00 PM Tommaso Tonolo GSSI, L'Aquila Mean Field Analysis of Hypergraph Contagion Model by Desmond J. Higham, Henry-Louis de Kergorlay We typically interact in groups, not just in pairs. For this reason, it has recently been proposed that the spread of information, opinion or disease should be modelled over a hypergraph rather than a standard graph. The use of hyperedges naturally allows for a nonlinear rate of transmission, in terms of both the group size and the number of infected group members, as is the case, for example, when social distancing is encouraged. We consider a general class of individual-level, stochastic, susceptible-infected-susceptible models on a hypergraph, and focus on a mean field approximation proposed in [Arruda et al., Phys. Rev. Res., 2020]. We derive spectral conditions under which the mean field model predicts local or global stability of the infection-free state. We also compare these results with (a) a new condition that we derive for decay to zero in mean for the exact process, (b) conditions for a different mean field approximation in [Higham and de Kergorlay, Proc. Roy. Soc. A, 2021], and (c) numerical simulations of the microscale model. Read the paper February 23, 2022 4:00 PM Arturo De Marinis GSSI, L'Aquila Regularized nonlinear acceleration by Damien Scieur, Alexandre d'Aspremont, Francis Bach We describe a convergence acceleration technique for unconstrained optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method. The weights in this average are computed via a simple linear system, whose solution can be updated online. This acceleration scheme runs in parallel to the base algorithm, providing improved estimates of the solution on the fly, while the original optimization method is running. Numerical experiments are detailed on classical classification problems. Read the paper February 9, 2022 5:00 PM Konstantin Prokopchik GSSI, L'Aquila Consensus Dynamics and Opinion Formation on Hypergraphs by Leonie Neuh\u00e4user, Renaud Lambiotte, Michael T. Schaub In this chapter, we derive and analyse models for consensus dynamics on hypergraphs. As we discuss, unless there are nonlinear node interaction functions, it is always possible to rewrite the system in terms of a new network of effective pairwise node interactions, regardless of the initially underlying multi-way interaction structure. We thus focus on dynamics based on a certain class of non-linear interaction functions, which can model different sociological phenomena such as peer pressure and stubbornness. Unlike for linear consensus dynamics on networks, we show how our nonlinear model dynamics can cause shifts away from the average system state. We examine how these shifts are influenced by the distribution of the initial states, the underlying hypergraph structure and different forms of non-linear scaling of the node interaction function. Read the paper January 26, 2022 5:00 PM Main Lecture Hall, ex-ISEF building Simone Fioravanti GSSI, L'Aquila EigenGame: PCA as Nash Equilibrium by Ian Gemp, Brian McWilliams, Claire Vernade, Thore Graepel We present a novel view on principal component analysis (PCA) as a competitive game in which each approximate eigenvector is controlled by a player whose goal is to maximize their own utility function. We analyze the properties of this PCA game and the behavior of its gradient based updates. The resulting algorithm -- which combines elements from Oja's rule with a generalized Gram-Schmidt orthogonalization -- is naturally decentralized and hence parallelizable through message passing. We demonstrate the scalability of the algorithm with experiments on large image datasets and neural network activations. We discuss how this new view of PCA as a differentiable game can lead to further algorithmic developments and insights. Read the paper December 9, 2021 5:00 PM Main Lecture Hall, ex-ISEF building Dayana Savostianova GSSI, L'Aquila Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching by Jonas Geiping, Liam Fowl, W. Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller, Tom Goldstein Data Poisoning attacks modify training data to maliciously control a model trained on such data. In this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a particularly malicious poisoning attack that is both \"from scratch\" and \"clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. Previous poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets. The central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset. Finally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems. Read the paper November 24, 2021 5:00 PM Auditorium, Rectorate building","title":"Academic Year 2021/22"},{"location":"research/","text":"Research & PhD Projects The research interests of the group include: Numerical integration of ODEs Numerical solution of nonlinear eigenvector problems Numerical linear algebra Computational machine learning and data science Spectral graph theory Numerical Optimization Analysis of complex networks and their applications to a wide range of real-world problems. Some specific topics of interest are described below. Efficient training of stable implicit-depth neural networks Supervised and semi-supervised classification are among the most important tasks in machine learning and data mining. The state-of-the-art techniques for both these two tasks are based on deep Neural Networks (NNs) or graph NNs. From a mathematical viewpoint, these models boil down to a regularized constrained optimization problem, $$\\begin{cases} \\min \\ell(f(x),y) + \\lambda \\varphi(f) & \\newline \\text{such that } f\\in \\mathcal F:=\\{f:f(x)=\\sigma(W_k\\sigma(\\cdots \\sigma(W_1\\sigma(W_0 x))\\cdots))\\} & \\end{cases}$$ where the constraints form a function space \\( \\mathcal F \\) parametrized by structured matrices \\(W = (W_0,\\dots,W_k)\\), including Toeplitz and multilevel Toeplitz matrices (convolutional operators) and graph or hypergraph matrices (discrete Laplacian operators and graph-convolutional filters). Recent work has shown the advantages of so-called implicit-depth or continuous-time neural networks, where the constraints in the NN optimization problem are defined via a nonlinear system of ODEs or PDEs, i.e. for instance we replace \\(f\\in \\mathcal F\\) with a differential equation of the form \\( \\dot f = f(W,t)\\). Popular instances of this type of NN models are so-called Neural ODE and Deep Equilibrium Models. These approaches, which trace back to early work on recurrent backpropagation, have a number of advantages: (a) they have been recently shown to match or even exceed the performance of traditional NNs on time series and image data (e.g. sequence modeling); (b) memory-wise, they are drastically more efficient than traditional NNs as backpropagation is done analytically and does not require to store internal weights, allowing to efficiently handle deeper architectures. While prediction accuracy of NN-based models has reached extraordinary performance, their potential instability with respect to adversarial attacks is a major limitation and one of the most important challenges in the field. Having guarantees of robustness against adversarial attacks is an essential requirement for the application of NNs in the society. In an unstable dynamical system, small perturbations of the initial state (the training dataset) may lead to very different solutions, thus a stable continuous-time NN architecture is an essential requirement for robustness against adversarial attacks. The obvious approach to ensure stability is to restrict the constraints set (the structure of the admissible functions), for example by limiting the matrix parameters to skew-Hermitian convolutional operators of the form $$ W = -BB^\\top,\\qquad W = \\begin{bmatrix} & B^\\top \\newline -B & \\end{bmatrix}\\, . $$ However, this approach further constrains the admissible functions set and reduces the applicability of the NN classifier and its classification performance. Leveraging recent techniques from matrix stability theory, we aim at developing new training algorithms for implicit-depth NN which will ensure forward (and, subsequently, backward) stability with the least reduction in terms of admissible constraint parameters. Methods from numerical ODEs and numerical linear algebra will be used to handle the differential nature of the problem and the large structured matrices involved as well as to prove the convergence of the training algorithm. Our numerical schemes will take advantage of the structure of convolutional operators and graph matrices to ensure efficiency in the training process and performance in terms of classification accuracy. Numerical integration of nonsmooth dynamical systems The numerical treatment of differential equations usually relies on smoothness assumptions which play a fundamental role in the development of suitable schemes and in their convergence analysis. However, an emergent field of interest is systems of ODEs with discontinuous right hand side and many applications can be found in control theory (bang-bang controls), mechanical systems (dry friction) biology (gene regulatory networks), chemistry (gas liquid models). In most mathematical models described by discontinuous ODEs the vector field is piecewise smooth in subregions separated by manifolds and discontinuities take place across the manifolds. The existence and uniqueness of solutions are not anymore guaranteed at such interfaces and the classical Filippov theory becomes ambiguous when the discontinuity manifolds have co-dimension greater than 1. Infinitely many vector fields could be selected along the discontinuity manifolds and the corresponding solutions might have different qualitative behavior. This issue clearly affects also the numerical approximation of the solutions. Our aim is to investigate physical regularizations so to allow one to select a Filippov solution that has physical relevance. If we can show that the solutions of the regularized system converge uniformly to a particular Filippov solution, then we can select this solution for the discontinuous problem. The persistence of the qualitative behavior of solutions under regularization is fundamental if we wish to replace the discontinuous system with the regular one or viceversa, hence we will investigate this persistence as well. At the state of the art, numerical techniques for discontinuous problems, even though efficient, need to select a priori a sliding vector field in Filippov convex combination. Our studies would give rigorous theoretical justifications to make such selection in agreement with the original real life model that one wishes to simulate. Our research project focuses on getting new theoretical insight into numerical approaches and on exploiting this insight for the design and implementation of efficient algorithms. Computational network science Networks are a very powerful tool to model complex systems and analyze large data. For example, in exploratory data analysis we are often interested in finding clusters or communities, or assessing the importance (or centrality) of datapoints. As many real-world systems feature geographical, temporal, or categorical metadata and higher-order structures (motifs), models based on higher-order graphs such as hypergraphs, multilayer graphs and simplicial complexes are nowadays becoming prevalent. We are interested in developing both theoretical foundations and efficient algorithms for a range of network analysis problems. We are particularly interested in methods that exploit matrix and tensor representations of the data to approximately solve related combinatorial optimization problems such as community detection, clustering, core-periphery analysis. Spectral methods for machine learning Spectral methods play a fundamental role in machine learning, statistics, and data mining. Methods for important tasks such as clustering, semi-supervised learning, dimensionality reduction, latent factor models, ranking and preference learning and covariance estimation all use information about eigenvalues and eigenvectors (or singular values and singular vectors) from an underlying data matrix. Even though spectral methods are both powerful and convenient, they often rely on a linear approximation of the ideal learning problem which sometimes yields inaccurate results. We are interested in developing spectral methods that are based on nonlinear eigenvalue problems with eigenvector nonlinearities. This class of eigenvector methods is very broad and includes several nonlinear dimensionality reduction models and tight relaxation of cut functions, to name some. Moreover, it allows us to develop numerical linear algebra tools to efficiently perform the nonlinear spectral methods.","title":"Research &amp; PhD Projects"},{"location":"research/#research-phd-projects","text":"The research interests of the group include: Numerical integration of ODEs Numerical solution of nonlinear eigenvector problems Numerical linear algebra Computational machine learning and data science Spectral graph theory Numerical Optimization Analysis of complex networks and their applications to a wide range of real-world problems. Some specific topics of interest are described below.","title":"Research &amp; PhD Projects"},{"location":"research/#efficient-training-of-stable-implicit-depth-neural-networks","text":"Supervised and semi-supervised classification are among the most important tasks in machine learning and data mining. The state-of-the-art techniques for both these two tasks are based on deep Neural Networks (NNs) or graph NNs. From a mathematical viewpoint, these models boil down to a regularized constrained optimization problem, $$\\begin{cases} \\min \\ell(f(x),y) + \\lambda \\varphi(f) & \\newline \\text{such that } f\\in \\mathcal F:=\\{f:f(x)=\\sigma(W_k\\sigma(\\cdots \\sigma(W_1\\sigma(W_0 x))\\cdots))\\} & \\end{cases}$$ where the constraints form a function space \\( \\mathcal F \\) parametrized by structured matrices \\(W = (W_0,\\dots,W_k)\\), including Toeplitz and multilevel Toeplitz matrices (convolutional operators) and graph or hypergraph matrices (discrete Laplacian operators and graph-convolutional filters). Recent work has shown the advantages of so-called implicit-depth or continuous-time neural networks, where the constraints in the NN optimization problem are defined via a nonlinear system of ODEs or PDEs, i.e. for instance we replace \\(f\\in \\mathcal F\\) with a differential equation of the form \\( \\dot f = f(W,t)\\). Popular instances of this type of NN models are so-called Neural ODE and Deep Equilibrium Models. These approaches, which trace back to early work on recurrent backpropagation, have a number of advantages: (a) they have been recently shown to match or even exceed the performance of traditional NNs on time series and image data (e.g. sequence modeling); (b) memory-wise, they are drastically more efficient than traditional NNs as backpropagation is done analytically and does not require to store internal weights, allowing to efficiently handle deeper architectures. While prediction accuracy of NN-based models has reached extraordinary performance, their potential instability with respect to adversarial attacks is a major limitation and one of the most important challenges in the field. Having guarantees of robustness against adversarial attacks is an essential requirement for the application of NNs in the society. In an unstable dynamical system, small perturbations of the initial state (the training dataset) may lead to very different solutions, thus a stable continuous-time NN architecture is an essential requirement for robustness against adversarial attacks. The obvious approach to ensure stability is to restrict the constraints set (the structure of the admissible functions), for example by limiting the matrix parameters to skew-Hermitian convolutional operators of the form $$ W = -BB^\\top,\\qquad W = \\begin{bmatrix} & B^\\top \\newline -B & \\end{bmatrix}\\, . $$ However, this approach further constrains the admissible functions set and reduces the applicability of the NN classifier and its classification performance. Leveraging recent techniques from matrix stability theory, we aim at developing new training algorithms for implicit-depth NN which will ensure forward (and, subsequently, backward) stability with the least reduction in terms of admissible constraint parameters. Methods from numerical ODEs and numerical linear algebra will be used to handle the differential nature of the problem and the large structured matrices involved as well as to prove the convergence of the training algorithm. Our numerical schemes will take advantage of the structure of convolutional operators and graph matrices to ensure efficiency in the training process and performance in terms of classification accuracy.","title":"Efficient training of stable implicit-depth neural networks"},{"location":"research/#numerical-integration-of-nonsmooth-dynamical-systems","text":"The numerical treatment of differential equations usually relies on smoothness assumptions which play a fundamental role in the development of suitable schemes and in their convergence analysis. However, an emergent field of interest is systems of ODEs with discontinuous right hand side and many applications can be found in control theory (bang-bang controls), mechanical systems (dry friction) biology (gene regulatory networks), chemistry (gas liquid models). In most mathematical models described by discontinuous ODEs the vector field is piecewise smooth in subregions separated by manifolds and discontinuities take place across the manifolds. The existence and uniqueness of solutions are not anymore guaranteed at such interfaces and the classical Filippov theory becomes ambiguous when the discontinuity manifolds have co-dimension greater than 1. Infinitely many vector fields could be selected along the discontinuity manifolds and the corresponding solutions might have different qualitative behavior. This issue clearly affects also the numerical approximation of the solutions. Our aim is to investigate physical regularizations so to allow one to select a Filippov solution that has physical relevance. If we can show that the solutions of the regularized system converge uniformly to a particular Filippov solution, then we can select this solution for the discontinuous problem. The persistence of the qualitative behavior of solutions under regularization is fundamental if we wish to replace the discontinuous system with the regular one or viceversa, hence we will investigate this persistence as well. At the state of the art, numerical techniques for discontinuous problems, even though efficient, need to select a priori a sliding vector field in Filippov convex combination. Our studies would give rigorous theoretical justifications to make such selection in agreement with the original real life model that one wishes to simulate. Our research project focuses on getting new theoretical insight into numerical approaches and on exploiting this insight for the design and implementation of efficient algorithms.","title":"Numerical integration of nonsmooth dynamical systems"},{"location":"research/#computational-network-science","text":"Networks are a very powerful tool to model complex systems and analyze large data. For example, in exploratory data analysis we are often interested in finding clusters or communities, or assessing the importance (or centrality) of datapoints. As many real-world systems feature geographical, temporal, or categorical metadata and higher-order structures (motifs), models based on higher-order graphs such as hypergraphs, multilayer graphs and simplicial complexes are nowadays becoming prevalent. We are interested in developing both theoretical foundations and efficient algorithms for a range of network analysis problems. We are particularly interested in methods that exploit matrix and tensor representations of the data to approximately solve related combinatorial optimization problems such as community detection, clustering, core-periphery analysis.","title":"Computational network science"},{"location":"research/#spectral-methods-for-machine-learning","text":"Spectral methods play a fundamental role in machine learning, statistics, and data mining. Methods for important tasks such as clustering, semi-supervised learning, dimensionality reduction, latent factor models, ranking and preference learning and covariance estimation all use information about eigenvalues and eigenvectors (or singular values and singular vectors) from an underlying data matrix. Even though spectral methods are both powerful and convenient, they often rely on a linear approximation of the ideal learning problem which sometimes yields inaccurate results. We are interested in developing spectral methods that are based on nonlinear eigenvalue problems with eigenvector nonlinearities. This class of eigenvector methods is very broad and includes several nonlinear dimensionality reduction models and tight relaxation of cut functions, to name some. Moreover, it allows us to develop numerical linear algebra tools to efficiently perform the nonlinear spectral methods.","title":"Spectral methods for machine learning"},{"location":"seminar/","text":"NOMADS Seminars We organize seminars in Numerical ODEs, Matrix Analysis and Data Science. The seminars focus on various topics in numerical analysis, numerical linear algebra, scientific computing and matrix theory with particular emphasis on their application in control theory, network science, data mining and machine learning. Questions or comments about the seminars should be sent to Nicola Guglielmi or Francesco Tudisco . Academic Year 2024/25 Speaker Title, Abstract & Other Info Date & Venue Yuji Nakatsukasa University of Oxford, United Kingdom Randomised algorithms in numerical linear algebra and the column subset selection problem Randomisation is among the most exciting developments in numerical linear algebra, and has led to algorithms that are fast, scalable, robust, and reliable. The column subset selection problem (CSSP) is simple to state but not easy to solve, and has far-reaching consequences in a number of applications in computational mathematics. Several randomised and deterministic algorithms are now available for the CSSP. In this talk we will review a few prominent topics in randomised NLA (low-rank approximation, least-squares problems, norm/trace estimation). We will then explore the CSSP from computational and theoretical viewpoints, highlighting their power in practical applications. Recording of the talk (available to GSSI only) February 20, 2025 2:30 PM Main Lecture Hall, ex-ISEF building Luigi Brugnano University of Florence, Italy Recent Advances in the Numerical Solution of Differential Equations The numerical solution of ODE-IVPs has been the subject of many investigations since many decades, and a number of reliable, state-of-art codes are available for their solution. However, when stability results by first approximation does not apply, things become more involved, and general purpose codes may fail. This is the case, e.g., of Hamiltonian problems, for which the class of Runge-Kutta methods, known as Hamiltonian Boundary Value Methods (HBVMs), has been recently proposed. Such methods, in turn, are equivalent to a truncated expansion of the vector field along the Legendre orthonormal polynomial basis. This fact, coupled with the availability of a very efficient nonlinear iteration for their implementation, allows the use of HBVMs as spectrally accurate methods in time. The truncated expansion can be also regarded as imposing the orthogonality of the residual, w.r.t. a suitable inner product, to all polynomials of a certain degree. This allows, in turn, the use of different polynomial bases, as well as the extension of the approach for coping with different differential problems. As an example, the extension to the case of fractional differential equations has been recently studied. February 17, 2025 5:30 PM Main Lecture Hall, ex-ISEF building Felice Iavernaro University of Bari Aldo Moro, Italy The principle of maximum entropy in least squares approximation We consider using a maximal entropy argument to determine the coefficients of the approximating function and squared error weights simultaneously as output of a (weighted) least-squares approximation problem. Interpreting the weights as a probability distribution, we choose them by maximizing the \u201camount of uncertainty\u201d or entropy, subject to the constraint that the mean squared error is prescribed to a desired (small) value. By acting on this error, we get a robust method for linear and nonlinear regression that automatically detects and removes outliers from the dataset during the fitting procedure, by assigning them a very small weight. We consider the use of both polynomials and univariate/bivariate splines. To show the potential of the maximal-entropy approach in least squares approximation of data we consider a number of illustrations in different application fields. February 17, 2025 4:15 PM Main Lecture Hall, ex-ISEF building Matthias Voigt UniDistance, Switzerland Treating inhomogeneous initial conditions in balancing-based model reduction In standard balanced truncation model order reduction, the initial condition is typically ignored in the reduction procedure and is assumed to be zero instead. However, such a reduced-order model may be a bad approximation to the full-order system, if the initial condition is not zero. In the literature there are several attempts for modified reduction methods at the price of having no error bound or only a posteriori error bounds which are often too expensive to evaluate. In this talk we propose a new balancing procedure that is based on a shift transformation on the state. We first derive a joint projection reduced-order model in which the part of the system depending only on the input and the one depending only on the initial value are reduced at once and we prove an a priori error bound. With this result at hand, we derive a separate projection procedure in which the two parts are reduced separately. This gives the freedom to choose different reduction orders for the different subsystems. Moreover, we discuss how the reduced-order models can be constructed in practice. Since the error bounds are parameter-dependent, we show how they can be optimized efficiently. We conclude by comparing our results with the ones from the literature by a series of numerical experiments. This is joint work with Christian Schr\u00f6der. February 13, 2025 4:00 PM Room D, ex-ISEF building Mar\u00eda L\u00f3pez Fern\u00e1ndez University of Malaga, Spain Generalized Convolution Quadrature for non smooth sectorial problems We consider the application of the generalized Convolution Quadrature (gCQ) to approximate the solution of an important class of sectorial problems. The gCQ is a generalization of Lubich's Convolution Quadrature (CQ) that allows for variable steps. The available stability and convergence theory for the gCQ requires non realistic regularity assumptions on the data, which do not hold in many applications of interest, such as the approximation of subdiffusion equations. It is well known that for non smooth enough data the original CQ, with uniform steps, presents an order reduction close to the singularity. We generalize the analysis of the gCQ to data satisfying realistic regularity assumptions and provide sufficient conditions for stability and convergence on arbitrary sequences of time points. We consider the particular case of graded meshes and show how to choose them optimally, according to the behaviour of the data. An important advantage of the gCQ method is that it allows for a fast and memory reduced implementation. We describe how the fast and oblivious gCQ can be implemented and illustrate our theoretical results with several numerical experiments. Recording of the talk (available to GSSI only) January 29, 2025 5:00 PM Main Lecture Hall, ex-ISEF building Froil\u00e1n Mart\u00ednez Dopico Universidad Carlos III de Madrid, Spain Polynomial and rational matrices with prescribed data We study necessary and sufficient conditions for the existence of polynomial and rational matrices with different prescribed data. First, we consider the problem for polynomial matrices when the size, degree, rank, invariant factors, infinite elementary divisors, and the minimal indices of their left and right null spaces are prescribed and prove that a polynomial matrix with these data exists if and only if these data satisfy a surprisingly simple unique condition related to a fundamental constraint known as the \"index sum theorem'\". In the second place, we extend this result to rational matrices when the size, rank, invariant rational functions, invariant orders at infinity, and minimal indices of their left and right null spaces are prescribed. The data prescribed so far are called in the literature the \"complete structural data\" or the \"complete eigenstructure\" of the polynomial or rational matrix. Finally, in addition to the complete eigenstructure, we also prescribe the minimal indices of the row and column spaces and show that the simple condition found in the previous problems must be completed with a majorization relation among the involved indices, the degrees of the invariant factors and of the infinite elementary divisors. January 23, 2024 2:30 PM Main Lecture Hall, ex-ISEF building Emre Mengi Koc University, Turkey A Subspace Framework for Large-Scale H-infinity Norm Computation I will describe a subspace framework to compute the H-infinity norm of the transfer function of a control system with large order, a commonly employed objective to minimize for instance in controller design, model order reduction and robust control. Our setting encompasses the transfer functions of descriptor systems, delay systems and many other non-rational transfer functions. In this H-infinity context, we employ tools from model order reduction such as Petrov-Galerkin projections in the state space. The talk is intended to be self-contained as much as possible. December 17, 2024 4:00 PM Main Lecture Hall, ex-ISEF building Emre Mengi Koc University, Turkey Large-Scale Eigenvalue Optimization The talk concerns a subspace framework for the general setting of optimizing a prescribed eigenvalue of a large parameter-dependent matrix. The approach aims to replace the large parameter-dependent matrix with a smaller one obtained via orthogonal projections. We analyze the convergence properties of the framework, and point out open problems. An adaptation of the framework for large-scale semidefinite programs is presented. December 11, 2024 4:30 PM Auditorium, Rectorate building Vladimir Protasov University of L'Aquila, Italy The Newton aerodynamic problem: all over again after 300 years In 1687 Isaac Newton in his main work \"Principia'' posed the following Aerodynamic Problem: find the convex surface of the minimal frontal resistance during its uniform motion through an inviscid and incompressible medium. The surface must contain a given disc orthogonal to the vector of velocity and have a given altitude. The solution of Newton became a classical example in the calculus of variations. However, in early 1990s G.Butazzo, D.Kawohl, and P.Guasoni presented a surface of a smaller resistance. They showed that the optimality of Newtons's surface concerns only a special case (although, considered to be general by most of specialists). The aerodynamic problem had to be solved again under general assumptions, which turned out to be a hard task. The solution is still unknown, although some properties of the optimal surface have been established. We give a survey of some of the known results including construction of non-convex surfaces of arbitrary small resistance and of invisible surfaces. Then we present a new approach to analyze the optimal convex surface by using inequalities between derivatives. Some of these inequalities are, probably, of independent interest. This is a joint work with A.Plakhov (University of Aveiro, Portugal). November 25, 2024 4:30 PM Conference Room, ex-INPS building Carola-Bibiane Sch\u00f6nlieb University of Cambridge, United Kingdom Mathematical imaging: from PDEs to deep learning for images Images are a rich source of beautiful mathematical formalism and analysis. Associated mathematical problems arise in functional and non-smooth analysis, the theory and numerical analysis of nonlinear partial differential equations, inverse problems, harmonic, stochastic and statistical analysis, and optimisation, just to name a few. Applications of mathematical imaging are profound and arise in biomedicine, material sciences, astronomy, digital humanities, as well as many technological developments such as autonomous driving, facial screening and many more. In this talk I will discuss my perspective onto mathematical imaging, share my fascination and vision for the subject. I will then zoom into one research problem that I am currently most excited about and that we helped make first advances on: the mathematical formalisation of machine learned approaches for solving inverse imaging problems. November 21, 2024 3:00 PM Main Lecture Hall, ex-ISEF building Nikos Pitsianis Aristotle University of Thessaloniki, Greece The Fiedler Connection to Graph Clustering with Resolution Variation TBA November 20, 2024 5:00 PM Main Lecture Hall, ex-ISEF building Academic Year 2023/24 Speaker Title, Abstract & Other Info Date & Venue Paolo Denti University of Cape Town, South Africa Pharmacometrics: a quantitative tool for pharmacological research July 1, 2024 11:00 AM Library, ex-ISEF building Christian Lubich University of T\u00fcbingen, Germany Regularized dynamical parametric approximation This talk is about the numerical approximation of evolution equations by nonlinear parametrizations $u(t)=\\Phi(q(t))$ with time-dependent parameters $q(t)$, which are to be determined in the computation. The motivation comes from approximations in quantum dynamics by multiple Gaussians and approximations of various dynamical problems by tensor networks and neural networks. In all these cases, the parametrization is typically irregular: the derivative $\\Phi'(q)$ can have arbitrarily small singular values and may have varying rank. We derive approximation results for a regularized approach in the time-continuous case as well as in time-discretized cases. With a suitable choice of the regularization parameter and the time stepsize, the approach can be successfully applied in irregular situations, even though it runs counter to the basic principle in numerical analysis to avoid solving ill-posed subproblems when aiming for a stable algorithm. Numerical experiments with sums of Gaussians for approximating quantum dynamics and with neural networks for approximating the flow map of a system of ordinary differential equations illustrate and complement the theoretical results. The talk is based on joint work with Caroline Lasser, J\u00f6rg Nick and Michael Feischl. June 6, 2024 3:30 PM Main Lecture Hall, ex-ISEF building Paolo Freguglia University of L'Aquila, Italy On the corporate organization. A proposal Inspired by the classic conceptions of \u201ceconomic equilibrium\u201d related to the Arrow (1971) and Debreu (1959) model, we have constructed a mathematical paradigm. We start from a possible \u201ceconomic game\u201d which simulates the relationship between companies and customers, involving company sales, costs and value-assets. So we establish an equilibrium which can be seen as a weighted average of a random variable. Then we arrive at a basic iterable algebraic equation that establishes a relationship between sales and its estimated forecast. The analysis of the evolution of the simple deviations between sales and estimate forecasts is also proposed. Afterwards we keep into account the actual results related to a manufacturing company (with 130 clerks and workers), limiting our analysis to the evolution during 40 months (time unit is the month) of a single product relating to a single customer. In conclusion we gave some ideas about the notion of \u201csales trend\u201d. May 28, 2024 5:15 PM Main Lecture Hall, ex-ISEF building Santolo Leveque Scuola Normale Superiore of Pisa, Italy Parallel-in-Time Solver for the All-at-Once Runge-Kutta Discretization Time-dependent PDEs arise quite often in many scientific areas, such as mechanics, biology, economics, or chemistry, just to name a few. Of late, researchers have devoted their effort in devising parallel-in-time methods for the numerical solution of time-dependent PDEs, adding a new dimension of parallelism and allowing to speed-up the solution process on modern supercomputers. In this talk, we present a parallel-in-time preconditioner for the all-at-once linear system arising when employing a Runge--Kutta method in time. The resulting system is solved iteratively for the numerical solution and for the stages of the method. The proposed preconditioner results in a block-diagonal solution for the stages, and a Schur complement obtained by solving again systems for the stages. A range of numerical experiments validate the robustness of the preconditioner with respect to the discretization parameters and to the number of stages, showing the speed-up achieved on a parallel architecture. This is a joint work with Luca Bergamaschi (University of Padua), Angeles Martinez (University of Trieste), and John W. Pearson (University of Edinburgh). May 28, 2024 4:00 PM Main Lecture Hall, ex-ISEF building Desmond Higham University of Edinburgh, United Kingdom A Numerical Analysis Perspective on the Stability of AI Systems Over the last decade, adversarial attack algorithms have revealed instabilities in deep learning tools. These algorithms raise issues regarding safety, reliability and interpretability in artificial intelligence (AI); especially in high risk settings. Ideas from numerical analysis play key roles in this landscape. From a practical perspective, there has been a war of escalation between those developing attack and defence strategies. At a more theoretical level, researchers have also studied bigger picture questions concerning the existence and computability of successful attacks. I will present examples of attack algorithms in image classification and optical character recognition. I will also outline recent results on the overarching question of whether, under reasonable assumptions, it is inevitable that AI tools will be vulnerable to attack. May 8, 2024 5:30 PM Auditorium, Rectorate building Catherine Higham University of Glasgow, United Kingdom Quantum imaging with deep learning algorithms Quantum imaging technology offers time-efficient, lightweight, less-invasive and low-cost solutions in many different contexts including autonomous driving, security and health. Enhancing this technology with deep learning addresses challenges arising in experimental optimisation, inverse and classification/regression tasks and improves overall performance. I will talk about the deep learning algorithms I have developed for several projects. These include real time scene and depth reconstruction for single photon sensor camera and LiDAR systems, and high-speed imaging using a Micro LED-on-CMOS light projector. May 8, 2024 4:15 PM Auditorium, Rectorate building Emre Mengi Koc University, Turkey Large-scale minimization of the pseudospectral abscissa The minimization of the spectral abscissa of a matrix dependent on parameters has drawn interest in the last couple of decades. The problem is motivated especially by the stability considerations for the associated linear control system. The major difficulty in the minimization of the spectral abscissa is that its dependence on the parameters is non-Lipschitz. A remedy to this is, for a prescribed epsilon > 0, minimizing the epsilon-pseudospectral abscissa, the real part of the rightmost point in the set consisting of eigenvalues of all matrices at a distance of epsilon with respect to the spectral norm. We present a subspace framework to minimize the epsilon-pseudospectral abscissa of a large matrix dependent on parameters analytically. At every subspace iteration, a one-sided subspace restriction on the parameter-dependent matrix yields a small rectangular pseudospectral abscissa minimization problem. We expand the restriction subspace based on the minimizer of this small problem. We prove in theory for the proposed subspace framework that, assuming the global minimizers of the small problems are retrieved, convergence to the global minimizer of the original large-scale pseudospectral abscissa minimization problem occurs in the infinite dimensional setting. Our theoretical findings are illustrated on real large-scale examples that concern the stabilization by static output feedback of benchmark linear control systems. April 23, 2024 4:45 PM Main Lecture Hall, ex-ISEF building Mattia Manucci University of Stuttgart, Germany Approximating the smallest eigenvalue of large Hermitian matrices that depend on parameters April 23, 2024 3:30 PM Main Lecture Hall, ex-ISEF building Ernst Hairer Universit\u00e9 de Gen\u00e8ve, Switzerland Leapfrog methods for relativistic charged-particle dynamics A basic leapfrog integrator and its variational variant are proposed and studied for the numerical integration of the equations of motion of relativistic charged particles in an electromagnetic field. The methods are based on a four-dimensional formulation of the equations of motion. Structure-preserving properties of the numerical methods are analysed, in particular conservation and long-time near-conservation of energy and mass shell as well as preservation of volume in phase space. In the non-relativistic limit, the considered methods reduce to the Boris algorithm for non-relativistic charged-particle dynamics and its variational variant. This talk is based on joint-work with Christian Lubich April 18, 2024 2:30 PM Main Lecture Hall, ex-ISEF building Matthias Voigt FernUni, Switzerland Structure-Preserving Model Order Reduction of Dynamical Systems Model order reduction is a field of mathematics that deals with the complexity reduction of mathematical models in order to reduce the computational costs of tasks such as numerical simulation, optimization, or control. In this talk, first an overview of the field will be given and two of the classical systems theoretic reduction methods (balanced truncation and IRKA) will be reviewed. The second part of the presentation will focus on structure-preserving reduction methods for structured models. This includes a variant of balanced truncation for symmetric second-order systems and a rational fitting method for the class of port-Hamiltonian systems. January 17, 2024 4:00 PM Conference Room, ex-INPS building Alessandro Valerio University of Trondheim, Norway Neuroscience - AI In this seminar, Alessandro Valerio, a graduate in neuroscience, explores the field of neuroscience and its potential intersections with artificial intelligence. Alessandro has an educational background in biology and neuroscience from the University of L\u2019Aquila and Trieste, complemented by research experiences at SISSA in Trieste and at the Kavli Institute for System Neuroscience in Trondheim. The seminar provides an overview of the brain's structure, covering aspects from the macroscopic to the molecular level. A significant focus is given to the role of electrophysiological methods in neuroscience. Alessandro details both in vitro and in vivo techniques, which are crucial in understanding the intricacies of neural dynamics. A part of the seminar is dedicated to Alessandro's research journey. His work at SISSA centered on cellular neurobiology and in vitro electrophysiology, particularly in studying rhythmogenesis mechanisms. His journey continued in Trondheim, where he engaged in in vivo electrophysiology and multisensory integration studies at the Kavli Institute for System Neuroscience. The seminar concludes with Alessandro presenting his research goals. He aims to leverage AI advancements in neuroscience research and reciprocally apply neuroscience insights to advance AI development. This includes a focus on utilizing deep learning for EEG signal classification and exploring the potential of bio-inspired neural networks. January 10, 2024 6:00 PM Conference Room, ex-INPS building Lauri Nyman Aalto University, Finland Nearest singular pencil via Riemannian optimization The problem of finding the nearest singular pencil to a given regular, complex or real, n \u00d7 n matrix pencil A + \u03bbB is a long-standing problem in Numerical Linear Algebra. This problem turned out to be very difficult and, so far, just a few numerical algorithms are available in the literature for its solution, though they may be very expensive from a computational point of view. In this talk, we introduce a new algorithm for solving this problem based on Riemannian optimization, which looks for the closest singular pencil via the minimization of an objective function over the cartesian product of the unitary group by itself. Moreover, we present a collection of numerical experiments that show that the new algorithm can deal effectively with pencils of larger sizes than those considered by previous algorithms and find minimizers of, at least, the same quality than previous algorithms. January 10, 2024 4:00 PM Conference Room, ex-INPS building Mattia Manucci University of Stuttgart, Germany Model Order Reduction for switched Differential Algebraic Equations In this presentation, we will discuss a projection-based Model Order Reduction (MOR) for large-scale systems of switched Differential Algebraic Equations (sDAEs). The main idea relies on exploiting the Quasi-Weierstrass form of each mode of the sDAEs system. Then, we can show that, under certain reasonable assumptions, the output of the sDAEs system is equivalent to the output of a switched system of ordinary differential equations (ODEs) with state jumps between the modes. We present how to construct a reduced system by computing the controllability and observability Gramians associated with the solution of generalized Lyapunov equations for bilinear systems. Finally, we discuss how to efficiently compute an approximation of the solution of such generalized Lyapunov equations, with error control, when the associated matrices are sparse and large. Numerical results are presented to showcase the efficiency and effectiveness of the developed MOR method. December 20, 2023 5:00 PM Conference Room, ex-INPS building Academic Year 2022/23 Speaker Title, Abstract & Other Info Date & Venue Christian Lubich Universit\u00e4t T\u00fcbingen, Germany Time integration of tree tensor networks First I report on recent numerical experiments with time-dependent tree tensor network algorithms for the approximation of quantum spin systems. I will then describe the basics in the design of time integration methods that are robust to the usual presence of small singular values, that have good structure-preserving properties (norm, energy conservation or dissipation), and that allow for rank (= bond dimension) adaptivity and also have some parallelism. This discussion of basic concepts will be done for the smallest possible type of tensor network differential equations, namely low-rank matrix differential equations. Once this simplest case is understood, there is a systematic path to the extension of the integrators and their favourable properties to general tree tensor networks. June 8, 2023 3:00 PM Auditorium, Rectorate building Simone Brugiapaglia Concordia University, Canada The mathematical foundations of deep learning: from rating impossibility to practical existence theorems Deep learning is having a profound impact on industry and scientific research. Yet, while this paradigm continues to show impressive performance in a wide variety of applications, its mathematical foundations are far from being well established. In this talk, I will present recent developments in this area by illustrating two case studies. First, motivated by applications in cognitive science, I will present 'rating impossibility' theorems. They identify frameworks where deep learning is provably unable to generalize outside the training set for the seemingly simple task of learning identity effects, i.e. classifying whether pairs of objects are identical or not. Second, motivated by applications in scientific computing, I will illustrate 'practical existence' theorems. They combine universal approximation results for deep neural networks with compressed sensing and high-dimensional polynomial approximation theory. As a result, they yield sufficient conditions on the network architecture, the training strategy, and the number of samples able to guarantee accurate approximation of smooth functions of many variables. Time permitting, I will also discuss work in progress and open questions. June 1, 2023 2:00 PM Auditorium, Rectorate building Ernst Hairer Universit\u00e9 de Gen\u00e8ve, Switzerland Large-stepsize integrators for charged particles in a strong magnetic field This talk considers the numerical treatment of the differential equation that describes the motion of electric particles in a strong magnetic field. A standard integrator is the Boris algorithm which, for small stepsizes, can be analysed by classical techniques. For a strong magnetic field the solution is highly oscillatory and the numerical integration is more challenging. New modifications of the Boris algorithm are discussed, and their accuracy and long-time behaviour are studied with means of modulated Fourier expansions. Emphasis is put on the situation where the stepsize is proportional to (or larger than) the wavelength of the oscillations. The presented results have been obtained in collaboration with Christian Lubich. April 20, 2023 2:30 PM Auditorium, Rectorate building Mauro Piccioni Sapienza Universit\u00e0 di Roma, Italy Estimating the interaction graph for a class of Galves-Locherbach models (Part 2) We explain how to estimate the presence and the nature of the interactions (either excitatory or inhibitory) in a multivariate point process modeling a network of spiking neurons (Galves and Locherbach, JSP 2013). In the first talk a Markovian model is analyzed whereas in the second delayed inhibitory interactions have to be detected. April 19, 2023 5:30 PM Main Lecture Hall, ex-ISEF building Emilio De Santis Sapienza Universit\u00e0 di Roma, Italy Estimating the interaction graph for a class of Galves-Locherbach models (Part 1) We explain how to estimate the presence and the nature of the interactions (either excitatory or inhibitory) in a multivariate point process modeling a network of spiking neurons (Galves and Locherbach, JSP 2013). In the first talk a Markovian model is analyzed whereas in the second delayed inhibitory interactions have to be detected. April 19, 2023 5:30 PM Main Lecture Hall, ex-ISEF building Kai Bergermann TU Chemnitz, Germany Adaptive rational Krylov methods for exponential Runge--Kutta integrators We consider the solution of large stiff systems of ordinary differential equations with explicit exponential Runge--Kutta integrators. These problems arise from semi-discretized semi-linear parabolic partial differential equations on continuous domains or on inherently discrete graph domains. A series of results reduces the requirement of computing linear combinations of $\\varphi$-functions in exponential integrators to the approximation of the action of a smaller number of matrix exponentials on certain vectors. State-of-the-art computational methods use polynomial Krylov subspaces of adaptive size for this task. They have the drawback that the required Krylov subspace iteration numbers to obtain a desired tolerance increase drastically with the spectral radius of the discrete linear differential operator, e.g., the problem size. We present an approach that leverages rational Krylov subspace methods promising superior approximation qualities. We prove a novel a-posteriori error estimate of rational Krylov approximations to the action of the matrix exponential on vectors for single time points, which allows for an adaptive approach similar to existing polynomial Krylov techniques. We discuss pole selection and the efficient solution of the arising sequences of shifted linear systems by direct and preconditioned iterative solvers. Numerical experiments show that our method outperforms the state of the art for sufficiently large spectral radii of the discrete linear differential operators. The key to this are approximately constant rational Krylov iteration numbers, which enable a near-linear scaling of the runtime with respect to the problem size. April 18, 2023 5:30 PM Main Lecture Hall, ex-ISEF building Gianluca Ceruti EPFL, Switzerland A rank-adaptive robust BUG for dynamical low-rank approximation In the present contribution, a rank-adaptive integrator for the dynamical low-rank approximation of matrix differential equations is introduced. First, the dynamical low-rank approximation together with the projector-splitting numerical integrator is summarised. Then, recent developments on the field are presented. The so-called fixed-rank unconventional BUG integrator is introduced and analysed. Next, a simple but extremely effective modification allowing for an adaptive choice of the rank, using subspaces generated by the integrator itself, is illustrated. It is shown that the novel BUG adaptive low-rank integrator retains the exactness, robustness and symmetry-preserving properties of the previously proposed fixed-rank integrators. Beyond that, up to the truncation tolerance, the rank-adaptive integrator preserves the norm when the differential equation does, it preserves the energy for Schroedinger equations and Hamiltonian systems, and it preserves the monotonic decrease in gradient flows. This seminar will focus on the numerical error analysis of the adaptive low-rank integrator. Furthemore, the first theoretically solid application to low-rank training in machine learning will be discussed. The present contribution is based upon joint works with Ch. Lubich, J. Kusch, and F. Tudisco. The last developments in the field of machine learning have been possible only due to the great contributions of PhD candidates E. Zangrando (GSSI) and S. Schotthoefer (KIT). March 16, 2023 3:00 PM Zoom meeting Alfio Quarteroni Politecnico di Milano, Italy, and EPFL, Switzerland A Mathematical Heart In this presentation I will report on the most recent achievements of the iHEART project, aimed at developing a comprehensive accurate mathematical and numerical model for the simulation of the complete cardiac function. December 15, 2022 3:00 PM Auditorium, Rectorate building Academic Year 2021/22 Speaker Title, Abstract & Other Info Date & Venue Emre Mengi Koc University, Turkey Large-Scale Estimation of the Dominant Poles of a Transfer Function The dominant poles of the transfer function of a descriptor system provide important insight into the behavior of the system. They indicate the parts of the imaginary axis where the transfer function exhibits large norm. Moreover, the dominant poles and corresponding eigenvectors can be put in use to form a reduced-order approximation to the system. In the talk, I will describe a subspace framework to compute a prescribed number of dominant poles of a large-scale descriptor system. The framework applies Petrov-Galerkin projections to the original system, then computes the dominant poles of the projected small-scale system, for instance by the QZ algorithm, and expands the subspaces so that the projected system after the subspace expansion interpolates the original system at these dominant poles. I will explain why the subspace framework converges at a quadratic rate, and report numerical results illustrating the rapid convergence, and accuracy of the approach. September 19, 2022 1:30 PM Main Lecture Hall, ex-ISEF building Stefano Massei University of Pisa, Italy Improved parallel-in-time integration via low-rank updates and interpolation This work is concerned with linear matrix equations that arise from the space-time discretization of time-dependent linear partial differential equations (PDEs). Such matrix equations have been considered, for example, in the context of parallel-in-time integration leading to a class of algorithms called ParaDiag. We develop and analyze two novel approaches for the numerical solution of such equations. Our first approach is based on the observation that the modification of these equations performed by ParaDiag in order to solve them in parallel has low rank. Building upon previous work on low-rank updates of matrix equations, this allows us to make use of tensorized Krylov subspace methods to account for the modification. Our second approach is based on interpolating the solution of the matrix equation from the solutions of several modifications. Both approaches avoid the use of iterative refinement needed by ParaDiag and related space-time approaches in order to attain good accuracy. In turn, our new approaches have the potential to outperform, sometimes significantly, existing approaches. This potential is demonstrated for several different types of PDEs. September 14, 2022 Main Lecture Hall, ex-ISEF building Massimiliano Fasi Durham University, UK CPFloat: A C Library for Emulating Low-Precision Arithmetic Low-precision floating-point arithmetic can be simulated via software by executing each arithmetic operation in hardware and rounding the result to the desired number of significant bits. For IEEE-compliant formats, rounding requires only standard mathematical library functions, but handling subnormals, underflow, and overflow demands special attention, and numerical errors can cause mathematically correct formulae to behave incorrectly in finite arithmetic. Moreover, the ensuing algorithms are not necessarily efficient, as the library functions these techniques build upon are typically designed to handle a broad range of cases and may not be optimized for the specific needs of floating-point rounding algorithms. CPFloat is a C library that offers efficient routines for rounding arrays of binary32 and binary64 numbers to lower precision. The software exploits the bit level representation of the underlying formats and performs only low-level bit manipulation and integer arithmetic, without relying on costly library calls. In numerical experiments the new techniques bring a considerable speedup (typically one order of magnitude or more) over existing alternatives in C, C++, and MATLAB. To the best of our knowledge, CPFloat is currently the most efficient and complete library for experimenting with custom low-precision floating-point arithmetic available in any language. June 24, 2022 1:00 PM Library, ex-ISEF building Stefano Serra-Capizzano University of Insubria, Italy The GLT class as a Generalized Fourier Analysis and applications Recently, the class of Generalized Locally Toeplitz (GLT) sequences has been introduced [1, 2] as a generalization both of classical Toeplitz sequences and of variable coefficient differential operators and, for every sequence of the class, it has been demonstrated that it is possible to give a rigorous description of the asymptotic spectrum in terms of a function (the symbol) that can be easily identified. This generalizes the notion of a symbol for differential operators also of fractional type (discrete and continuous) or for Toeplitz sequences for which it is identified through the Fourier coefficients and is related to the classical Fourier Analysis. The GLT class has nice algebraic properties and indeed it has been proven that it is stable under linear combinations, products, and inversion when the sequence which is inverted shows a sparsely vanishing symbol (sparsely vanishing symbol = a symbol which vanishes at most in a set of zero Lebesgue measure). Furthermore, the GLT class virtually includes any approximation by local methods (Finite Difference, Finite Element, Isogeometric Analysis of partial differential equations (PDEs)) and, based on this, we demonstrate that our results on GLT sequences can be used in a PDE setting in various directions. [1] S. Serra-Capizzano. Generalized Locally Toeplitz sequences: spectral analysis and applications to discretized partial differential equations . Linear Algebra Appl. 366 (2003), 371\u2013402. [2] S. Serra-Capizzano. The GLT class as a generalized Fourier Analysis and applications . Linear Algebra Appl. 419 (2006), 180\u2013233. Further references can be found here . May 26, 2022 2:00 PM Main Lecture Hall, ex-ISEF building Paolo Cifani Gran Sasso Science Institute, Italy Geometric integration of Lie-Poisson flows on the sphere In this seminar I will touch upon the recent developments in structure-preserving (geometric) integration of Euler\u2019s equations for two-dimensional incompressible flows. It has been known for half a century that the dynamics of incompressible ideal fluids in two dimensions can be understood as an evolution equation on the contangent bundle of the infinite-dimensional Lie group of symplectic dffeomorphisms. In particular, the vorticity equation constitutes a Lie-Poisson system characterized by an infinite number of first integrals, i.e. the integrated powers of vorticity. This set of constraints, absent in three dimensions, has profound effects on the energy transfer mechanisms across scales of motion. Yet, the construction of a numerical system which preserves this rich Poisson structure has been elusive. Most attempts either fail in fully preserving the geometric structure or have a high computational complexity. Here, I will show that, thanks to our recent advances, it possible to design a geometric integrator which embeds this fundamental principle of the continuum into the discrete system at a modest computational cost. The construction of such scheme, the main numerical algorithms and their parallelisation on modern supercomputing facilities will be discussed. Finally, an application to the spectrum of homogeneous two-dimensional turbulence will be illustrated. May 24, 2022 12:45 PM Main Lecture Hall, ex-ISEF building Simone Camarri University of Pisa, Italy Adjoint-based passive control of hydrodynamic instabilities A large number of research papers in the literature have been dedicated to the use of adjoint-based sensitivity and global stability analyses for both characterising and controlling instabilities in fluid mechanics. Such controls, which are mainly passive, are designed for stabilising linearly unstable configurations. The design strategy based on global stability analysis can be rigorously applied to relatively simple flows in laminar regime. More complex configurations can also be rigorously treated, as for instance cases in which the flow to be controlled is time periodic. Moreover, a large interest exists in the application of the same methods for the control of coherent large-scale flow structures in turbulent flows as, for instance, the quasi-periodic shedding of vortices in turbulent wakes. This is possible by postulating the marginal stability of mean flows, which is shown to apply for several cases of interest. In this seminar a review of the methods based on global stability and sensitivity analyses for the design and/or analysis of passive controls will be presented. Moreover, configurations of increasing complexity will be considered, ranging from laminar steady flows to turbulent flows. May 18, 2022 4:00 PM Main Lecture Hall, ex-ISEF building Lev Lokutsievskiy Steklov Institute, Moscow, Russia Asymptotic control theory for affine switching systems of oscillators The talk will be devoted to continuous-time affine control systems and their reachable sets. I will focus on the case when all eigenvalues of the linear part of the system have zero real part. In this case, the reachable sets usually have a non-exponential growth rate as T\u2192\u221e, and it is usually polynomial. The simplest non-trivial example is the problem of stabilisation (or, conversely, destabilisation) of two pendulums by the same common control. An exact description of reachable sets is most often impossible here, but their asymptotic behaviour as T\u2192\u221e can be found with high accuracy. In the talk, I will present the asymptotic behaviour of reachable sets in the problem of controlling a system of N independent oscillators, and in the problem of controlling the wave equation for a closed string. In particular, in these problems the corresponding analog of the Lyapunov function can be found explicitly, and, consequently, the optimal behaviour at high energies can be found very accurately May 11, 2022 4:30 PM Main Lecture Hall, ex-ISEF building Ernst Hairer University of Geneva, Switzerland High order PDE-convergence of ADI-type integrators for parabolic problems This work considers space-discretised parabolic problems on a rectangular domain subject to Dirichlet boundary conditions. For the time integration s-stage AMF-W-methods, which are ADI (Alternating Direction Implicit) type integrators, are considered. They are particularly efficient when the space dimension of the problem is large. The classical algebraic conditions for order p (with p<=3) are shown to be sufficient for PDE-convergence of order p (independently of the spatial resolution) in the case of time independent Dirichlet boundary conditions. Under additional conditions, PDE-convergence of order p=3.25-eps for every eps>0 can be obtained. In the case of time dependent boundary conditions there is an order reduction. This is joint work with Severiano Gonzalez-Pinto and Domingo Hernandez-Abreu. Related publications can be downloaded from http://www.unige.ch/~hairer/preprints.html Recording of the talk (available to GSSI only) April 26, 2022 1:00 PM Auditorium, Rectorate building Marino Zennaro University of Trieste, Italy Computing antinorms on multicones The theoretical results we consider in this talk may find an interesting application in the study of discrete-time linear switched systems of the form x(n + 1) = A_{\u03c3(n)} x(n), \u03c3 : N \u2212\u2192 {1, 2, . . . , m}, where x(0) \u2208 R^k, the matrix A_{\u03c3(n)} \u2208 R^{k\u00d7k} belongs to a finite family F = {A_i}{1\u2264i\u2264m} and \u03c3 denotes the switching law. It is known that the most stable switching laws are associated to the so-called spectrum-minimizing products of the family F. Moreover, for a normalized family F of matrices (i.e., with lower spectral radius \u03c1\u02c7(F) = 1) that share an invariant cone K, all the most stable trajectories starting from the interior of K lie on the boundary of the antiball of a so-called invariant Barabanov antinorm, for which a canonical constructive procedure is available (Guglielmi & Z. (2015)). In the particular case of families F sharing an invariant cone K, we show how to provide lower bounds to \u02c7\u03c1(F) by a suitable adaptation of the Gelfand limit in the framework of antinorms (Guglielmi & Z. (2020)). Then we consider families of matrices F that share an invariant multicone K_{mul} (Brundu & Z. (2018, 2019)) and show how to generalize some of the known results on antinorms to this more general setting (Guglielmi & Z. (in progress)). These generalizations are of interest because common invariant multicones may well exist when common invariant cones do not. March 31, 2022 2:00 PM Auditorium, Rectorate building Giorgio Fusco University of L'Aquila, Italy TBA February 24, 2022 Erkki Somersalo Case Western Reserve University, USA Bayes meets data science to identify changes in brain activity during meditation from MEG measurements Meditation as a potential alternative for pharmaceutical intervention to mitigate conditions such as chronic pain or clinical depression continues to obtain significant attention. One of the problems is that often the positive effects of meditation that have been reported are anecdotal or are based on self reporting. To quantify the effects of meditation, it is therefore important to develop methods based on medical imaging to identify brain regions that are involved in the meditation practice. In this talk, we review some recent results about this topic, addressed by using magnetoencephalography (MEG) to map brain activity during meditation. One of the difficulties here is that the data are less sensitive to activity taking place in the deep brain regions, including the limbic system that is believed to play an important role in meditation. The MEG inverse problem is addressed by using novel Bayesian methods combined with advanced numerical techniques, applied on data from professional Buddhist meditators. The reconstructed activity is then analyzed using data science techniques to distill the information about the activation changes during meditation. Recording of the talk (available to GSSI only) December 16, 2021 3:00 PM Auditorium, Rectorate building Matthew Colbrook University of Cambridge, UK Computing semigroups and time-fractional PDEs with error control We develop an algorithm that computes strongly continuous semigroups on infinite-dimensional Hilbert spaces with explicit error control. Given a generator $A$, a time $t > 0$, an arbitrary initial vector $u_0$ and an error tolerance $\\epsilon > 0$, the algorithm computes $\\exp(tA)u_0$ with error bounded by $\\epsilon$. The (parallelisable) algorithm is based on a combination of a regularized functional calculus, suitable contour quadrature rules and the adaptive computation of resolvents in infinite dimensions. As a particular case, we deal with semigroups on $L^2(R^d)$ that are generated by partial differential operators with polynomially bounded coefficients of locally bounded total variation. For analytic semigroups, we provide a quadrature rule whose error decreases like $\\exp(\u2212cN/ log(N))$ for $N$ quadrature points, that remains stable as $N \\to \\infty$, and which is also suitable for infinite-dimensional operators. Finally, we extend the method to time-fractional PDEs (where it avoids singularities as $t \\to 0$ and large memory consumption). Numerical examples are given, including: Schr\u00f6dinger and wave equations on the aperiodic Ammann\u2013Beenker tiling and fractional beam equations arising in the modelling of small-amplitude vibration of viscoelastic materials. The spectral analysis (which is always needed for contour methods) is considerably simplified due to an infinite-dimensional \u201csolve-then-discretise\u201d approach. December 1, 2021 5:00 PM Main Lecture Hall, ex-ISEF building Academic Year 2020/21 Speaker Title, Abstract & Other Info Date & Venue Lars Ruthotto Emory University, USA Numerical Methods for Deep Learning motivated by Partial Differential Equations Understanding the world through data and computation has always formed the core of scientific discovery. Amid many different approaches, two common paradigms have emerged. On the one hand, primarily data-driven approaches\u2014such as deep neural networks\u2014have proven extremely successful in recent years. Their success is based mainly on their ability to approximate complicated functions with generic models when trained using vast amounts of data and enormous computational resources. But despite many recent triumphs, deep neural networks are difficult to analyze and thus remain mysterious. Most importantly, they lack the robustness, explainability, interpretability, efficiency, and fairness needed for high-stakes decision-making. On the other hand, increasingly realistic model-based approaches\u2014typically derived from first principles and formulated as partial differential equations (PDEs)\u2014are now available for various tasks. One can often calibrate these models\u2014which enable detailed theoretical studies, analysis, and interpretation\u2014with relatively few measurements, thus facilitating their accurate predictions of phenomena. In this talk, I will highlight recent advances and ongoing work to understand and improve deep learning by using techniques from partial differential equations. I will demonstrate how PDE techniques can yield better insight into deep learning algorithms, more robust networks, and more efficient numerical algorithms. I will also expose some of the remaining computational and numerical challenges in this area. Slides of the talk Recording of the talk (available to GSSI only) June 17, 2021 4:00 PM Ivan Markovsky Vrije Universiteit Brussel, Belgium Data-driven dynamic interpolation and approximation The behavioral system theory give theoretical foundation for nonparameteric representations of linear time-invariant systems based on Hankel matrices constructed from data. These data-driven representations led in turn to new system identification, signal processing, and control methods. In particular, data-driven simulation and linear quadratic tracking control problems were solved using the new approach [1,2]. This talk shows how the approach can be used further on for solving data-driven interpolation and approximation problems (missing data estimation) and how it can be generalized to some classes of nonlinear systems. The theory leads to algorithms that are both general (can deal simultaneously with missing, exact, and noisy data of multivariable systems) and simple (require existing numerical linear algebra methods only). This opens a practical computational way of doing system theory and signal processing directly from data without identification of a transfer function or a state space representation and doing model-based design. [1] I. Markovsky and P. Rapisarda. \u201cData-driven simulation and control\u201d. Int. J. Control 81.12 (2008), pp. 1946--1959. [2] I. Markovsky. A missing data approach to data-driven filtering and control. IEEE Trans. Automat. Contr., 62:1972--1978, April 2017. [3] I. Markovsky and F. D\u00f6rfler. Data-driven dynamic interpolation and approximation. Technical report, Vrije Universiteit Brussel, 2021. Recording of the talk (available to GSSI only) March 30, 2021 5:00 PM Eugene E. Tyrtyshnikov Institute for Numerical Mathematics, Russian Academy of Sciences Tikhonov's solution to a class of linear systems equivalent within perturbations A standard approach to incorrect problems suggests that a problem of interest is reformulated with the knowledge of some additional a-priori information. This can be done by several well-known regularization techniques. Many practical problems are successfully solved on this way. What does not still look as completely satisfactory is that the new reset problem seems to appear rather implicitly in the very process of its solution. In 1980, A. N. Tikhonov proposed a reformulation [1] that arises explicitly before the discussion of the solution methods. He suggested a notion of normal solution to a family of linear algebraic systems described by a given individual system and its vicinity comprising perturbed systems, under the assumption that there are compatible systems in the class notwithstanding the compatibility property of the given individual system. Tikhovov proved that the normal solution exists and is unique. However, a natural question about the correctness of the reset problem was not answered. In this talk we address a question of correctness of the reformulated incorrect problems that seems to have been missed in all previous considerations. The main result is the proof of correctness for Tikhonov's normal solution. Possible generalizations and difficulties will be also discussed. [1] A. N. Tikhonov, Approximate systems of linear algebraic equations, USSR Computational Mathematics and Mathematical Physics, vol. 20, issue 6 (1980) March 9, 2021 6:00 PM Michael Schaub RWTH Aachen University Learning from signals on graphs with unobserved edges In many applications we are confronted with the following system identification scenario: we observe a dynamical process that describes the state of a system at particular times. Based on these observations we want to infer the (dynamical) interactions between the entities we observe. In the context of a distributed system, this typically corresponds to a \"network identification\" task: find the (weighted) edges of the graph of interconnections. However, often the number of samples we can obtain from such a process are far too few to identify the edges of the network exactly. Can we still reliably infer some aspects of the underlying system? Motivated by this question we consider the following identification problem: instead of trying to infer the exact network, we aim to recover a (low-dimensional) statistical model of the network based on the observed signals on the nodes. More concretely, here we focus on observations that consist of snapshots of a diffusive process that evolves over the unknown network. We model the (unobserved) network as generated from an independent draw from a latent stochastic blockmodel (SBM), and our goal is to infer both the partition of the nodes into blocks, as well as the parameters of this SBM. We present simple spectral algorithms that provably solve the partition and parameter inference problems with high-accuracy. We further discuss some possible variations and extensions of this problem setup. Recording of the talk (available to GSSI only) February 17, 2021 6:00 PM Lothar Reichel Kent State University Large-scale regression with non-convex loss and penalty We do non-convex optimization with application to image restoration and regression problems for which a sparse solution is desired. February 4, 2021 5:00 PM Anders Hansen University of Cambridge, UK On the foundations of computational mathematics, Smale's 18th problem and the potential limits of AI There is a profound optimism on the impact of deep learning (DL) and AI in the sciences with Geoffrey Hinton concluding that 'They should stop training radiologists now'. However, DL has an Achilles heel: it is universaly unstable so that small changes in the initial data can lead to large errors in the final result. This has been documented in a wide variety of applications. Paradoxically, the existence of stable neural networks for these applications is guaranteed by the celebrated Universal Approximation Theorem, however, the stable neural networks are never computed by the current training approaches. We will address this problem and the potential limitations of AI from a foundations point of view. Indeed, the current situation in AI is comparable to the situation in mathematics in the early 20th century, when David Hilbert\u2019s optimism (typically reflected in his 10th problem) suggested no limitations to what mathematics could prove and no restrictions on what computers could compute. Hilbert\u2019s optimism was turned upside down by Goedel and Turing, who established limitations on what mathematics can prove and which problems computers can solve (however, without limiting the impact of mathematics and computer science). We predict a similar outcome for modern AI and DL, where the limitations of AI (the main topic of Smale\u2019s 18th problem) will be established through the foundations of computational mathematics. We sketch the beginning of such a program by demonstrating how there exist neural networks approximating classical mappings in scientific computing, however, no algorithm (even randomised) can compute such a network to even 1-digit accuracy (with probability better than 1/2). We will also show how instability is inherit in the methodology of DL demonstrating that there is no easy remedy, given the current methodology. Finally, we will demonstrate basic examples in inverse problems where there exists (untrained) neural networks that can easily compute a solution to the problem, however, the current DL techniques will need 10^80 data points in the training set to get even 1% success rate. Recording of the talk January 28, 2021 3:00 PM Gianluca Ceruti University of Tuebingen Numerical integrators for dynamical low-rank approximation Discretization of time-dependent high-dimensional PDEs suffers of an undesired effect, known as curse of dimensionality. The amount of data to be stored and treated, grows exponentially, and exceeds standard capacity of common computational devices. In this setting, time dependent model order reductions techniques are desirable. In the present seminar, together with efficient numerical integrators, we present a recently developed approach: dynamical low-rank approximation. Dynamical low-rank approximation for matrices will be firstly presented, and a numerical integrator with two remarkable properties will be introduced: the matrix projector splitting integrator. Based upon this numerical integrator, we will construct two equivalent extensions for tensors, multi-dimensional arrays, in Tucker format - a high-order generalization of the SVD decomposition for matrices. These extensions are proven to preserve the excellent qualities of the matrix integrator. To conclude, via a novel compact formulation of the Tucker integrator, we will further extend the matrix and Tucker projector splitting integrators to the most general class of Tree Tensor Networks. Important examples belonging to this class and of interest for applications are given, but not only restricted to, by Tensor Trains. This seminar is based upon a joint work with Ch. Lubich and H. Walach. January 13, 2021 5:00 PM Alexander Viguerie GSSI Efficient, stable, and reliable solvers for the Steady Incompressible Navier-Stokes equations: application to Computational Hemodynamics. Over the past several years, computational fluid dynamics (CFD) simulations have become increasingly popular as a clinical tool for cardiologists at the patient-specific level. The use of CFD in this area poses several challenges. The clinical setting places heavy restrictions on both computational time and power. Simulation results are usually desired within minutes and are usually run on standard computers. For these reasons, steady-state Navier-Stokes simulations are usually preferred, as they can be completed in a fraction of the time required to run an unsteady computation. However, in many respects the steady problem is more difficult than the unsteady one, particularly in regards to solving the associated linear and nonlinear systems. Additionally, boundary data for patient-specific problems is often missing, incomplete, or unreliable. This makes the determination of a useful model challenging, as it requires the generation of reliable boundary data without introducing heavy computational costs. This seminar will address these challenges, as well as some others, and introduce new techniques for workarounds. Results from patient-specific cases will be presented and discussed. Recording of the talk (available to GSSI only) December 16, 2020 5:00 PM Martin Stoll TU-Chemnitz From PDEs to data science: an adventure with the graph Laplacian In this talk we briefly review some basic PDE models that are used to model phase separation in materials science. They have since become important tools in image processing and over the last years semi-supervised learning strategies could be implemented with these PDEs at the core. The main ingredient is the graph Laplacian that stems from a graph representation of the data. This matrix is large and typically dense. We illustrate some of its crucial features and show how to efficiently work with the graph Laplacian. In particular, we need some of its eigenvectors and for this the Lanczos process needs to be implemented efficiently. Here, we suggest the use of the NFFT method for evaluating the matrix vector products without even fully constructing the matrix. We illustrate the performance on several examples. Recording of the talk (available to GSSI only) December 2, 2020 5:00 PM Patricia Diaz De Alba GSSI Numerical treatment for inverse electromagnetic problems Electromagnetic induction surveys are among the most popular techniques for non-destructive investigation of soil properties, in order to detect the presence of both ground inhomogeneities and particular substances. Frequency-domain electromagnetic instruments allow the collection of data in different configurations, that is, varying the intercoil spacing, the frequency, and the height above the ground. Based on a non-linear forward model used to describe the interaction between an electromagnetic field and the soil, the aim is to reconstruct the distribution of either the electrical conductivity or the magnetic permeability with respect to depth. To this end, the inversion of both the real (in-phase) and the imaginary (quadrature) components of the signal are studied by a regularized damped Gauss-Newton method. The regularization part of the algorithm is based on a low-rank approximation of the Jacobian of the non-linear model. Furthermore, in many situations, a regularization scheme retrieving smooth solutions is blindly applied, without taking into account the prior available knowledge. An algorithm for a regularization method that promotes the sparsity of the reconstructed electrical conductivity or magnetic permeability distribution is available. This regularization strategy incorporates a minimum gradient support stabilizer into a truncated generalized singular value decomposition scheme. The whole inversion algorithm has been enclosed in a MATLAB package, called FDEMtools, allowing the user to experiment with synthetic and experimental data sets, and different regularization strategies, in order to compare them and draw conclusions. The numerical effectiveness of the inversion procedure is demonstrated on synthetic and real datasets by using FDEMtools package. November 20, 2020 5:15 PM Christian Lubich University of Tuebingen Dynamical low-rank approximation This talk reviews differential equations and their numerical solution on manifolds of low-rank matrices or of tensors with a rank structure such as tensor trains or general tree tensor networks. These low-rank differential equations serve to approximate, in a data-compressed format, large time-dependent matrices and tensors or multivariate functions that are either given explicitly via their increments or are unknown solutions to high-dimensional evolutionary differential equations, with multi-particle time-dependent Schr\u00f6dinger equations and kinetic equations such as Vlasov equations as noteworthy examples of applications. Recently developed numerical time integrators are based on splitting the projection onto the tangent space of the low-rank manifold at the current approximation. In contrast to all standard integrators, these projector-splitting methods are robust to the unavoidable presence of small singular values in the low-rank approximation. This robustness relies on exploiting geometric properties of the manifold of low-rank matrices or tensors: in each substep of the projector-splitting algorithm, the approximation moves along a flat subspace of the low-rank manifold. In this way, high curvature due to small singular values does no harm. This talk is based on work done intermittently over the last decade with Othmar Koch, Bart Vandereycken, Ivan Oseledets, Emil Kieri, Hanna Walach and Gianluca Ceruti. November 18, 2020 5:00 PM Raffaele D'Ambrosio University of L'Aquila Structure-preserving numerics for stochastic differential equations Modern Numerical Analysis is not only devoted to accurately approximating the solutions of various problems through efficient and robust schemes, but also to retaining qualitative properties of the continuous problem over long times. Sometimes such conservation properties naturally characterize the numerical schemes, while in more complex situations preservation issues have to be conveyed into the numerical approximations. The talk is focused on presenting recent advances in structure-preservation issues for selected stochastic differential equations satisfying some characteristic invariance laws. The behaviour of stochastic multistep methods in the preservation of mean-square contractivity will be analyzed; we show, in this case, that conservation properties are hidden, as a matter of fact, into conditional stability properties of numerical schemes. The analysis will also be conveyed to the discretization of stochastic Hamiltonian problems for the numerical preservation of the behaviour of the expected Hamiltonian. The theoretical analysis will also be supported by the numerical evidence. November 4, 2020 3:00 PM Giacomo Baggio University of Padua From Model-Based to Data-Driven Control of Network Dynamics The control of complex dynamical networks has attracted increasing interest over the past few years, with reference, in particular, to problems of controllability, optimal input design, and minimal actuator placement. In this talk, I will address the problem of controlling linear dynamical networks from a control-energy or \"practical\" perspective. I will first focus on the model-based scenario and review the fundamental metrics, theoretical limitations, and challenges in controlling networks using a limited number of control nodes. In particular, I will emphasize the impact of the \"degree\" of non-normality of the network's adjacency matrix on the control performance. Then, I will switch to a data-driven scenario, and show how some network control problems can be efficiently solved by relying on experimental data only. Slides of the seminar October 14, 2020 4:00 PM Federico Poloni University of Pisa Inverses of quasidefinite matrices in block-factored form, with an application to control theory joint work with P. Benner We describe an algorithm to compute the explicit inverse of a dense quasi-definite matrix, i.e., a symmetric matrix of the form [-BB^T, A; A^T, C^TC], with the (1,1) block negative semidefinite and the (2,2) block positive semidefinite. The algorithm is a variant of Gauss-Jordan elimination that works on the low-rank factors $B$ and $C$ directly without ever forming those blocks. The individual elimination steps amount to a transformation called principal pivot transform ; it was shown in [Poloni, Strabic 2016] how to perform it by working only on $A, B, C$, and we rely on that procedure here. We discuss the stability of the resulting method, and show how the algorithm (and in particular the produced low-rank factors) can be of use in control theory, in the context of the matrix sign iteration, which is a method used to solve algebraic Riccati equations. September 30, 2020 4:00 PM","title":"Seminars"},{"location":"seminar/#nomads-seminars","text":"We organize seminars in Numerical ODEs, Matrix Analysis and Data Science. The seminars focus on various topics in numerical analysis, numerical linear algebra, scientific computing and matrix theory with particular emphasis on their application in control theory, network science, data mining and machine learning. Questions or comments about the seminars should be sent to Nicola Guglielmi or Francesco Tudisco .","title":"NOMADS Seminars"},{"location":"seminar/#academic-year-202425","text":"Speaker Title, Abstract & Other Info Date & Venue Yuji Nakatsukasa University of Oxford, United Kingdom Randomised algorithms in numerical linear algebra and the column subset selection problem Randomisation is among the most exciting developments in numerical linear algebra, and has led to algorithms that are fast, scalable, robust, and reliable. The column subset selection problem (CSSP) is simple to state but not easy to solve, and has far-reaching consequences in a number of applications in computational mathematics. Several randomised and deterministic algorithms are now available for the CSSP. In this talk we will review a few prominent topics in randomised NLA (low-rank approximation, least-squares problems, norm/trace estimation). We will then explore the CSSP from computational and theoretical viewpoints, highlighting their power in practical applications. Recording of the talk (available to GSSI only) February 20, 2025 2:30 PM Main Lecture Hall, ex-ISEF building Luigi Brugnano University of Florence, Italy Recent Advances in the Numerical Solution of Differential Equations The numerical solution of ODE-IVPs has been the subject of many investigations since many decades, and a number of reliable, state-of-art codes are available for their solution. However, when stability results by first approximation does not apply, things become more involved, and general purpose codes may fail. This is the case, e.g., of Hamiltonian problems, for which the class of Runge-Kutta methods, known as Hamiltonian Boundary Value Methods (HBVMs), has been recently proposed. Such methods, in turn, are equivalent to a truncated expansion of the vector field along the Legendre orthonormal polynomial basis. This fact, coupled with the availability of a very efficient nonlinear iteration for their implementation, allows the use of HBVMs as spectrally accurate methods in time. The truncated expansion can be also regarded as imposing the orthogonality of the residual, w.r.t. a suitable inner product, to all polynomials of a certain degree. This allows, in turn, the use of different polynomial bases, as well as the extension of the approach for coping with different differential problems. As an example, the extension to the case of fractional differential equations has been recently studied. February 17, 2025 5:30 PM Main Lecture Hall, ex-ISEF building Felice Iavernaro University of Bari Aldo Moro, Italy The principle of maximum entropy in least squares approximation We consider using a maximal entropy argument to determine the coefficients of the approximating function and squared error weights simultaneously as output of a (weighted) least-squares approximation problem. Interpreting the weights as a probability distribution, we choose them by maximizing the \u201camount of uncertainty\u201d or entropy, subject to the constraint that the mean squared error is prescribed to a desired (small) value. By acting on this error, we get a robust method for linear and nonlinear regression that automatically detects and removes outliers from the dataset during the fitting procedure, by assigning them a very small weight. We consider the use of both polynomials and univariate/bivariate splines. To show the potential of the maximal-entropy approach in least squares approximation of data we consider a number of illustrations in different application fields. February 17, 2025 4:15 PM Main Lecture Hall, ex-ISEF building Matthias Voigt UniDistance, Switzerland Treating inhomogeneous initial conditions in balancing-based model reduction In standard balanced truncation model order reduction, the initial condition is typically ignored in the reduction procedure and is assumed to be zero instead. However, such a reduced-order model may be a bad approximation to the full-order system, if the initial condition is not zero. In the literature there are several attempts for modified reduction methods at the price of having no error bound or only a posteriori error bounds which are often too expensive to evaluate. In this talk we propose a new balancing procedure that is based on a shift transformation on the state. We first derive a joint projection reduced-order model in which the part of the system depending only on the input and the one depending only on the initial value are reduced at once and we prove an a priori error bound. With this result at hand, we derive a separate projection procedure in which the two parts are reduced separately. This gives the freedom to choose different reduction orders for the different subsystems. Moreover, we discuss how the reduced-order models can be constructed in practice. Since the error bounds are parameter-dependent, we show how they can be optimized efficiently. We conclude by comparing our results with the ones from the literature by a series of numerical experiments. This is joint work with Christian Schr\u00f6der. February 13, 2025 4:00 PM Room D, ex-ISEF building Mar\u00eda L\u00f3pez Fern\u00e1ndez University of Malaga, Spain Generalized Convolution Quadrature for non smooth sectorial problems We consider the application of the generalized Convolution Quadrature (gCQ) to approximate the solution of an important class of sectorial problems. The gCQ is a generalization of Lubich's Convolution Quadrature (CQ) that allows for variable steps. The available stability and convergence theory for the gCQ requires non realistic regularity assumptions on the data, which do not hold in many applications of interest, such as the approximation of subdiffusion equations. It is well known that for non smooth enough data the original CQ, with uniform steps, presents an order reduction close to the singularity. We generalize the analysis of the gCQ to data satisfying realistic regularity assumptions and provide sufficient conditions for stability and convergence on arbitrary sequences of time points. We consider the particular case of graded meshes and show how to choose them optimally, according to the behaviour of the data. An important advantage of the gCQ method is that it allows for a fast and memory reduced implementation. We describe how the fast and oblivious gCQ can be implemented and illustrate our theoretical results with several numerical experiments. Recording of the talk (available to GSSI only) January 29, 2025 5:00 PM Main Lecture Hall, ex-ISEF building Froil\u00e1n Mart\u00ednez Dopico Universidad Carlos III de Madrid, Spain Polynomial and rational matrices with prescribed data We study necessary and sufficient conditions for the existence of polynomial and rational matrices with different prescribed data. First, we consider the problem for polynomial matrices when the size, degree, rank, invariant factors, infinite elementary divisors, and the minimal indices of their left and right null spaces are prescribed and prove that a polynomial matrix with these data exists if and only if these data satisfy a surprisingly simple unique condition related to a fundamental constraint known as the \"index sum theorem'\". In the second place, we extend this result to rational matrices when the size, rank, invariant rational functions, invariant orders at infinity, and minimal indices of their left and right null spaces are prescribed. The data prescribed so far are called in the literature the \"complete structural data\" or the \"complete eigenstructure\" of the polynomial or rational matrix. Finally, in addition to the complete eigenstructure, we also prescribe the minimal indices of the row and column spaces and show that the simple condition found in the previous problems must be completed with a majorization relation among the involved indices, the degrees of the invariant factors and of the infinite elementary divisors. January 23, 2024 2:30 PM Main Lecture Hall, ex-ISEF building Emre Mengi Koc University, Turkey A Subspace Framework for Large-Scale H-infinity Norm Computation I will describe a subspace framework to compute the H-infinity norm of the transfer function of a control system with large order, a commonly employed objective to minimize for instance in controller design, model order reduction and robust control. Our setting encompasses the transfer functions of descriptor systems, delay systems and many other non-rational transfer functions. In this H-infinity context, we employ tools from model order reduction such as Petrov-Galerkin projections in the state space. The talk is intended to be self-contained as much as possible. December 17, 2024 4:00 PM Main Lecture Hall, ex-ISEF building Emre Mengi Koc University, Turkey Large-Scale Eigenvalue Optimization The talk concerns a subspace framework for the general setting of optimizing a prescribed eigenvalue of a large parameter-dependent matrix. The approach aims to replace the large parameter-dependent matrix with a smaller one obtained via orthogonal projections. We analyze the convergence properties of the framework, and point out open problems. An adaptation of the framework for large-scale semidefinite programs is presented. December 11, 2024 4:30 PM Auditorium, Rectorate building Vladimir Protasov University of L'Aquila, Italy The Newton aerodynamic problem: all over again after 300 years In 1687 Isaac Newton in his main work \"Principia'' posed the following Aerodynamic Problem: find the convex surface of the minimal frontal resistance during its uniform motion through an inviscid and incompressible medium. The surface must contain a given disc orthogonal to the vector of velocity and have a given altitude. The solution of Newton became a classical example in the calculus of variations. However, in early 1990s G.Butazzo, D.Kawohl, and P.Guasoni presented a surface of a smaller resistance. They showed that the optimality of Newtons's surface concerns only a special case (although, considered to be general by most of specialists). The aerodynamic problem had to be solved again under general assumptions, which turned out to be a hard task. The solution is still unknown, although some properties of the optimal surface have been established. We give a survey of some of the known results including construction of non-convex surfaces of arbitrary small resistance and of invisible surfaces. Then we present a new approach to analyze the optimal convex surface by using inequalities between derivatives. Some of these inequalities are, probably, of independent interest. This is a joint work with A.Plakhov (University of Aveiro, Portugal). November 25, 2024 4:30 PM Conference Room, ex-INPS building Carola-Bibiane Sch\u00f6nlieb University of Cambridge, United Kingdom Mathematical imaging: from PDEs to deep learning for images Images are a rich source of beautiful mathematical formalism and analysis. Associated mathematical problems arise in functional and non-smooth analysis, the theory and numerical analysis of nonlinear partial differential equations, inverse problems, harmonic, stochastic and statistical analysis, and optimisation, just to name a few. Applications of mathematical imaging are profound and arise in biomedicine, material sciences, astronomy, digital humanities, as well as many technological developments such as autonomous driving, facial screening and many more. In this talk I will discuss my perspective onto mathematical imaging, share my fascination and vision for the subject. I will then zoom into one research problem that I am currently most excited about and that we helped make first advances on: the mathematical formalisation of machine learned approaches for solving inverse imaging problems. November 21, 2024 3:00 PM Main Lecture Hall, ex-ISEF building Nikos Pitsianis Aristotle University of Thessaloniki, Greece The Fiedler Connection to Graph Clustering with Resolution Variation TBA November 20, 2024 5:00 PM Main Lecture Hall, ex-ISEF building","title":"Academic Year 2024/25"},{"location":"seminar/#academic-year-202324","text":"Speaker Title, Abstract & Other Info Date & Venue Paolo Denti University of Cape Town, South Africa Pharmacometrics: a quantitative tool for pharmacological research July 1, 2024 11:00 AM Library, ex-ISEF building Christian Lubich University of T\u00fcbingen, Germany Regularized dynamical parametric approximation This talk is about the numerical approximation of evolution equations by nonlinear parametrizations $u(t)=\\Phi(q(t))$ with time-dependent parameters $q(t)$, which are to be determined in the computation. The motivation comes from approximations in quantum dynamics by multiple Gaussians and approximations of various dynamical problems by tensor networks and neural networks. In all these cases, the parametrization is typically irregular: the derivative $\\Phi'(q)$ can have arbitrarily small singular values and may have varying rank. We derive approximation results for a regularized approach in the time-continuous case as well as in time-discretized cases. With a suitable choice of the regularization parameter and the time stepsize, the approach can be successfully applied in irregular situations, even though it runs counter to the basic principle in numerical analysis to avoid solving ill-posed subproblems when aiming for a stable algorithm. Numerical experiments with sums of Gaussians for approximating quantum dynamics and with neural networks for approximating the flow map of a system of ordinary differential equations illustrate and complement the theoretical results. The talk is based on joint work with Caroline Lasser, J\u00f6rg Nick and Michael Feischl. June 6, 2024 3:30 PM Main Lecture Hall, ex-ISEF building Paolo Freguglia University of L'Aquila, Italy On the corporate organization. A proposal Inspired by the classic conceptions of \u201ceconomic equilibrium\u201d related to the Arrow (1971) and Debreu (1959) model, we have constructed a mathematical paradigm. We start from a possible \u201ceconomic game\u201d which simulates the relationship between companies and customers, involving company sales, costs and value-assets. So we establish an equilibrium which can be seen as a weighted average of a random variable. Then we arrive at a basic iterable algebraic equation that establishes a relationship between sales and its estimated forecast. The analysis of the evolution of the simple deviations between sales and estimate forecasts is also proposed. Afterwards we keep into account the actual results related to a manufacturing company (with 130 clerks and workers), limiting our analysis to the evolution during 40 months (time unit is the month) of a single product relating to a single customer. In conclusion we gave some ideas about the notion of \u201csales trend\u201d. May 28, 2024 5:15 PM Main Lecture Hall, ex-ISEF building Santolo Leveque Scuola Normale Superiore of Pisa, Italy Parallel-in-Time Solver for the All-at-Once Runge-Kutta Discretization Time-dependent PDEs arise quite often in many scientific areas, such as mechanics, biology, economics, or chemistry, just to name a few. Of late, researchers have devoted their effort in devising parallel-in-time methods for the numerical solution of time-dependent PDEs, adding a new dimension of parallelism and allowing to speed-up the solution process on modern supercomputers. In this talk, we present a parallel-in-time preconditioner for the all-at-once linear system arising when employing a Runge--Kutta method in time. The resulting system is solved iteratively for the numerical solution and for the stages of the method. The proposed preconditioner results in a block-diagonal solution for the stages, and a Schur complement obtained by solving again systems for the stages. A range of numerical experiments validate the robustness of the preconditioner with respect to the discretization parameters and to the number of stages, showing the speed-up achieved on a parallel architecture. This is a joint work with Luca Bergamaschi (University of Padua), Angeles Martinez (University of Trieste), and John W. Pearson (University of Edinburgh). May 28, 2024 4:00 PM Main Lecture Hall, ex-ISEF building Desmond Higham University of Edinburgh, United Kingdom A Numerical Analysis Perspective on the Stability of AI Systems Over the last decade, adversarial attack algorithms have revealed instabilities in deep learning tools. These algorithms raise issues regarding safety, reliability and interpretability in artificial intelligence (AI); especially in high risk settings. Ideas from numerical analysis play key roles in this landscape. From a practical perspective, there has been a war of escalation between those developing attack and defence strategies. At a more theoretical level, researchers have also studied bigger picture questions concerning the existence and computability of successful attacks. I will present examples of attack algorithms in image classification and optical character recognition. I will also outline recent results on the overarching question of whether, under reasonable assumptions, it is inevitable that AI tools will be vulnerable to attack. May 8, 2024 5:30 PM Auditorium, Rectorate building Catherine Higham University of Glasgow, United Kingdom Quantum imaging with deep learning algorithms Quantum imaging technology offers time-efficient, lightweight, less-invasive and low-cost solutions in many different contexts including autonomous driving, security and health. Enhancing this technology with deep learning addresses challenges arising in experimental optimisation, inverse and classification/regression tasks and improves overall performance. I will talk about the deep learning algorithms I have developed for several projects. These include real time scene and depth reconstruction for single photon sensor camera and LiDAR systems, and high-speed imaging using a Micro LED-on-CMOS light projector. May 8, 2024 4:15 PM Auditorium, Rectorate building Emre Mengi Koc University, Turkey Large-scale minimization of the pseudospectral abscissa The minimization of the spectral abscissa of a matrix dependent on parameters has drawn interest in the last couple of decades. The problem is motivated especially by the stability considerations for the associated linear control system. The major difficulty in the minimization of the spectral abscissa is that its dependence on the parameters is non-Lipschitz. A remedy to this is, for a prescribed epsilon > 0, minimizing the epsilon-pseudospectral abscissa, the real part of the rightmost point in the set consisting of eigenvalues of all matrices at a distance of epsilon with respect to the spectral norm. We present a subspace framework to minimize the epsilon-pseudospectral abscissa of a large matrix dependent on parameters analytically. At every subspace iteration, a one-sided subspace restriction on the parameter-dependent matrix yields a small rectangular pseudospectral abscissa minimization problem. We expand the restriction subspace based on the minimizer of this small problem. We prove in theory for the proposed subspace framework that, assuming the global minimizers of the small problems are retrieved, convergence to the global minimizer of the original large-scale pseudospectral abscissa minimization problem occurs in the infinite dimensional setting. Our theoretical findings are illustrated on real large-scale examples that concern the stabilization by static output feedback of benchmark linear control systems. April 23, 2024 4:45 PM Main Lecture Hall, ex-ISEF building Mattia Manucci University of Stuttgart, Germany Approximating the smallest eigenvalue of large Hermitian matrices that depend on parameters April 23, 2024 3:30 PM Main Lecture Hall, ex-ISEF building Ernst Hairer Universit\u00e9 de Gen\u00e8ve, Switzerland Leapfrog methods for relativistic charged-particle dynamics A basic leapfrog integrator and its variational variant are proposed and studied for the numerical integration of the equations of motion of relativistic charged particles in an electromagnetic field. The methods are based on a four-dimensional formulation of the equations of motion. Structure-preserving properties of the numerical methods are analysed, in particular conservation and long-time near-conservation of energy and mass shell as well as preservation of volume in phase space. In the non-relativistic limit, the considered methods reduce to the Boris algorithm for non-relativistic charged-particle dynamics and its variational variant. This talk is based on joint-work with Christian Lubich April 18, 2024 2:30 PM Main Lecture Hall, ex-ISEF building Matthias Voigt FernUni, Switzerland Structure-Preserving Model Order Reduction of Dynamical Systems Model order reduction is a field of mathematics that deals with the complexity reduction of mathematical models in order to reduce the computational costs of tasks such as numerical simulation, optimization, or control. In this talk, first an overview of the field will be given and two of the classical systems theoretic reduction methods (balanced truncation and IRKA) will be reviewed. The second part of the presentation will focus on structure-preserving reduction methods for structured models. This includes a variant of balanced truncation for symmetric second-order systems and a rational fitting method for the class of port-Hamiltonian systems. January 17, 2024 4:00 PM Conference Room, ex-INPS building Alessandro Valerio University of Trondheim, Norway Neuroscience - AI In this seminar, Alessandro Valerio, a graduate in neuroscience, explores the field of neuroscience and its potential intersections with artificial intelligence. Alessandro has an educational background in biology and neuroscience from the University of L\u2019Aquila and Trieste, complemented by research experiences at SISSA in Trieste and at the Kavli Institute for System Neuroscience in Trondheim. The seminar provides an overview of the brain's structure, covering aspects from the macroscopic to the molecular level. A significant focus is given to the role of electrophysiological methods in neuroscience. Alessandro details both in vitro and in vivo techniques, which are crucial in understanding the intricacies of neural dynamics. A part of the seminar is dedicated to Alessandro's research journey. His work at SISSA centered on cellular neurobiology and in vitro electrophysiology, particularly in studying rhythmogenesis mechanisms. His journey continued in Trondheim, where he engaged in in vivo electrophysiology and multisensory integration studies at the Kavli Institute for System Neuroscience. The seminar concludes with Alessandro presenting his research goals. He aims to leverage AI advancements in neuroscience research and reciprocally apply neuroscience insights to advance AI development. This includes a focus on utilizing deep learning for EEG signal classification and exploring the potential of bio-inspired neural networks. January 10, 2024 6:00 PM Conference Room, ex-INPS building Lauri Nyman Aalto University, Finland Nearest singular pencil via Riemannian optimization The problem of finding the nearest singular pencil to a given regular, complex or real, n \u00d7 n matrix pencil A + \u03bbB is a long-standing problem in Numerical Linear Algebra. This problem turned out to be very difficult and, so far, just a few numerical algorithms are available in the literature for its solution, though they may be very expensive from a computational point of view. In this talk, we introduce a new algorithm for solving this problem based on Riemannian optimization, which looks for the closest singular pencil via the minimization of an objective function over the cartesian product of the unitary group by itself. Moreover, we present a collection of numerical experiments that show that the new algorithm can deal effectively with pencils of larger sizes than those considered by previous algorithms and find minimizers of, at least, the same quality than previous algorithms. January 10, 2024 4:00 PM Conference Room, ex-INPS building Mattia Manucci University of Stuttgart, Germany Model Order Reduction for switched Differential Algebraic Equations In this presentation, we will discuss a projection-based Model Order Reduction (MOR) for large-scale systems of switched Differential Algebraic Equations (sDAEs). The main idea relies on exploiting the Quasi-Weierstrass form of each mode of the sDAEs system. Then, we can show that, under certain reasonable assumptions, the output of the sDAEs system is equivalent to the output of a switched system of ordinary differential equations (ODEs) with state jumps between the modes. We present how to construct a reduced system by computing the controllability and observability Gramians associated with the solution of generalized Lyapunov equations for bilinear systems. Finally, we discuss how to efficiently compute an approximation of the solution of such generalized Lyapunov equations, with error control, when the associated matrices are sparse and large. Numerical results are presented to showcase the efficiency and effectiveness of the developed MOR method. December 20, 2023 5:00 PM Conference Room, ex-INPS building","title":"Academic Year 2023/24"},{"location":"seminar/#academic-year-202223","text":"Speaker Title, Abstract & Other Info Date & Venue Christian Lubich Universit\u00e4t T\u00fcbingen, Germany Time integration of tree tensor networks First I report on recent numerical experiments with time-dependent tree tensor network algorithms for the approximation of quantum spin systems. I will then describe the basics in the design of time integration methods that are robust to the usual presence of small singular values, that have good structure-preserving properties (norm, energy conservation or dissipation), and that allow for rank (= bond dimension) adaptivity and also have some parallelism. This discussion of basic concepts will be done for the smallest possible type of tensor network differential equations, namely low-rank matrix differential equations. Once this simplest case is understood, there is a systematic path to the extension of the integrators and their favourable properties to general tree tensor networks. June 8, 2023 3:00 PM Auditorium, Rectorate building Simone Brugiapaglia Concordia University, Canada The mathematical foundations of deep learning: from rating impossibility to practical existence theorems Deep learning is having a profound impact on industry and scientific research. Yet, while this paradigm continues to show impressive performance in a wide variety of applications, its mathematical foundations are far from being well established. In this talk, I will present recent developments in this area by illustrating two case studies. First, motivated by applications in cognitive science, I will present 'rating impossibility' theorems. They identify frameworks where deep learning is provably unable to generalize outside the training set for the seemingly simple task of learning identity effects, i.e. classifying whether pairs of objects are identical or not. Second, motivated by applications in scientific computing, I will illustrate 'practical existence' theorems. They combine universal approximation results for deep neural networks with compressed sensing and high-dimensional polynomial approximation theory. As a result, they yield sufficient conditions on the network architecture, the training strategy, and the number of samples able to guarantee accurate approximation of smooth functions of many variables. Time permitting, I will also discuss work in progress and open questions. June 1, 2023 2:00 PM Auditorium, Rectorate building Ernst Hairer Universit\u00e9 de Gen\u00e8ve, Switzerland Large-stepsize integrators for charged particles in a strong magnetic field This talk considers the numerical treatment of the differential equation that describes the motion of electric particles in a strong magnetic field. A standard integrator is the Boris algorithm which, for small stepsizes, can be analysed by classical techniques. For a strong magnetic field the solution is highly oscillatory and the numerical integration is more challenging. New modifications of the Boris algorithm are discussed, and their accuracy and long-time behaviour are studied with means of modulated Fourier expansions. Emphasis is put on the situation where the stepsize is proportional to (or larger than) the wavelength of the oscillations. The presented results have been obtained in collaboration with Christian Lubich. April 20, 2023 2:30 PM Auditorium, Rectorate building Mauro Piccioni Sapienza Universit\u00e0 di Roma, Italy Estimating the interaction graph for a class of Galves-Locherbach models (Part 2) We explain how to estimate the presence and the nature of the interactions (either excitatory or inhibitory) in a multivariate point process modeling a network of spiking neurons (Galves and Locherbach, JSP 2013). In the first talk a Markovian model is analyzed whereas in the second delayed inhibitory interactions have to be detected. April 19, 2023 5:30 PM Main Lecture Hall, ex-ISEF building Emilio De Santis Sapienza Universit\u00e0 di Roma, Italy Estimating the interaction graph for a class of Galves-Locherbach models (Part 1) We explain how to estimate the presence and the nature of the interactions (either excitatory or inhibitory) in a multivariate point process modeling a network of spiking neurons (Galves and Locherbach, JSP 2013). In the first talk a Markovian model is analyzed whereas in the second delayed inhibitory interactions have to be detected. April 19, 2023 5:30 PM Main Lecture Hall, ex-ISEF building Kai Bergermann TU Chemnitz, Germany Adaptive rational Krylov methods for exponential Runge--Kutta integrators We consider the solution of large stiff systems of ordinary differential equations with explicit exponential Runge--Kutta integrators. These problems arise from semi-discretized semi-linear parabolic partial differential equations on continuous domains or on inherently discrete graph domains. A series of results reduces the requirement of computing linear combinations of $\\varphi$-functions in exponential integrators to the approximation of the action of a smaller number of matrix exponentials on certain vectors. State-of-the-art computational methods use polynomial Krylov subspaces of adaptive size for this task. They have the drawback that the required Krylov subspace iteration numbers to obtain a desired tolerance increase drastically with the spectral radius of the discrete linear differential operator, e.g., the problem size. We present an approach that leverages rational Krylov subspace methods promising superior approximation qualities. We prove a novel a-posteriori error estimate of rational Krylov approximations to the action of the matrix exponential on vectors for single time points, which allows for an adaptive approach similar to existing polynomial Krylov techniques. We discuss pole selection and the efficient solution of the arising sequences of shifted linear systems by direct and preconditioned iterative solvers. Numerical experiments show that our method outperforms the state of the art for sufficiently large spectral radii of the discrete linear differential operators. The key to this are approximately constant rational Krylov iteration numbers, which enable a near-linear scaling of the runtime with respect to the problem size. April 18, 2023 5:30 PM Main Lecture Hall, ex-ISEF building Gianluca Ceruti EPFL, Switzerland A rank-adaptive robust BUG for dynamical low-rank approximation In the present contribution, a rank-adaptive integrator for the dynamical low-rank approximation of matrix differential equations is introduced. First, the dynamical low-rank approximation together with the projector-splitting numerical integrator is summarised. Then, recent developments on the field are presented. The so-called fixed-rank unconventional BUG integrator is introduced and analysed. Next, a simple but extremely effective modification allowing for an adaptive choice of the rank, using subspaces generated by the integrator itself, is illustrated. It is shown that the novel BUG adaptive low-rank integrator retains the exactness, robustness and symmetry-preserving properties of the previously proposed fixed-rank integrators. Beyond that, up to the truncation tolerance, the rank-adaptive integrator preserves the norm when the differential equation does, it preserves the energy for Schroedinger equations and Hamiltonian systems, and it preserves the monotonic decrease in gradient flows. This seminar will focus on the numerical error analysis of the adaptive low-rank integrator. Furthemore, the first theoretically solid application to low-rank training in machine learning will be discussed. The present contribution is based upon joint works with Ch. Lubich, J. Kusch, and F. Tudisco. The last developments in the field of machine learning have been possible only due to the great contributions of PhD candidates E. Zangrando (GSSI) and S. Schotthoefer (KIT). March 16, 2023 3:00 PM Zoom meeting Alfio Quarteroni Politecnico di Milano, Italy, and EPFL, Switzerland A Mathematical Heart In this presentation I will report on the most recent achievements of the iHEART project, aimed at developing a comprehensive accurate mathematical and numerical model for the simulation of the complete cardiac function. December 15, 2022 3:00 PM Auditorium, Rectorate building","title":"Academic Year 2022/23"},{"location":"seminar/#academic-year-202122","text":"Speaker Title, Abstract & Other Info Date & Venue Emre Mengi Koc University, Turkey Large-Scale Estimation of the Dominant Poles of a Transfer Function The dominant poles of the transfer function of a descriptor system provide important insight into the behavior of the system. They indicate the parts of the imaginary axis where the transfer function exhibits large norm. Moreover, the dominant poles and corresponding eigenvectors can be put in use to form a reduced-order approximation to the system. In the talk, I will describe a subspace framework to compute a prescribed number of dominant poles of a large-scale descriptor system. The framework applies Petrov-Galerkin projections to the original system, then computes the dominant poles of the projected small-scale system, for instance by the QZ algorithm, and expands the subspaces so that the projected system after the subspace expansion interpolates the original system at these dominant poles. I will explain why the subspace framework converges at a quadratic rate, and report numerical results illustrating the rapid convergence, and accuracy of the approach. September 19, 2022 1:30 PM Main Lecture Hall, ex-ISEF building Stefano Massei University of Pisa, Italy Improved parallel-in-time integration via low-rank updates and interpolation This work is concerned with linear matrix equations that arise from the space-time discretization of time-dependent linear partial differential equations (PDEs). Such matrix equations have been considered, for example, in the context of parallel-in-time integration leading to a class of algorithms called ParaDiag. We develop and analyze two novel approaches for the numerical solution of such equations. Our first approach is based on the observation that the modification of these equations performed by ParaDiag in order to solve them in parallel has low rank. Building upon previous work on low-rank updates of matrix equations, this allows us to make use of tensorized Krylov subspace methods to account for the modification. Our second approach is based on interpolating the solution of the matrix equation from the solutions of several modifications. Both approaches avoid the use of iterative refinement needed by ParaDiag and related space-time approaches in order to attain good accuracy. In turn, our new approaches have the potential to outperform, sometimes significantly, existing approaches. This potential is demonstrated for several different types of PDEs. September 14, 2022 Main Lecture Hall, ex-ISEF building Massimiliano Fasi Durham University, UK CPFloat: A C Library for Emulating Low-Precision Arithmetic Low-precision floating-point arithmetic can be simulated via software by executing each arithmetic operation in hardware and rounding the result to the desired number of significant bits. For IEEE-compliant formats, rounding requires only standard mathematical library functions, but handling subnormals, underflow, and overflow demands special attention, and numerical errors can cause mathematically correct formulae to behave incorrectly in finite arithmetic. Moreover, the ensuing algorithms are not necessarily efficient, as the library functions these techniques build upon are typically designed to handle a broad range of cases and may not be optimized for the specific needs of floating-point rounding algorithms. CPFloat is a C library that offers efficient routines for rounding arrays of binary32 and binary64 numbers to lower precision. The software exploits the bit level representation of the underlying formats and performs only low-level bit manipulation and integer arithmetic, without relying on costly library calls. In numerical experiments the new techniques bring a considerable speedup (typically one order of magnitude or more) over existing alternatives in C, C++, and MATLAB. To the best of our knowledge, CPFloat is currently the most efficient and complete library for experimenting with custom low-precision floating-point arithmetic available in any language. June 24, 2022 1:00 PM Library, ex-ISEF building Stefano Serra-Capizzano University of Insubria, Italy The GLT class as a Generalized Fourier Analysis and applications Recently, the class of Generalized Locally Toeplitz (GLT) sequences has been introduced [1, 2] as a generalization both of classical Toeplitz sequences and of variable coefficient differential operators and, for every sequence of the class, it has been demonstrated that it is possible to give a rigorous description of the asymptotic spectrum in terms of a function (the symbol) that can be easily identified. This generalizes the notion of a symbol for differential operators also of fractional type (discrete and continuous) or for Toeplitz sequences for which it is identified through the Fourier coefficients and is related to the classical Fourier Analysis. The GLT class has nice algebraic properties and indeed it has been proven that it is stable under linear combinations, products, and inversion when the sequence which is inverted shows a sparsely vanishing symbol (sparsely vanishing symbol = a symbol which vanishes at most in a set of zero Lebesgue measure). Furthermore, the GLT class virtually includes any approximation by local methods (Finite Difference, Finite Element, Isogeometric Analysis of partial differential equations (PDEs)) and, based on this, we demonstrate that our results on GLT sequences can be used in a PDE setting in various directions. [1] S. Serra-Capizzano. Generalized Locally Toeplitz sequences: spectral analysis and applications to discretized partial differential equations . Linear Algebra Appl. 366 (2003), 371\u2013402. [2] S. Serra-Capizzano. The GLT class as a generalized Fourier Analysis and applications . Linear Algebra Appl. 419 (2006), 180\u2013233. Further references can be found here . May 26, 2022 2:00 PM Main Lecture Hall, ex-ISEF building Paolo Cifani Gran Sasso Science Institute, Italy Geometric integration of Lie-Poisson flows on the sphere In this seminar I will touch upon the recent developments in structure-preserving (geometric) integration of Euler\u2019s equations for two-dimensional incompressible flows. It has been known for half a century that the dynamics of incompressible ideal fluids in two dimensions can be understood as an evolution equation on the contangent bundle of the infinite-dimensional Lie group of symplectic dffeomorphisms. In particular, the vorticity equation constitutes a Lie-Poisson system characterized by an infinite number of first integrals, i.e. the integrated powers of vorticity. This set of constraints, absent in three dimensions, has profound effects on the energy transfer mechanisms across scales of motion. Yet, the construction of a numerical system which preserves this rich Poisson structure has been elusive. Most attempts either fail in fully preserving the geometric structure or have a high computational complexity. Here, I will show that, thanks to our recent advances, it possible to design a geometric integrator which embeds this fundamental principle of the continuum into the discrete system at a modest computational cost. The construction of such scheme, the main numerical algorithms and their parallelisation on modern supercomputing facilities will be discussed. Finally, an application to the spectrum of homogeneous two-dimensional turbulence will be illustrated. May 24, 2022 12:45 PM Main Lecture Hall, ex-ISEF building Simone Camarri University of Pisa, Italy Adjoint-based passive control of hydrodynamic instabilities A large number of research papers in the literature have been dedicated to the use of adjoint-based sensitivity and global stability analyses for both characterising and controlling instabilities in fluid mechanics. Such controls, which are mainly passive, are designed for stabilising linearly unstable configurations. The design strategy based on global stability analysis can be rigorously applied to relatively simple flows in laminar regime. More complex configurations can also be rigorously treated, as for instance cases in which the flow to be controlled is time periodic. Moreover, a large interest exists in the application of the same methods for the control of coherent large-scale flow structures in turbulent flows as, for instance, the quasi-periodic shedding of vortices in turbulent wakes. This is possible by postulating the marginal stability of mean flows, which is shown to apply for several cases of interest. In this seminar a review of the methods based on global stability and sensitivity analyses for the design and/or analysis of passive controls will be presented. Moreover, configurations of increasing complexity will be considered, ranging from laminar steady flows to turbulent flows. May 18, 2022 4:00 PM Main Lecture Hall, ex-ISEF building Lev Lokutsievskiy Steklov Institute, Moscow, Russia Asymptotic control theory for affine switching systems of oscillators The talk will be devoted to continuous-time affine control systems and their reachable sets. I will focus on the case when all eigenvalues of the linear part of the system have zero real part. In this case, the reachable sets usually have a non-exponential growth rate as T\u2192\u221e, and it is usually polynomial. The simplest non-trivial example is the problem of stabilisation (or, conversely, destabilisation) of two pendulums by the same common control. An exact description of reachable sets is most often impossible here, but their asymptotic behaviour as T\u2192\u221e can be found with high accuracy. In the talk, I will present the asymptotic behaviour of reachable sets in the problem of controlling a system of N independent oscillators, and in the problem of controlling the wave equation for a closed string. In particular, in these problems the corresponding analog of the Lyapunov function can be found explicitly, and, consequently, the optimal behaviour at high energies can be found very accurately May 11, 2022 4:30 PM Main Lecture Hall, ex-ISEF building Ernst Hairer University of Geneva, Switzerland High order PDE-convergence of ADI-type integrators for parabolic problems This work considers space-discretised parabolic problems on a rectangular domain subject to Dirichlet boundary conditions. For the time integration s-stage AMF-W-methods, which are ADI (Alternating Direction Implicit) type integrators, are considered. They are particularly efficient when the space dimension of the problem is large. The classical algebraic conditions for order p (with p<=3) are shown to be sufficient for PDE-convergence of order p (independently of the spatial resolution) in the case of time independent Dirichlet boundary conditions. Under additional conditions, PDE-convergence of order p=3.25-eps for every eps>0 can be obtained. In the case of time dependent boundary conditions there is an order reduction. This is joint work with Severiano Gonzalez-Pinto and Domingo Hernandez-Abreu. Related publications can be downloaded from http://www.unige.ch/~hairer/preprints.html Recording of the talk (available to GSSI only) April 26, 2022 1:00 PM Auditorium, Rectorate building Marino Zennaro University of Trieste, Italy Computing antinorms on multicones The theoretical results we consider in this talk may find an interesting application in the study of discrete-time linear switched systems of the form x(n + 1) = A_{\u03c3(n)} x(n), \u03c3 : N \u2212\u2192 {1, 2, . . . , m}, where x(0) \u2208 R^k, the matrix A_{\u03c3(n)} \u2208 R^{k\u00d7k} belongs to a finite family F = {A_i}{1\u2264i\u2264m} and \u03c3 denotes the switching law. It is known that the most stable switching laws are associated to the so-called spectrum-minimizing products of the family F. Moreover, for a normalized family F of matrices (i.e., with lower spectral radius \u03c1\u02c7(F) = 1) that share an invariant cone K, all the most stable trajectories starting from the interior of K lie on the boundary of the antiball of a so-called invariant Barabanov antinorm, for which a canonical constructive procedure is available (Guglielmi & Z. (2015)). In the particular case of families F sharing an invariant cone K, we show how to provide lower bounds to \u02c7\u03c1(F) by a suitable adaptation of the Gelfand limit in the framework of antinorms (Guglielmi & Z. (2020)). Then we consider families of matrices F that share an invariant multicone K_{mul} (Brundu & Z. (2018, 2019)) and show how to generalize some of the known results on antinorms to this more general setting (Guglielmi & Z. (in progress)). These generalizations are of interest because common invariant multicones may well exist when common invariant cones do not. March 31, 2022 2:00 PM Auditorium, Rectorate building Giorgio Fusco University of L'Aquila, Italy TBA February 24, 2022 Erkki Somersalo Case Western Reserve University, USA Bayes meets data science to identify changes in brain activity during meditation from MEG measurements Meditation as a potential alternative for pharmaceutical intervention to mitigate conditions such as chronic pain or clinical depression continues to obtain significant attention. One of the problems is that often the positive effects of meditation that have been reported are anecdotal or are based on self reporting. To quantify the effects of meditation, it is therefore important to develop methods based on medical imaging to identify brain regions that are involved in the meditation practice. In this talk, we review some recent results about this topic, addressed by using magnetoencephalography (MEG) to map brain activity during meditation. One of the difficulties here is that the data are less sensitive to activity taking place in the deep brain regions, including the limbic system that is believed to play an important role in meditation. The MEG inverse problem is addressed by using novel Bayesian methods combined with advanced numerical techniques, applied on data from professional Buddhist meditators. The reconstructed activity is then analyzed using data science techniques to distill the information about the activation changes during meditation. Recording of the talk (available to GSSI only) December 16, 2021 3:00 PM Auditorium, Rectorate building Matthew Colbrook University of Cambridge, UK Computing semigroups and time-fractional PDEs with error control We develop an algorithm that computes strongly continuous semigroups on infinite-dimensional Hilbert spaces with explicit error control. Given a generator $A$, a time $t > 0$, an arbitrary initial vector $u_0$ and an error tolerance $\\epsilon > 0$, the algorithm computes $\\exp(tA)u_0$ with error bounded by $\\epsilon$. The (parallelisable) algorithm is based on a combination of a regularized functional calculus, suitable contour quadrature rules and the adaptive computation of resolvents in infinite dimensions. As a particular case, we deal with semigroups on $L^2(R^d)$ that are generated by partial differential operators with polynomially bounded coefficients of locally bounded total variation. For analytic semigroups, we provide a quadrature rule whose error decreases like $\\exp(\u2212cN/ log(N))$ for $N$ quadrature points, that remains stable as $N \\to \\infty$, and which is also suitable for infinite-dimensional operators. Finally, we extend the method to time-fractional PDEs (where it avoids singularities as $t \\to 0$ and large memory consumption). Numerical examples are given, including: Schr\u00f6dinger and wave equations on the aperiodic Ammann\u2013Beenker tiling and fractional beam equations arising in the modelling of small-amplitude vibration of viscoelastic materials. The spectral analysis (which is always needed for contour methods) is considerably simplified due to an infinite-dimensional \u201csolve-then-discretise\u201d approach. December 1, 2021 5:00 PM Main Lecture Hall, ex-ISEF building","title":"Academic Year 2021/22"},{"location":"seminar/#academic-year-202021","text":"Speaker Title, Abstract & Other Info Date & Venue Lars Ruthotto Emory University, USA Numerical Methods for Deep Learning motivated by Partial Differential Equations Understanding the world through data and computation has always formed the core of scientific discovery. Amid many different approaches, two common paradigms have emerged. On the one hand, primarily data-driven approaches\u2014such as deep neural networks\u2014have proven extremely successful in recent years. Their success is based mainly on their ability to approximate complicated functions with generic models when trained using vast amounts of data and enormous computational resources. But despite many recent triumphs, deep neural networks are difficult to analyze and thus remain mysterious. Most importantly, they lack the robustness, explainability, interpretability, efficiency, and fairness needed for high-stakes decision-making. On the other hand, increasingly realistic model-based approaches\u2014typically derived from first principles and formulated as partial differential equations (PDEs)\u2014are now available for various tasks. One can often calibrate these models\u2014which enable detailed theoretical studies, analysis, and interpretation\u2014with relatively few measurements, thus facilitating their accurate predictions of phenomena. In this talk, I will highlight recent advances and ongoing work to understand and improve deep learning by using techniques from partial differential equations. I will demonstrate how PDE techniques can yield better insight into deep learning algorithms, more robust networks, and more efficient numerical algorithms. I will also expose some of the remaining computational and numerical challenges in this area. Slides of the talk Recording of the talk (available to GSSI only) June 17, 2021 4:00 PM Ivan Markovsky Vrije Universiteit Brussel, Belgium Data-driven dynamic interpolation and approximation The behavioral system theory give theoretical foundation for nonparameteric representations of linear time-invariant systems based on Hankel matrices constructed from data. These data-driven representations led in turn to new system identification, signal processing, and control methods. In particular, data-driven simulation and linear quadratic tracking control problems were solved using the new approach [1,2]. This talk shows how the approach can be used further on for solving data-driven interpolation and approximation problems (missing data estimation) and how it can be generalized to some classes of nonlinear systems. The theory leads to algorithms that are both general (can deal simultaneously with missing, exact, and noisy data of multivariable systems) and simple (require existing numerical linear algebra methods only). This opens a practical computational way of doing system theory and signal processing directly from data without identification of a transfer function or a state space representation and doing model-based design. [1] I. Markovsky and P. Rapisarda. \u201cData-driven simulation and control\u201d. Int. J. Control 81.12 (2008), pp. 1946--1959. [2] I. Markovsky. A missing data approach to data-driven filtering and control. IEEE Trans. Automat. Contr., 62:1972--1978, April 2017. [3] I. Markovsky and F. D\u00f6rfler. Data-driven dynamic interpolation and approximation. Technical report, Vrije Universiteit Brussel, 2021. Recording of the talk (available to GSSI only) March 30, 2021 5:00 PM Eugene E. Tyrtyshnikov Institute for Numerical Mathematics, Russian Academy of Sciences Tikhonov's solution to a class of linear systems equivalent within perturbations A standard approach to incorrect problems suggests that a problem of interest is reformulated with the knowledge of some additional a-priori information. This can be done by several well-known regularization techniques. Many practical problems are successfully solved on this way. What does not still look as completely satisfactory is that the new reset problem seems to appear rather implicitly in the very process of its solution. In 1980, A. N. Tikhonov proposed a reformulation [1] that arises explicitly before the discussion of the solution methods. He suggested a notion of normal solution to a family of linear algebraic systems described by a given individual system and its vicinity comprising perturbed systems, under the assumption that there are compatible systems in the class notwithstanding the compatibility property of the given individual system. Tikhovov proved that the normal solution exists and is unique. However, a natural question about the correctness of the reset problem was not answered. In this talk we address a question of correctness of the reformulated incorrect problems that seems to have been missed in all previous considerations. The main result is the proof of correctness for Tikhonov's normal solution. Possible generalizations and difficulties will be also discussed. [1] A. N. Tikhonov, Approximate systems of linear algebraic equations, USSR Computational Mathematics and Mathematical Physics, vol. 20, issue 6 (1980) March 9, 2021 6:00 PM Michael Schaub RWTH Aachen University Learning from signals on graphs with unobserved edges In many applications we are confronted with the following system identification scenario: we observe a dynamical process that describes the state of a system at particular times. Based on these observations we want to infer the (dynamical) interactions between the entities we observe. In the context of a distributed system, this typically corresponds to a \"network identification\" task: find the (weighted) edges of the graph of interconnections. However, often the number of samples we can obtain from such a process are far too few to identify the edges of the network exactly. Can we still reliably infer some aspects of the underlying system? Motivated by this question we consider the following identification problem: instead of trying to infer the exact network, we aim to recover a (low-dimensional) statistical model of the network based on the observed signals on the nodes. More concretely, here we focus on observations that consist of snapshots of a diffusive process that evolves over the unknown network. We model the (unobserved) network as generated from an independent draw from a latent stochastic blockmodel (SBM), and our goal is to infer both the partition of the nodes into blocks, as well as the parameters of this SBM. We present simple spectral algorithms that provably solve the partition and parameter inference problems with high-accuracy. We further discuss some possible variations and extensions of this problem setup. Recording of the talk (available to GSSI only) February 17, 2021 6:00 PM Lothar Reichel Kent State University Large-scale regression with non-convex loss and penalty We do non-convex optimization with application to image restoration and regression problems for which a sparse solution is desired. February 4, 2021 5:00 PM Anders Hansen University of Cambridge, UK On the foundations of computational mathematics, Smale's 18th problem and the potential limits of AI There is a profound optimism on the impact of deep learning (DL) and AI in the sciences with Geoffrey Hinton concluding that 'They should stop training radiologists now'. However, DL has an Achilles heel: it is universaly unstable so that small changes in the initial data can lead to large errors in the final result. This has been documented in a wide variety of applications. Paradoxically, the existence of stable neural networks for these applications is guaranteed by the celebrated Universal Approximation Theorem, however, the stable neural networks are never computed by the current training approaches. We will address this problem and the potential limitations of AI from a foundations point of view. Indeed, the current situation in AI is comparable to the situation in mathematics in the early 20th century, when David Hilbert\u2019s optimism (typically reflected in his 10th problem) suggested no limitations to what mathematics could prove and no restrictions on what computers could compute. Hilbert\u2019s optimism was turned upside down by Goedel and Turing, who established limitations on what mathematics can prove and which problems computers can solve (however, without limiting the impact of mathematics and computer science). We predict a similar outcome for modern AI and DL, where the limitations of AI (the main topic of Smale\u2019s 18th problem) will be established through the foundations of computational mathematics. We sketch the beginning of such a program by demonstrating how there exist neural networks approximating classical mappings in scientific computing, however, no algorithm (even randomised) can compute such a network to even 1-digit accuracy (with probability better than 1/2). We will also show how instability is inherit in the methodology of DL demonstrating that there is no easy remedy, given the current methodology. Finally, we will demonstrate basic examples in inverse problems where there exists (untrained) neural networks that can easily compute a solution to the problem, however, the current DL techniques will need 10^80 data points in the training set to get even 1% success rate. Recording of the talk January 28, 2021 3:00 PM Gianluca Ceruti University of Tuebingen Numerical integrators for dynamical low-rank approximation Discretization of time-dependent high-dimensional PDEs suffers of an undesired effect, known as curse of dimensionality. The amount of data to be stored and treated, grows exponentially, and exceeds standard capacity of common computational devices. In this setting, time dependent model order reductions techniques are desirable. In the present seminar, together with efficient numerical integrators, we present a recently developed approach: dynamical low-rank approximation. Dynamical low-rank approximation for matrices will be firstly presented, and a numerical integrator with two remarkable properties will be introduced: the matrix projector splitting integrator. Based upon this numerical integrator, we will construct two equivalent extensions for tensors, multi-dimensional arrays, in Tucker format - a high-order generalization of the SVD decomposition for matrices. These extensions are proven to preserve the excellent qualities of the matrix integrator. To conclude, via a novel compact formulation of the Tucker integrator, we will further extend the matrix and Tucker projector splitting integrators to the most general class of Tree Tensor Networks. Important examples belonging to this class and of interest for applications are given, but not only restricted to, by Tensor Trains. This seminar is based upon a joint work with Ch. Lubich and H. Walach. January 13, 2021 5:00 PM Alexander Viguerie GSSI Efficient, stable, and reliable solvers for the Steady Incompressible Navier-Stokes equations: application to Computational Hemodynamics. Over the past several years, computational fluid dynamics (CFD) simulations have become increasingly popular as a clinical tool for cardiologists at the patient-specific level. The use of CFD in this area poses several challenges. The clinical setting places heavy restrictions on both computational time and power. Simulation results are usually desired within minutes and are usually run on standard computers. For these reasons, steady-state Navier-Stokes simulations are usually preferred, as they can be completed in a fraction of the time required to run an unsteady computation. However, in many respects the steady problem is more difficult than the unsteady one, particularly in regards to solving the associated linear and nonlinear systems. Additionally, boundary data for patient-specific problems is often missing, incomplete, or unreliable. This makes the determination of a useful model challenging, as it requires the generation of reliable boundary data without introducing heavy computational costs. This seminar will address these challenges, as well as some others, and introduce new techniques for workarounds. Results from patient-specific cases will be presented and discussed. Recording of the talk (available to GSSI only) December 16, 2020 5:00 PM Martin Stoll TU-Chemnitz From PDEs to data science: an adventure with the graph Laplacian In this talk we briefly review some basic PDE models that are used to model phase separation in materials science. They have since become important tools in image processing and over the last years semi-supervised learning strategies could be implemented with these PDEs at the core. The main ingredient is the graph Laplacian that stems from a graph representation of the data. This matrix is large and typically dense. We illustrate some of its crucial features and show how to efficiently work with the graph Laplacian. In particular, we need some of its eigenvectors and for this the Lanczos process needs to be implemented efficiently. Here, we suggest the use of the NFFT method for evaluating the matrix vector products without even fully constructing the matrix. We illustrate the performance on several examples. Recording of the talk (available to GSSI only) December 2, 2020 5:00 PM Patricia Diaz De Alba GSSI Numerical treatment for inverse electromagnetic problems Electromagnetic induction surveys are among the most popular techniques for non-destructive investigation of soil properties, in order to detect the presence of both ground inhomogeneities and particular substances. Frequency-domain electromagnetic instruments allow the collection of data in different configurations, that is, varying the intercoil spacing, the frequency, and the height above the ground. Based on a non-linear forward model used to describe the interaction between an electromagnetic field and the soil, the aim is to reconstruct the distribution of either the electrical conductivity or the magnetic permeability with respect to depth. To this end, the inversion of both the real (in-phase) and the imaginary (quadrature) components of the signal are studied by a regularized damped Gauss-Newton method. The regularization part of the algorithm is based on a low-rank approximation of the Jacobian of the non-linear model. Furthermore, in many situations, a regularization scheme retrieving smooth solutions is blindly applied, without taking into account the prior available knowledge. An algorithm for a regularization method that promotes the sparsity of the reconstructed electrical conductivity or magnetic permeability distribution is available. This regularization strategy incorporates a minimum gradient support stabilizer into a truncated generalized singular value decomposition scheme. The whole inversion algorithm has been enclosed in a MATLAB package, called FDEMtools, allowing the user to experiment with synthetic and experimental data sets, and different regularization strategies, in order to compare them and draw conclusions. The numerical effectiveness of the inversion procedure is demonstrated on synthetic and real datasets by using FDEMtools package. November 20, 2020 5:15 PM Christian Lubich University of Tuebingen Dynamical low-rank approximation This talk reviews differential equations and their numerical solution on manifolds of low-rank matrices or of tensors with a rank structure such as tensor trains or general tree tensor networks. These low-rank differential equations serve to approximate, in a data-compressed format, large time-dependent matrices and tensors or multivariate functions that are either given explicitly via their increments or are unknown solutions to high-dimensional evolutionary differential equations, with multi-particle time-dependent Schr\u00f6dinger equations and kinetic equations such as Vlasov equations as noteworthy examples of applications. Recently developed numerical time integrators are based on splitting the projection onto the tangent space of the low-rank manifold at the current approximation. In contrast to all standard integrators, these projector-splitting methods are robust to the unavoidable presence of small singular values in the low-rank approximation. This robustness relies on exploiting geometric properties of the manifold of low-rank matrices or tensors: in each substep of the projector-splitting algorithm, the approximation moves along a flat subspace of the low-rank manifold. In this way, high curvature due to small singular values does no harm. This talk is based on work done intermittently over the last decade with Othmar Koch, Bart Vandereycken, Ivan Oseledets, Emil Kieri, Hanna Walach and Gianluca Ceruti. November 18, 2020 5:00 PM Raffaele D'Ambrosio University of L'Aquila Structure-preserving numerics for stochastic differential equations Modern Numerical Analysis is not only devoted to accurately approximating the solutions of various problems through efficient and robust schemes, but also to retaining qualitative properties of the continuous problem over long times. Sometimes such conservation properties naturally characterize the numerical schemes, while in more complex situations preservation issues have to be conveyed into the numerical approximations. The talk is focused on presenting recent advances in structure-preservation issues for selected stochastic differential equations satisfying some characteristic invariance laws. The behaviour of stochastic multistep methods in the preservation of mean-square contractivity will be analyzed; we show, in this case, that conservation properties are hidden, as a matter of fact, into conditional stability properties of numerical schemes. The analysis will also be conveyed to the discretization of stochastic Hamiltonian problems for the numerical preservation of the behaviour of the expected Hamiltonian. The theoretical analysis will also be supported by the numerical evidence. November 4, 2020 3:00 PM Giacomo Baggio University of Padua From Model-Based to Data-Driven Control of Network Dynamics The control of complex dynamical networks has attracted increasing interest over the past few years, with reference, in particular, to problems of controllability, optimal input design, and minimal actuator placement. In this talk, I will address the problem of controlling linear dynamical networks from a control-energy or \"practical\" perspective. I will first focus on the model-based scenario and review the fundamental metrics, theoretical limitations, and challenges in controlling networks using a limited number of control nodes. In particular, I will emphasize the impact of the \"degree\" of non-normality of the network's adjacency matrix on the control performance. Then, I will switch to a data-driven scenario, and show how some network control problems can be efficiently solved by relying on experimental data only. Slides of the seminar October 14, 2020 4:00 PM Federico Poloni University of Pisa Inverses of quasidefinite matrices in block-factored form, with an application to control theory joint work with P. Benner We describe an algorithm to compute the explicit inverse of a dense quasi-definite matrix, i.e., a symmetric matrix of the form [-BB^T, A; A^T, C^TC], with the (1,1) block negative semidefinite and the (2,2) block positive semidefinite. The algorithm is a variant of Gauss-Jordan elimination that works on the low-rank factors $B$ and $C$ directly without ever forming those blocks. The individual elimination steps amount to a transformation called principal pivot transform ; it was shown in [Poloni, Strabic 2016] how to perform it by working only on $A, B, C$, and we rely on that procedure here. We discuss the stability of the resulting method, and show how the algorithm (and in particular the produced low-rank factors) can be of use in control theory, in the context of the matrix sign iteration, which is a method used to solve algebraic Riccati equations. September 30, 2020 4:00 PM","title":"Academic Year 2020/21"},{"location":"vacancies/","text":"Vacancies NEWS We are looking to hire a new colleague at the level of Tenure-Track Assistant Professor to join our group ! We are looking for a postdoctoral research associate to join our group on a joint project with Michele Benzi from Scuola Normale Superiore in Pisa. The postdoctoral fellow will be working on topics at the interface between Numerical Methods and Machine Learning and will be funded by the MUR-Pro3 grant \"STANDS - Numerical STAbility of Neural Dynamical Systems\". The official call for application will open up soon. For more details and in order to express your interest, please refer to this form . Tenure-Track Assistant Professor Call for Expressions of Interest for a Faculty Position at the level of tenure track Assistant Professor (RTDB) in the Numerical Analysis Group (Math Division) of the Gran Sasso Science Institute, School of Advanced Studies. The Numerical Analysis and Data Science group of the Gran Sasso Science Institute (GSSI) invites expressions of interest for a faculty position at the Assistant Professor level (Tenure Track - RTDB) from highly qualified scholars worldwide. The GSSI Numerical Analysis Division hosts scientists carrying out interdisciplinary research on a variety of subjects, ranging from Matrix Analysis, Numerical ODEs, Numerical PDEs, Dynamical systems, Graph Theory, Numerical Linear Algebra, Computational Network Science and Mathematics of Machine Learning . The GSSI is an International School of Advanced Doctoral Studies, with a prominent program of graduate courses. Located in the city of L\u2019Aquila, in the heart of the Apennine Mountains east of Rome, the institute offers a stimulating environment with numerous PhD students and postdoctoral researchers selected internationally every year. English is the official language for all the activities at the institute (teaching, seminars, tutoring, etc.). About 40 PhD Students in Applied Mathematics from all over the world are at GSSI at any given time, and part of the duties of the Faculty is to provide teaching and supervision of their research activity. Interdisciplinary projects aimed at increasing the connections with other PhD courses at GSSI within the Math Division as well as the other areas (physics, computer science and regional economics) are encouraged. The GSSI plans to announce one tenure-track position in 2022. The present call for expressions of interest is aimed at probing the community of scientists at the international level that might want to join the GSSI faculty. We emphasize that all non-Italian citizens and all Italian candidates having spent sufficient time working abroad are entitled to up to 90% reduction of tax load for at least the first 6 years of employment, as per current Italian regulations. Additionally, the successful candidate will be entitled to up to 30% salary increase with respect to National standards, subject to positive evaluation by the hiring committee. Candidates willing to work in the exciting GSSI atmosphere, as tenure track Assistant Professors, are required to have a doctoral degree in mathematics or related fields prior to the appointment, with a proven record of achievements, a clear potential to promote and lead research activities and a specific interest in teaching at the graduate level to a small set of particularly skilled and motivated students. A curriculum vitae, a list of publications, and a brief research statement should be sent by email ( numerics@gssi.it ), together with a short motivation letter in which the candidate discusses the reasons for their interest in joining the GSSI team. The candidates should also arrange for two recommendation letters to be sent directly to the GSSI ( numerics@gssi.it ), to support their application. The expressions of interest should be sent at your earliest convenience and in any case not later than Jan 31, 2022. GSSI is committed to gender balance, inclusion and diversity. All expressions of interest will be given proper consideration, independent of ethnicity, religion, age, gender, sexual orientation, or disability. PhD students Applications for the PhD program at GSSI open every year around March with several open positions in all the four areas. Deadline for the application is usually in mid June. The PhD program consists of four years with qualifying exams to be taken during the first year. The scholarship for the program offers a monthly salary/stipend, research funds and free housing in institute's residences in the city center of L'Aquila. More details on how to apply can be found here and here .","title":"Vacancies"},{"location":"vacancies/#vacancies","text":"","title":"Vacancies"},{"location":"vacancies/#news","text":"We are looking to hire a new colleague at the level of Tenure-Track Assistant Professor to join our group ! We are looking for a postdoctoral research associate to join our group on a joint project with Michele Benzi from Scuola Normale Superiore in Pisa. The postdoctoral fellow will be working on topics at the interface between Numerical Methods and Machine Learning and will be funded by the MUR-Pro3 grant \"STANDS - Numerical STAbility of Neural Dynamical Systems\". The official call for application will open up soon. For more details and in order to express your interest, please refer to this form .","title":"NEWS"},{"location":"vacancies/#tenure-track-assistant-professor","text":"Call for Expressions of Interest for a Faculty Position at the level of tenure track Assistant Professor (RTDB) in the Numerical Analysis Group (Math Division) of the Gran Sasso Science Institute, School of Advanced Studies. The Numerical Analysis and Data Science group of the Gran Sasso Science Institute (GSSI) invites expressions of interest for a faculty position at the Assistant Professor level (Tenure Track - RTDB) from highly qualified scholars worldwide. The GSSI Numerical Analysis Division hosts scientists carrying out interdisciplinary research on a variety of subjects, ranging from Matrix Analysis, Numerical ODEs, Numerical PDEs, Dynamical systems, Graph Theory, Numerical Linear Algebra, Computational Network Science and Mathematics of Machine Learning . The GSSI is an International School of Advanced Doctoral Studies, with a prominent program of graduate courses. Located in the city of L\u2019Aquila, in the heart of the Apennine Mountains east of Rome, the institute offers a stimulating environment with numerous PhD students and postdoctoral researchers selected internationally every year. English is the official language for all the activities at the institute (teaching, seminars, tutoring, etc.). About 40 PhD Students in Applied Mathematics from all over the world are at GSSI at any given time, and part of the duties of the Faculty is to provide teaching and supervision of their research activity. Interdisciplinary projects aimed at increasing the connections with other PhD courses at GSSI within the Math Division as well as the other areas (physics, computer science and regional economics) are encouraged. The GSSI plans to announce one tenure-track position in 2022. The present call for expressions of interest is aimed at probing the community of scientists at the international level that might want to join the GSSI faculty. We emphasize that all non-Italian citizens and all Italian candidates having spent sufficient time working abroad are entitled to up to 90% reduction of tax load for at least the first 6 years of employment, as per current Italian regulations. Additionally, the successful candidate will be entitled to up to 30% salary increase with respect to National standards, subject to positive evaluation by the hiring committee. Candidates willing to work in the exciting GSSI atmosphere, as tenure track Assistant Professors, are required to have a doctoral degree in mathematics or related fields prior to the appointment, with a proven record of achievements, a clear potential to promote and lead research activities and a specific interest in teaching at the graduate level to a small set of particularly skilled and motivated students. A curriculum vitae, a list of publications, and a brief research statement should be sent by email ( numerics@gssi.it ), together with a short motivation letter in which the candidate discusses the reasons for their interest in joining the GSSI team. The candidates should also arrange for two recommendation letters to be sent directly to the GSSI ( numerics@gssi.it ), to support their application. The expressions of interest should be sent at your earliest convenience and in any case not later than Jan 31, 2022. GSSI is committed to gender balance, inclusion and diversity. All expressions of interest will be given proper consideration, independent of ethnicity, religion, age, gender, sexual orientation, or disability.","title":"Tenure-Track Assistant Professor"},{"location":"vacancies/#phd-students","text":"Applications for the PhD program at GSSI open every year around March with several open positions in all the four areas. Deadline for the application is usually in mid June. The PhD program consists of four years with qualifying exams to be taken during the first year. The scholarship for the program offers a monthly salary/stipend, research funds and free housing in institute's residences in the city center of L'Aquila. More details on how to apply can be found here and here .","title":"PhD students"},{"location":"venues/","text":"Venues All our seminars and activities are held in the Gran Sasso Science Institute buildings. These are the Ex-ISEF building , the Ex-INPS building and the Rectorate building . Ex-ISEF building Address: Viale Francesco Crispi, 7, 67100 L'Aquila AQ The main rooms we make use of are: Main Lecture Hall (MLH), ground floor; Room A, B, C, D, floor -1; Library, floor -1. Ex-INPS building Address: Viale Luigi Rendina, 26-28, 67100 L'Aquila AQ The rooms of Ex-INPS we mainly make use of are: Conference room, floor -1. Rectorate building Address: Via Michele Iacobucci, 2, 67100 L'Aquila AQ The rooms of the rectorate we mainly make use of are: Auditorium, floor -1.","title":"Venues"},{"location":"venues/#venues","text":"All our seminars and activities are held in the Gran Sasso Science Institute buildings. These are the Ex-ISEF building , the Ex-INPS building and the Rectorate building .","title":"Venues"},{"location":"venues/#ex-isef-building","text":"Address: Viale Francesco Crispi, 7, 67100 L'Aquila AQ The main rooms we make use of are: Main Lecture Hall (MLH), ground floor; Room A, B, C, D, floor -1; Library, floor -1.","title":"Ex-ISEF building"},{"location":"venues/#ex-inps-building","text":"Address: Viale Luigi Rendina, 26-28, 67100 L'Aquila AQ The rooms of Ex-INPS we mainly make use of are: Conference room, floor -1.","title":"Ex-INPS building"},{"location":"venues/#rectorate-building","text":"Address: Via Michele Iacobucci, 2, 67100 L'Aquila AQ The rooms of the rectorate we mainly make use of are: Auditorium, floor -1.","title":"Rectorate building"}]}