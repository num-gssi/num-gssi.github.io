---
disable_toc: true
---

# COMPiLE Reading Group 


We organize a reading group on *COMPutational LEarning*, specifically on topics related to **Numerics/Matrix Theory/Graph Theory with particular emphasis on their application/connection with Machine Learning, Data Mining and Network Science.** 
We meet regularly to read a paper of common interest, together, with the goal of creating some motivation for staying up to date with the new papers or to catch up with some papers that you wanted to read but postponed for some time.  

The meetings take place in the Main Lecture Hall on Tuesdays (most of the time). In person participation is highly encouraged, but there will be the possibility to join also via zoom.   

If you are interested in participating please complete the form below to be to be notified about the meetings:   
Google Form: [Subscription to Reading Group mailing list](https://forms.gle/8BUovFDTbx9YH6L2A)

If you have suggestions for papers to ready you can propose your ideas here:   
Google Form: [Suggestions for papers to read](https://forms.gle/fzrMX4Vyu7WBmbYB6)

If you want inspiration for a possible paper to read, here is a list of the papers that have been suggested so far:  
[Suggested papers for COMPiLE Reading Group](https://docs.google.com/spreadsheets/d/1YM04jxGIwCXoHGE0n1wysrhqgKZRfYUl-H1FggotmCE/edit?usp=sharing)

The reading group is organized by our early career group members and supervised/coordinated by  [Nicola Guglielmi](https://www.gssi.it/people/professors/lectures-maths/item/545-guglielmi-nicola) and [Francesco Tudisco](https://ftudisco.gitlab.io). 


|Organizer|||Period|
|-------|-------|--------|-------|
|<img style="width:60px;" src="https://www.gssi.it/media/k2/items/cache/d2808c1e280bd25060c03c46b800b728_L.jpg">|[Arturo De Marinis](https://www.gssi.it/people/students/students-maths/item/15651-de-marinis-arturo), GSSI| <div style="width:200px;"></div>| February 2023 -- Present
|<img style="width:60px;" src="https://www.gssi.it/media/k2/items/cache/7f9f1b99149b66abc7b5a2731faa302f_L.jpg">|[Dayana Savostianova](https://www.gssi.it/people/students/students-maths/item/11045-savostianova-dayana), GSSI| <div style="width:200px;"></div>| November 2021 -- Summer 2022

|Presenter||Title, Abstract & Other Info|Date|
|------|------|------|----|
[Piero Deidda](https://www.gssi.it/people/post-doc/post-doc-maths/item/22288-piero-deidda) || Spectral Decompositions using One-Homogeneous Functionals, by Martin Burger, Guy Gilboa, Michael Moeller, Lina Eckardt and Daniel Cremers<br><br> This paper discusses the use of absolutely one-homogeneous regularization functionals in a variational, scale space, and inverse scale space setting to define a nonlinear spectral decomposition of input data. We present several theoretical results that explain the relation between the different definitions. Additionally, results on the orthogonality of the decomposition, a Parseval-type identity and the notion of generalized (nonlinear) eigenvectors closely link our nonlinear multiscale decompositions to the well-known linear filtering theory. Numerical results are used to illustrate our findings.<br><br>[link](https://arxiv.org/abs/1601.02912)|May 31, 2023|
[Emanuele Natale](https://natema.github.io/ema-webpage/) || On the Random Subset Sum Problem and Neural Networks<br><br> The Random Subset Sum Problem (RSSP) is a fundamental problem in mathematical optimization, especially in the understanding of the statistical behavior of integer linear programs. Recently, the theory related to the problem has found applications also in theoretical machine learning, providing key tools for the proof of the Strong Lottery Ticket Hypothesis (SLTH) for dense neural network architectures. In this talk, I will present two recent joint works that push the application of the RSSP further. First, we provide a proof of the SLTH for convolutional neural networks, generalizing the original result. Second, we leverage those ideas to provide a novel way to build neuromorphic hardware.|March 23, 2023|
[Arturo De Marinis](https://www.gssi.it/people/students/students-maths/item/15651-de-marinis-arturo) || The Forward-Forward Algorithm: Some Preliminary Investigations, by Geoffrey Hinton<br><br> The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth further investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes could be separated in time, the negative passes could be done offline, which would make the learning much simpler in the positive pass and allow video to be pipelined through the network without ever storing activities or stopping to propagate derivatives.<br><br>[link](https://arxiv.org/abs/2212.13345)|February 27, 2023|
[Pierluigi Crescenzi](https://www.gssi.it/institute/disciplinary-board/item/10311-crescenzi-pierluigi) || Proving the Lottery Ticket Hypothesis for Convolutional Neural Networks, by Arthur da Cunha, Emanuele Natale, Laurent Viennot<br><br> The lottery ticket hypothesis states that a randomly-initialized neural network contains a small subnetwork which, when trained in isolation, can compete with the performance of the original network. Recent theoretical works proved an even stronger version: every sufficiently overparameterized (dense) neural network contains a subnetwork that, even without training, achieves accuracy comparable to that of the trained large network. These works left as an open problem to extend the result to convolutional neural networks (CNNs). In this work we provide such generalization by showing that, with high probability, it is possible to approximate any CNN by pruning a random CNN whose size is larger by a logarithmic factor.<br><br>[link](https://openreview.net/pdf?id=Vjki79-619-)|April 27, 2022|
[Arturo De Marinis](https://www.gssi.it/people/students/students-maths/item/15651-de-marinis-arturo) || Structure-preserving deep learning, by Elena Celledoni, Matthias J. Ehrhardt, Christian Etmann, Robert I McLachlan, Brynjulf Owren, Carola-Bibiane Schönlieb, Ferdia Sherry<br><br> Over the past few years, deep learning has risen to the foreground as a topic of massive interest, mainly as a result of successes obtained in solving large-scale image processing tasks. There are multiple challenging mathematical problems involved in applying deep learning: most deep learning methods require the solution of hard optimisation problems, and a good understanding of the tradeoff between computational effort, amount of data and model complexity is required to successfully design a deep learning approach for a given problem. A large amount of progress made in deep learning has been based on heuristic explorations, but there is a growing effort to mathematically understand the structure in existing deep learning methods and to systematically design new deep learning methods to preserve certain types of structure in deep learning. In this article, we review a number of these directions: some deep neural networks can be understood as discretisations of dynamical systems, neural networks can be designed to have desirable properties such as invertibility or group equivariance, and new algorithmic frameworks based on conformal Hamiltonian systems and Riemannian manifolds to solve the optimisation problems have been proposed. We conclude our review of each of these topics by discussing some open problems that we consider to be interesting directions for future research.<br><br>[link](https://www.cambridge.org/core/journals/european-journal-of-applied-mathematics/article/structurepreserving-deep-learning/15384A9F2776B2D1C1F1D3CDA390D779)|April 6, 2022|
[Giuseppe Lipardi](https://www.gssi.it/people/students/students-maths/item/15653-lipardi-giuseppe) || Computing semiclassical quantum dynamics with Hagedorn wavepackets, Erwan Faou, Vasile Gradinaru, and Christian Lubich<br><br> We consider the approximation of multiparticle quantum dynamics in the semiclassical regime by Hagedorn wavepackets, which are products of complex Gaussians with polynomials that form an orthonormal $L^2$ basis and preserve their type under propagation in Schrödinger equations with quadratic potentials. We build a fully explicit, time-reversible time-stepping algorithm to approximate the solution of the Hagedorn wavepacket dynamics. The algorithm is based on a splitting between the kinetic and potential part of the Hamiltonian operator, as well as on a splitting of the potential into its local quadratic approximation and the remainder. The algorithm is robust in the semiclassical limit. It reduces to the Strang splitting of the Schrödinger equation in the limit of the full basis set, and it advances positions and momenta by the Störmer–Verlet method for the classical equations of motion. The algorithm allows for the treatment of multiparticle problems by thinning out the basis according to a hyperbolic cross approximation and of high-dimensional problems by Hartree-type approximations in a moving coordinate frame.<br><br>[link](https://epubs.siam.org/doi/epdf/10.1137/080729724)|March 30, 2022|
|[Martino Caliaro]() || Stability analysis of a chain of non-identical vehicles under bilateral cruise control<br><br>Bilateral cruise control (BCC) suppresses traffic flow instabilities. Previously, for simplicity of analysis, vehicles in BCC traffic flow were assumed to be identical, i.e., using the same gains for control. In this study, we analyze the stability of an inhomogeneous vehicular chain in which the gains used by different vehicles are not the same. Not unexpectedly, mathematical analysis becomes more difficult, and leads to a quadratic eigenvalue problem. We study several different cases, and shows that a chain of vehicles under bilateral cruise control is stable even when the vehicles do not all have the same control system properties. Numerical simulations validate the analysis.<br><br>[http://eprints.maths.manchester.ac.uk/2688/1/wtsh19.pdf](http://eprints.maths.manchester.ac.uk/2688/1/wtsh19.pdf)|March 23, 2022|
|[Vishnu Sanjay]() || Hodge Laplacians on Graphs, Lek-Heng Lim<br><br>This is an elementary introduction to the Hodge Laplacian on a graph, a higher-order generalization of the graph Laplacian. We will discuss basic properties including cohomology and Hodge theory. The main feature of our approach is simplicity, requiring only knowledge of linear algebra and graph theory. We have also isolated the algebra from the topology to show that a large part of cohomology and Hodge theory is nothing more than the linear algebra of matrices satisfying AB=0. For the remaining topological aspect, we cast our discussions entirely in terms of graphs as opposed to less-familiar topological objects like simplicial complexes.<br><br>[https://arxiv.org/abs/1507.05379](https://arxiv.org/abs/1507.05379)|March 16, 2022|
|[Helena Biscevic]() || Time-symmetric integration in astrophysics<br><br>Calculating the long term solution of ordinary differential equations, such as those of the N-body problem, is central to understanding a wide range of dynamics in astrophysics, from galaxy formation to planetary chaos. Because generally no analytic solution exists to these equations, researchers rely on numerical methods which are prone to various errors. In an effort to mitigate these errors, powerful symplectic integrators have been employed. But symplectic integrators can be severely limited because they are not compatible with adaptive stepping and thus they have difficulty accommodating changing time and length scales. A promising alternative is time-reversible integration, which can handle adaptive time stepping, but the errors due to time-reversible integration in astrophysics are less understood. The goal of this work is to study analytically and numerically the errors caused by time-reversible integration, with and without adaptive stepping. We derive the modified differential equations of these integrators to perform the error analysis. As an example, we consider the trapezoidal rule, a reversible non-symplectic integrator, and show it gives secular energy error increase for a pendulum problem and for a Hénon---Heiles orbit. We conclude that using reversible integration does not guarantee good energy conservation and that, when possible, use of symplectic integrators is favored. We also show that time-symmetry and time-reversibility are properties that are distinct for an integrator.<br><br>[https://arxiv.org/abs/1708.07266](https://arxiv.org/abs/1708.07266)|March 8, 2022|
|[Pierpaolo Bilotto]() || On the spectra of general random graphs, Fan Chung and Mary Radcliffe<br><br>We consider random graphs such that each edge is determined by an independent random variable, where the probability of each edge is not assumed to be equal. We use a Chernoff inequality for matrices to show that the eigenvalues of the adjacency matrix and the normalized Laplacian of such a random graph can be approximated by those of the weighted expectation graph, with error bounds dependent upon the minimum and maximum expected degrees. In particular, we use these results to bound the spectra of random graphs with given expected degree sequences, including random power law graphs. Moreover, we prove a similar result giving concentration of the spectrum of a matrix martingale on its expectation.<br><br>[https://www.combinatorics.org/ojs/index.php/eljc/article/view/v18i1p215](https://www.combinatorics.org/ojs/index.php/eljc/article/view/v18i1p215)|March 2, 2022|
[Mattia Manucci](https://www.gssi.it/institute/organization/item/4811-manucci-mattia) || Physics-informed machine learning for reduced-order modeling of nonlinear problems <br><br>  A reduced basis method based on a physics-informed machine learning framework is developed for efficient reduced-order modeling of parametrized partial differential equations (PDEs). A feedforward neural network is used to approximate the mapping from the time-parameter to the reduced coefficients. During the offline stage, the network is trained by minimizing the weighted sum of the residual loss of the reduced-order equations, and the data loss of the labeled reduced coefficients that are obtained via the projection of high-fidelity snapshots onto the reduced space. Such a network is referred to as physics-reinforced neural network (PRNN). As the number of residual points in time-parameter space can be very large, an accurate network – referred to as physics-informed neural network (PINN) – can be trained by minimizing only the residual loss. However, for complex nonlinear problems, the solution of the reduced-order equation is less accurate than the projection of high-fidelity solution onto the reduced space. Therefore, the PRNN trained with the snapshot data is expected to have higher accuracy than the PINN. Numerical results demonstrate that the PRNN is more accurate than the PINN and a purely data-driven neural network for complex problems. During the reduced basis refinement, the PRNN may obtain higher accuracy than the direct reduced-order model based on a Galerkin projection. The online evaluation of PINN/PRNN is orders of magnitude faster than that of the Galerkin reduced-order model.<br><br>[link](https://www.researchgate.net/publication/354182698_Physics-informed_machine_learning_for_reduced-order_modeling_of_nonlinear_problems)| Feb 23, 2022|
|[Tommaso Tonolo]() || Mean Field Analysis of Hypergraph Contagion Model, Desmond J. Higham and Henry-Louis de Kergorlay<br><br>We typically interact in groups, not just in pairs. For this reason, it has recently been proposed that the spread of information, opinion or disease should be modelled over a hypergraph rather than a standard graph. The use of hyperedges naturally allows for a nonlinear rate of transmission, in terms of both the group size and the number of infected group members, as is the case, for example, when social distancing is encouraged. We consider a general class of individual-level, stochastic, susceptible-infected-susceptible models on a hypergraph, and focus on a mean field approximation proposed in [Arruda et al., Phys. Rev. Res., 2020]. We derive spectral conditions under which the mean field model predicts local or global stability of the infection-free state. We also compare these results with (a) a new condition that we derive for decay to zero in mean for the exact process, (b) conditions for a different mean field approximation in [Higham and de Kergorlay, Proc. Roy. Soc. A, 2021], and (c) numerical simulations of the microscale model.<br><br>[https://arxiv.org/abs/2108.05451](https://arxiv.org/abs/2108.05451)|Feb 23, 2022|
[Arturo De Marinis](https://www.gssi.it/people/students/students-maths/item/15651-de-marinis-arturo) || Regularized nonlinear acceleration, by Damien Scieur,  Alexandre d’Aspremont, Francis Bach <br><br>We describe a convergence acceleration technique for unconstrained optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method. The weights in this average are computed via a simple linear system, whose solution can be updated online. This acceleration scheme runs in parallel to the base algorithm, providing improved estimates of the solution on the fly, while the original optimization method is running. Numerical experiments are detailed on classical classification problems. <br><br>[https://arxiv.org/abs/1606.04133](https://arxiv.org/abs/1606.04133)|Feb 9, 2022
[Konstantin Prokopchik](https://www.gssi.it/people/students/students-computer-science/item/4804-prokopchik-konstantin) || Consensus Dynamics and Opinion Formation on Hypergraphs <br><br> In this chapter, we derive and analyse models for consensus dynamics on hypergraphs. As we discuss, unless there are nonlinear node interaction functions, it is always possible to rewrite the system in terms of a new network of effective pairwise node interactions, regardless of the initially underlying multi-way interaction structure. We thus focus on dynamics based on a certain class of non-linear interaction functions, which can model different sociological phenomena such as peer pressure and stubbornness. Unlike for linear consensus dynamics on networks, we show how our nonlinear model dynamics can cause shifts away from the average system state. We examine how these shifts are influenced by the distribution of the initial states, the underlying hypergraph structure and different forms of non-linear scaling of the node interaction function.  <br><br>  [https://arxiv.org/abs/2105.01369](https://arxiv.org/abs/2105.01369) | Jan 26, 2022 |
[Simone Fioravanti](https://www.gssi.it/people/students/students-computer-science/item/7896-fioravanti-simone) || EigenGame: PCA as Nash Equilibrium <br><br> We present a novel view on principal component analysis (PCA) as a competitive game in which each approximate eigenvector is controlled by a player whose goal is to maximize their own utility function. We analyze the properties of this PCA game and the behavior of its gradient based updates. The resulting algorithm -- which combines elements from Oja's rule with a generalized Gram-Schmidt orthogonalization -- is naturally decentralized and hence parallelizable through message passing. We demonstrate the scalability of the algorithm with experiments on large image datasets and neural network activations. We discuss how this new view of PCA as a differentiable game can lead to further algorithmic developments and insights.  <br><br>  ICLR 2021 - [https://arxiv.org/abs/2010.00554](https://arxiv.org/abs/2010.00554) | Dec 9, 2021 |
[Dayana Savostianova](https://www.gssi.it/people/students/students-maths/item/11045-savostianova-dayana) || Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching <br><br> Data Poisoning attacks modify training data to maliciously control a model trained on such data. In this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a particularly malicious poisoning attack that is both "from scratch" and "clean label", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. Previous poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets. The central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset. Finally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.  <br><br>  ICLR 2021 - [https://arxiv.org/abs/2009.02276](https://arxiv.org/abs/2009.02276) | November 24, 2021 |
